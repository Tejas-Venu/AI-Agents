{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41T_Mg6uugbm"
      },
      "source": [
        "Part 2 - YouTube Transcript\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVIjX8voN3pk",
        "outputId": "b0002b70-ec8b-4bca-8785-1c932c2d6e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-1.2.4-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2026.1.4)\n",
            "Downloading youtube_transcript_api-1.2.4-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.2/485.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-1.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP_JimcVOBUs",
        "outputId": "75543637-cf5d-46fe-f912-f16ce9d11267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain.tools in /usr/local/lib/python3.12/dist-packages (0.1.34)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from langchain.tools) (1.2.10)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (from langchain.tools) (0.4.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (from langchain.tools) (1.1.10)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain->langchain.tools) (1.2.13)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain->langchain.tools) (1.0.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain->langchain.tools) (2.12.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (9.1.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (2.13.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (0.7.3)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (2.0.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain.tools) (2.21.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain.tools) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->langchain.tools) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->langchain.tools) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community->langchain.tools) (1.1.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (0.3.6)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (4.67.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->langchain.tools) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->langchain.tools) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->langchain.tools) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->langchain.tools) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community->langchain.tools) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community->langchain.tools) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community->langchain.tools) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community->langchain.tools) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community->langchain.tools) (3.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai->langchain.tools) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (1.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->langchain.tools) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain.tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "trAWtWpASYUn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlXajZTjRF7P",
        "outputId": "77966e23-7a37-4a3b-929b-4bba8cdf303e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.10)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.5)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.21.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.6)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.67.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (0.25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api langchain langchain-openai langgraph python-dotenv tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r_GNlczS0q2L"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B7gWkrRu06iR"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Mzhm1F5pR0pH"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def extract_video_id(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the 11-character YouTube video ID from a URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): A YouTube URL containing a video ID.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted video ID or error message if parsing fails.\n",
        "    \"\"\"\n",
        "\n",
        "    # Regex pattern to match video IDs\n",
        "    pattern = r'(?:v=|be/|embed/)([a-zA-Z0-9_-]{11})'\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else \"Error: Invalid YouTube URL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4ydBznIhSCIg",
        "outputId": "3676c193-8a81-45e8-c64c-e3006c59cb9f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hfIUstzHs9A'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_video_id.run(\"https://www.youtube.com/watch?v=hfIUstzHs9A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lIoWQ6Xj1CAH"
      },
      "outputs": [],
      "source": [
        "tools = []\n",
        "tools.append(extract_video_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qno1vnnZ1Q2R"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "\n",
        "@tool\n",
        "def fetch_transcript(video_id: str, language: str = \"en\") -> str:\n",
        "    \"\"\"\n",
        "    Fetches the transcript of a YouTube video.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n",
        "        language (str): Language code for the transcript (e.g., \"en\", \"es\").\n",
        "\n",
        "    Returns:\n",
        "        str: The transcript text or an error message.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        ytt_api = YouTubeTranscriptApi()\n",
        "        transcript = ytt_api.fetch(video_id, languages=[language])\n",
        "        return \" \".join([snippet.text for snippet in transcript.snippets])\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "GXjgitwi1Vby",
        "outputId": "7269baeb-58d7-4d84-a3b5-e16e7440350e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Over the past couple of months, large language models, or LLMs, such as chatGPT, have taken the world by storm. Whether it\\'s writing poetry or helping plan your upcoming vacation, we are seeing a step change in the performance of AI and its potential to drive enterprise value. My name is Kate Soule. I\\'m a senior manager of business strategy at IBM Research, and today I\\'m going to give a brief overview of this new field of AI that\\'s emerging and how it can be used in a business setting to drive value. Now, large language models are actually a part of a different class of models called foundation models. Now, the term \"foundation models\" was actually first coined by a team from Stanford when they saw that the field of AI was converging to a new paradigm. Where before AI applications were being built by training, maybe a library of different AI models, where each AI model was trained on very task-specific data to perform very specific task. They predicted that we were going to start moving to a new paradigm, where we would have a foundational capability, or a foundation model, that would drive all of these same use cases and applications. So the same exact applications that we were envisioning before with conventional AI, and the same model could drive any number of additional applications. The point is that this model could be transferred to any number of tasks. What gives this model the super power to be able to transfer to multiple different tasks and perform multiple different functions is that it\\'s been trained on a huge amount, in an unsupervised manner, on unstructured data. And what that means, in the language domain, is basically I\\'ll feed a bunch of sentences-- and I\\'m talking terabytes of data here --to train this model. And the start of my sentence might be \"no use crying over spilled\" and the end of my sentence might be \"milk\". And I\\'m trying to get my model to predict the last word of the sentence based off of the words that it saw before. And it\\'s this generative capability of the model-- predicting and generating the next word --based off of previous words that it\\'s seen beforehand, that is why that foundation models are actually a part of the field of AI called generative AI because we\\'re generating something new in this case, the next word in a sentence. And even though these models are trained to perform, at its core, a generation past, predicting the next word in the sentence, we actually can take these models, and if you introduce a small amount of labeled data to the equation, you can tune them to perform traditional NLP tasks-- things like classification, or named-entity recognition --things that you don\\'t normally associate as being a generative-based model or capability. And this process is called tuning. Where you can tune your foundation model by introducing a small amount of data, you update the parameters of your model and now perform a very specific natural language task. If you don\\'t have data, or have only very few data points, you can still take these foundation models and they actually work very well in low-labeled data domains. And in a process called prompting or prompt engineering, you can apply these models for some of those same exact tasks. So an example of prompting a model to perform a classification task might be you could give a model a sentence and then ask it a question: Does this sentence have a positive sentiment or negative sentiment? The model\\'s going to try and finish generating words in that sentence, and the next natural word in that sentence would be the answer to your classification problem, which would respond either positive or negative, depending on where it estimated the sentiment of the sentence would be. And these models work surprisingly well when applied to these new settings and domains. Now, this is a lot of where the advantages of foundation models come into play. So if we talk about the advantages, the chief advantage is the performance. These models have seen so much data. Again, data with a capital D-- terabytes of data --that by the time that they\\'re applied to small tasks, they can drastically outperform a model that was only trained on just a few data points. The second advantage of these models are the productivity gains. So just like I said earlier, through prompting or tuning, you need far less label data to get to task-specific model than if you had to start from scratch because your model is taking advantage of all the unlabeled data that it saw in its pre-training when we created this generative task. With these advantages, there are also some disadvantages that are important to keep in mind. And the first of those is the compute cost. So that penalty for having this model see so much data is that they\\'re very expensive to train, making it difficult for smaller enterprises to train a foundation model on their own. They\\'re also expensive-- by the time they get to a huge size, a couple billion parameters --they\\'re also very expensive to run inference. You might require multiple GPUs at a time just to host these models and run inference, making them a more costly method than traditional approaches. The second disadvantage of these models is on the trustworthiness side. So just like data is a huge advantage for these models, they\\'ve seen so much unstructured data, it also comes at a cost, especially in the domain like language. A lot of these models are trained basically off of language data that\\'s been scraped from the Internet. And there\\'s so much data that these models have been trained on. Even if you had a whole team of human annotators, you wouldn\\'t be able to go through and actually vet every single data point to make sure that it wasn\\'t biased and didn\\'t contain hate speech or other toxic information. And that\\'s just assuming you actually know what the data is. Often we don\\'t even know-- for a lot of these open source models that have been posted --what the exact datasets are that these models have been trained on leading to trustworthiness issues. So IBM recognizes the huge potential of these technologies. But my partners in IBM Research are working on multiple different innovations to try and improve also the efficiency of these models and the trustworthiness and reliability of these models to make them more relevant in a business setting. All of these examples that I\\'ve talked through so far have just been on the language side. But the reality is, there are a lot of other domains that foundation models can be applied towards. Famously, we\\'ve seen foundation models for vision --looking at models such as DALL-E 2, which takes text data, and that\\'s then used to generate a custom image. We\\'ve seen models for code with products like Copilot that can help complete code as it\\'s being authored. And IBM\\'s innovating across all of these domains. So whether it\\'s language models that we\\'re building into products like Watson Assistant and Watson Discovery, vision models that we\\'re building into products like Maximo Visual Inspection, or Ansible code models that we\\'re building with our partners at Red Hat under Project Wisdom. We\\'re innovating across all of these domains and more. We\\'re working on chemistry. So, for example, we just published and released molformer, which is a foundation model to promote molecule discovery or different targeted therapeutics. And we\\'re working on models for climate change, building Earth Science Foundation models using geospatial data to improve climate research. I hope you found this video both informative and helpful. If you\\'re interested in learning more, particularly how IBM is working to improve some of these disadvantages, making foundation models more trustworthy and more efficient, please take a look at the links below. Thank you.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fetch_transcript.run(\"hfIUstzHs9A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlpsQZ3W6Drl",
        "outputId": "6b834eb2-480c-4d5b-ced1-e68d20cd2f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing https://www.youtube.com/watch?v=x7X9w_GIm1s\n",
            "\n",
            "VIDEO ID: x7X9w_GIm1s\n",
            "\n",
            "--- SUMMARY ---\n",
            " Python is a high-level, interpreted programming language created by Guido van Rossum in 1991, named after Monty Python's Flying Circus. Known for its readability and simplicity, Python is popular among both beginners and advanced developers. It is widely used in server-side applications, big data analysis, and machine learning. Python supports multiple programming paradigms, including functional and object-oriented programming, and boasts a vast ecosystem of third-party libraries. Its syntax, which uses indentation for code structure, promotes efficient coding practices. The Zen of Python emphasizes readability, contributing to its widespread adoption.\n",
            "\n",
            "--- KEY CONCEPTS ---\n",
            "1. **Python Programming Language**: Python is a high-level, interpreted programming language known for its readability and simplicity, making it popular for both beginners and experienced developers.\n",
            "2. **Zen of Python**: A collection of aphorisms that capture the philosophy of Python, emphasizing readability and simplicity, such as \"Beautiful is better than ugly\" and \"Explicit is better than implicit.\"\n",
            "3. **Python Syntax**: Python uses indentation to define the scope of code blocks instead of curly braces or semicolons, which promotes clean and readable code.\n",
            "4. **Dynamic Typing**: Python is dynamically typed, meaning you don't need to declare variable types explicitly, but it is strongly typed, so variables don't change types unexpectedly.\n",
            "5. **Interactive Notebooks**: Python code can be organized into notebooks (e.g., Jupyter Notebooks) where code and documentation coexist, allowing for interactive data analysis and visualization.\n",
            "6. **Multiparadigm Language**: Python supports multiple programming paradigms, including functional programming with anonymous functions (lambdas) and object-oriented programming with classes and inheritance.\n",
            "7. **Third: party Libraries**: Python has a vast ecosystem of libraries, such as TensorFlow for deep learning and OpenCV for computer vision, which can be easily installed using the PIP package manager.\n",
            "8. **Pythonic Code**: Writing \"Pythonic\" code means adhering to Python's idioms and style guidelines, such as avoiding unnecessary semicolons and using list comprehensions for concise code.\n",
            "\n",
            "--- QUIZ ---\n",
            "\n",
            "Q1: Who created the Python programming language?\n",
            "  A. Guido van Rossum\n",
            "  B. James Gosling\n",
            "  C. Bjarne Stroustrup\n",
            "  D. Dennis Ritchie\n",
            "Answer: A\n",
            "Explanation: Python was created by Guido van Rossum and released in 1991.\n",
            "\n",
            "Q2: What is the Python programming language named after?\n",
            "  A. Monty Python's Flying Circus\n",
            "  B. A type of snake\n",
            "  C. A Greek mythological figure\n",
            "  D. A famous scientist\n",
            "Answer: A\n",
            "Explanation: Python is named after Monty Python's Flying Circus, a British comedy series.\n",
            "\n",
            "Q3: Which of the following is a key principle of the Zen of Python?\n",
            "  A. Beautiful is better than ugly\n",
            "  B. Complex is better than complicated\n",
            "  C. Fast is better than slow\n",
            "  D. Verbose is better than concise\n",
            "Answer: A\n",
            "Explanation: The Zen of Python includes the principle 'Beautiful is better than ugly'.\n",
            "\n",
            "Q4: How does Python determine the scope of a line of code?\n",
            "  A. Indentation\n",
            "  B. Curly braces\n",
            "  C. Semicolons\n",
            "  D. Parentheses\n",
            "Answer: A\n",
            "Explanation: Python uses indentation to determine the scope of a line of code, unlike many other languages that use curly braces or semicolons.\n",
            "\n",
            "Q5: Which package manager is commonly used to install third-party libraries in Python?\n",
            "  A. PIP\n",
            "  B. NPM\n",
            "  C. Yarn\n",
            "  D. Composer\n",
            "Answer: A\n",
            "Explanation: PIP is the package manager commonly used to install third-party libraries in Python.\n",
            "\n",
            "Q6: What type of programming paradigms does Python support?\n",
            "  A. Multi-paradigm\n",
            "  B. Procedural only\n",
            "  C. Functional only\n",
            "  D. Object-oriented only\n",
            "Answer: A\n",
            "Explanation: Python is a multi-paradigm language, supporting functional, procedural, and object-oriented programming patterns.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "TEMPERATURE = 0.0\n",
        "CHUNK_CHAR_SIZE = 3500\n",
        "SUMMARY_MAX_TOKENS = 400\n",
        "KEY_CONCEPTS_COUNT = 8\n",
        "QUIZ_QUESTIONS = 6\n",
        "\n",
        "def get_llm():\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"OPENAI_API_KEY not set in environment. Set it before calling get_llm().\")\n",
        "    return ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\n",
        "\n",
        "def chunk_text(text: str, max_chars: int = CHUNK_CHAR_SIZE) -> List[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(text)\n",
        "    while start < n:\n",
        "        end = min(start + max_chars, n)\n",
        "        if end < n:\n",
        "            last_period = text.rfind(\". \", start, end)\n",
        "            last_newline = text.rfind(\"\\n\", start, end)\n",
        "            cut = max(last_period + 2, last_newline + 1, -1)\n",
        "            if cut >= start:\n",
        "                end = cut\n",
        "        chunks.append(text[start:end].strip())\n",
        "        start = end\n",
        "    return chunks\n",
        "\n",
        "def call_llm_prompt(prompt_messages: List[dict]) -> str:\n",
        "    llm = get_llm()\n",
        "    messages = []\n",
        "    for m in prompt_messages:\n",
        "        role = m.get(\"role\", \"user\")\n",
        "        content = m.get(\"content\", \"\")\n",
        "        if role == \"system\":\n",
        "            messages.append(SystemMessage(content=content))\n",
        "        else:\n",
        "            messages.append(HumanMessage(content=content))\n",
        "    resp = llm.invoke(messages)\n",
        "    if hasattr(resp, \"content\") and resp.content:\n",
        "        return resp.content\n",
        "    if isinstance(resp, dict) and \"content\" in resp:\n",
        "        return resp[\"content\"]\n",
        "    return str(resp)\n",
        "\n",
        "def summarize_transcript(transcript: str) -> str:\n",
        "    if not transcript:\n",
        "        return \"No transcript available.\"\n",
        "    chunks = chunk_text(transcript)\n",
        "    chunk_summaries = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        prompt = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a concise summarization assistant.\"},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"Summarize the following portion of a lecture/transcript in 3-5 short bullet points. \"\n",
        "                    \"Be concise and focus on the main points.\\n\\n\"\n",
        "                    f\"PORTION (chunk {i+1}/{len(chunks)}):\\n{chunk}\"\n",
        "                ),\n",
        "            },\n",
        "        ]\n",
        "        out = call_llm_prompt(prompt)\n",
        "        chunk_summaries.append(out.strip())\n",
        "    merged_text = \"\\n\\n\".join(chunk_summaries)\n",
        "    merge_prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that produces a short coherent summary.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Given the following bullet-style summaries from different parts of a lecture, \"\n",
        "                f\"produce a single concise summary (around {SUMMARY_MAX_TOKENS//10}–{SUMMARY_MAX_TOKENS//5} words) \"\n",
        "                \"that captures the overall topic, the main arguments/steps, and the important conclusions. \"\n",
        "                \"Use 4–8 short sentences.\\n\\n\"\n",
        "                f\"INPUT:\\n{merged_text}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "    final_summary = call_llm_prompt(merge_prompt).strip()\n",
        "    return final_summary\n",
        "\n",
        "def extract_key_concepts(transcript: str, n_concepts:int = KEY_CONCEPTS_COUNT) -> List[Dict[str,str]]:\n",
        "    prompt = [\n",
        "        {\"role\":\"system\", \"content\":\"You are a concise educational assistant who extracts important concepts.\"},\n",
        "        {\"role\":\"user\", \"content\": (\n",
        "            f\"From the following transcript, list the {n_concepts} most important concepts or terms a student should learn. \"\n",
        "            \"For each concept, give a one-sentence plain-English explanation and (optionally) a short example or formula.\\n\\n\"\n",
        "            f\"TRANSCRIPT:\\n{(transcript[:20000] + '...') if len(transcript) > 20000 else transcript}\"\n",
        "        )},\n",
        "    ]\n",
        "    out = call_llm_prompt(prompt)\n",
        "    lines = [ln.strip() for ln in out.splitlines() if ln.strip()]\n",
        "    concepts = []\n",
        "    for ln in lines:\n",
        "        if len(concepts) >= n_concepts:\n",
        "            break\n",
        "        parts = re.split(r\"\\s*[:\\-–—]\\s*\", ln, maxsplit=1)\n",
        "        if len(parts) == 2:\n",
        "            concept = parts[0].strip(\"0123456789. )\")\n",
        "            explanation = parts[1].strip()\n",
        "            concepts.append({\"concept\": concept, \"explanation\": explanation})\n",
        "        else:\n",
        "            concepts.append({\"concept\": ln[:60], \"explanation\": ln[60:].strip() or \"See transcript.\"})\n",
        "    return concepts[:n_concepts]\n",
        "\n",
        "def _synthesize_quiz_from_concepts(concepts: List[Dict[str,str]], n_questions: int):\n",
        "    items = []\n",
        "    for i in range(min(n_questions, len(concepts))):\n",
        "        c = concepts[i]\n",
        "        q = f\"What is the best short description of \\\"{c['concept']}\\\"?\"\n",
        "        correct = c['explanation']\n",
        "        distractors = []\n",
        "        for j in range(len(concepts)):\n",
        "            if j != i and len(distractors) < 3:\n",
        "                distractors.append(concepts[j]['explanation'].split('.')[0])\n",
        "        while len(distractors) < 3:\n",
        "            distractors.append(\"None of the above\")\n",
        "        options = [correct] + distractors\n",
        "        import random\n",
        "        random.shuffle(options)\n",
        "        answer_index = options.index(correct)\n",
        "        items.append({\n",
        "            \"question\": q,\n",
        "            \"options\": options,\n",
        "            \"answer_index\": answer_index,\n",
        "            \"explanation\": correct\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def generate_quiz(transcript: str, n_questions:int = QUIZ_QUESTIONS) -> List[Dict[str,Any]]:\n",
        "    prompt = [\n",
        "        {\"role\":\"system\",\"content\":\"You are an instructor creating accurate multiple-choice quiz questions. Return ONLY valid JSON.\"},\n",
        "        {\"role\":\"user\",\"content\": (\n",
        "            f\"Create {n_questions} multiple-choice questions (4 options each) based strictly on the transcript below.\\n\"\n",
        "            \"Return EXACTLY a JSON array. Each element must be:\\n\"\n",
        "            '{\"question\": \"...\", \"options\": [\"A text\",\"B text\",\"C text\",\"D text\"], \"answer_index\": 0, \"explanation\": \"...\"}\\n'\n",
        "            \"answer_index must match the correct option.\\n\\n\"\n",
        "            f\"TRANSCRIPT:\\n{(transcript[:20000] + '...') if len(transcript) > 20000 else transcript}\"\n",
        "        )}\n",
        "    ]\n",
        "    out = call_llm_prompt(prompt)\n",
        "    #print(\"RAW QUIZ OUTPUT:\\n\", out[:4000])\n",
        "    # try to locate a JSON substring in the output\n",
        "    json_text = None\n",
        "    try:\n",
        "        json_text = out.strip()\n",
        "        parsed = json.loads(json_text)\n",
        "    except Exception:\n",
        "        # try to extract first JSON array substring\n",
        "        m = re.search(r\"(\\[.*\\])\", out, flags=re.S)\n",
        "        if m:\n",
        "            candidate = m.group(1)\n",
        "            try:\n",
        "                parsed = json.loads(candidate)\n",
        "                json_text = candidate\n",
        "            except Exception:\n",
        "                parsed = None\n",
        "        else:\n",
        "            parsed = None\n",
        "\n",
        "    if parsed and isinstance(parsed, list):\n",
        "        items = []\n",
        "        for item in parsed[:n_questions]:\n",
        "            if (\n",
        "                isinstance(item, dict) and\n",
        "                \"question\" in item and\n",
        "                \"options\" in item and\n",
        "                \"answer_index\" in item and\n",
        "                \"explanation\" in item and\n",
        "                isinstance(item[\"options\"], list) and\n",
        "                len(item[\"options\"]) == 4\n",
        "            ):\n",
        "                items.append({\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"options\": item[\"options\"],\n",
        "                    \"answer_index\": int(item[\"answer_index\"]),\n",
        "                    \"explanation\": item[\"explanation\"]\n",
        "                })\n",
        "        if items:\n",
        "            return items\n",
        "\n",
        "    # Retry once with a stricter JSON-only instruction\n",
        "    retry_prompt = [\n",
        "        {\"role\":\"system\",\"content\":\"You must return ONLY a pure JSON array and nothing else.\"},\n",
        "        {\"role\":\"user\",\"content\": (\n",
        "            f\"The previous output was not parseable. Now return EXACTLY a JSON array with {n_questions} objects of the form:\\n\"\n",
        "            '{\"question\":\"...\",\"options\":[\"A\",\"B\",\"C\",\"D\"],\"answer_index\":0,\"explanation\":\"...\"}\\n\\n'\n",
        "            f\"TRANSCRIPT:\\n{(transcript[:20000] + '...') if len(transcript) > 20000 else transcript}\"\n",
        "        )}\n",
        "    ]\n",
        "    out2 = call_llm_prompt(retry_prompt)\n",
        "    print(\"RAW QUIZ OUTPUT (retry):\\n\", out2[:4000])\n",
        "    try:\n",
        "        parsed2 = json.loads(out2)\n",
        "        items2 = []\n",
        "        for item in parsed2[:n_questions]:\n",
        "            if (\n",
        "                isinstance(item, dict) and\n",
        "                \"question\" in item and\n",
        "                \"options\" in item and\n",
        "                \"answer_index\" in item and\n",
        "                \"explanation\" in item and\n",
        "                isinstance(item[\"options\"], list) and\n",
        "                len(item[\"options\"]) == 4\n",
        "            ):\n",
        "                items2.append({\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"options\": item[\"options\"],\n",
        "                    \"answer_index\": int(item[\"answer_index\"]),\n",
        "                    \"explanation\": item[\"explanation\"]\n",
        "                })\n",
        "        if items2:\n",
        "            return items2\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # As a last resort, synthesize quiz from key concepts\n",
        "    try:\n",
        "        concepts = extract_key_concepts(transcript, n_concepts= max(3, n_questions))\n",
        "        fallback = _synthesize_quiz_from_concepts(concepts, n_questions)\n",
        "        if fallback:\n",
        "            print(\"FALLBACK: generated quiz from key concepts.\")\n",
        "            return fallback\n",
        "    except Exception as e:\n",
        "        print(\"Fallback synthesis failed:\", e)\n",
        "\n",
        "    return []\n",
        "\n",
        "def analyze_video(url: str, language:str=\"en\", n_concepts:int=KEY_CONCEPTS_COUNT, n_quiz:int=QUIZ_QUESTIONS) -> Dict[str,Any]:\n",
        "    vid_id = extract_video_id.run(url)\n",
        "    if vid_id.startswith(\"Error\"):\n",
        "        raise ValueError(f\"Could not extract video id: {vid_id}\")\n",
        "    transcript = fetch_transcript.run(vid_id, language=language)\n",
        "    if transcript.startswith(\"Error\"):\n",
        "        raise RuntimeError(f\"Error fetching transcript: {transcript}\")\n",
        "    summary = summarize_transcript(transcript)\n",
        "    concepts = extract_key_concepts(transcript, n_concepts)\n",
        "    quiz = generate_quiz(transcript, n_quiz)\n",
        "    return {\n",
        "        \"video_id\": vid_id,\n",
        "        \"transcript\": transcript,\n",
        "        \"summary\": summary,\n",
        "        \"key_concepts\": concepts,\n",
        "        \"quiz\": quiz\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
        "        print(\"ERROR: OPENAI_API_KEY not set. In Colab do:\\n\"\n",
        "              \"from google.colab import userdata\\n\"\n",
        "              \"import os\\n\"\n",
        "              \"os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\\n\")\n",
        "        sys.exit(1)\n",
        "    test_url = \"https://www.youtube.com/watch?v=x7X9w_GIm1s\"\n",
        "    print(\"Analyzing\", test_url)\n",
        "    result = analyze_video(test_url)\n",
        "    print(\"\\nVIDEO ID:\", result[\"video_id\"])\n",
        "    print(\"\\n--- SUMMARY ---\\n\", result[\"summary\"])\n",
        "    print(\"\\n--- KEY CONCEPTS ---\")\n",
        "    for i,c in enumerate(result[\"key_concepts\"],1):\n",
        "        print(f\"{i}. {c['concept']}: {c['explanation']}\")\n",
        "    print(\"\\n--- QUIZ ---\")\n",
        "    for i,q in enumerate(result[\"quiz\"],1):\n",
        "        print(f\"\\nQ{i}: {q['question']}\")\n",
        "        for idx,opt in enumerate(q['options']):\n",
        "            print(f\"  {['A','B','C','D'][idx]}. {opt}\")\n",
        "        print(\"Answer:\", ['A','B','C','D'][q['answer_index']])\n",
        "        print(\"Explanation:\", q[\"explanation\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
