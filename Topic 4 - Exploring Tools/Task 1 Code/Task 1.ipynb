{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S0xX8KUqKGR"
      },
      "source": [
        "Part 1 - Running the code for toolnode_example.py and react_agent_example.py and analyzing the mermaid graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEHgrwIxx1z4",
        "outputId": "6fcc7f7a-caef-4308-f5ed-2bf7f1ae7c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.10-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.13 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.2.13)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.21.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (4.67.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=2.20.0->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain_openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.10-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-1.1.10\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-w3Fe2wsR85"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcfhFCkFqJIe",
        "outputId": "6c3d893c-ae07-4415-9b02-6adb56c28509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LangGraph Manual Tool Calling - Persistent Multi-Turn Conversation\n",
            "================================================================================\n",
            "\n",
            "This system uses manual tool calling with ToolNode:\n",
            "  - call_model: Invokes LLM with tool bindings\n",
            "  - ToolNode: Executes requested tools in parallel\n",
            "  - Loop: tools -> call_model (until no more tools needed)\n",
            "  - Single persistent conversation across all turns\n",
            "  - History managed automatically (trimmed after 100 messages)\n",
            "\n",
            "Commands:\n",
            "  - Type 'quit' or 'exit' to end the conversation\n",
            "  - Type 'verbose' to enable detailed tracing\n",
            "  - Type 'quiet' to disable detailed tracing\n",
            "\n",
            "Available tools:\n",
            "  - get_weather(location): Get weather information\n",
            "  - get_population(city): Get population data\n",
            "  - calculate(expression): Evaluate math expressions\n",
            "================================================================================\n",
            "[SYSTEM] Conversation graph created successfully (using manual ToolNode)\n",
            "[SYSTEM] Graph visualization saved to 'langchain_manual_tool_graph.png'\n",
            "\n",
            "[SYSTEM] Starting conversation...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: Weather in New York\n",
            "[DEBUG] User input: Weather in New York\n",
            "[DEBUG] Routing to call_model\n",
            "\n",
            "================================================================================\n",
            "NODE: call_model\n",
            "================================================================================\n",
            "[DEBUG] Calling model with 1 messages\n",
            "[DEBUG] Added system prompt\n",
            "[DEBUG] Model requested 1 tool call(s):\n",
            "  - get_weather({'location': 'New York'})\n",
            "[DEBUG] Routing to tools\n",
            "\n",
            "================================================================================\n",
            "NODE: call_model\n",
            "================================================================================\n",
            "[DEBUG] Calling model with 4 messages\n",
            "[DEBUG] Added system prompt\n",
            "[DEBUG] Model response (no tools): The current weather in New York is sunny, with a temperature of 72°F and light winds....\n",
            "[DEBUG] Routing to output\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: The current weather in New York is sunny, with a temperature of 72°F and light winds.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: exit\n",
            "[DEBUG] Exit command received\n",
            "[DEBUG] Routing to END (exit requested)\n",
            "\n",
            "[SYSTEM] Conversation ended. Goodbye!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "LangGraph Multi-Agent System with Manual ToolNode Implementation\n",
        "\n",
        "This program demonstrates a LangGraph application with manual tool calling:\n",
        "- A single persistent conversation across multiple turns\n",
        "- Manual tool calling loop with ToolNode (no create_react_agent)\n",
        "- Graph-based looping (no Python loops or checkpointing)\n",
        "- Automatic conversation history management (trimming after 100 messages)\n",
        "- Verbose debugging output\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# ============================================================================\n",
        "# STATE DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationState(TypedDict):\n",
        "    \"\"\"\n",
        "    State schema for the conversation.\n",
        "\n",
        "    Attributes:\n",
        "        messages: Full conversation history with automatic message merging\n",
        "        verbose: Controls detailed tracing output\n",
        "        command: Special command from user (exit, verbose, quiet, or None)\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    verbose: bool\n",
        "    command: str  # \"exit\", \"verbose\", \"quiet\", or None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TOOL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "async def get_weather(location: str) -> str:\n",
        "    \"\"\"\n",
        "    Get current weather information for a specified location.\n",
        "\n",
        "    Args:\n",
        "        location: City name or location string\n",
        "\n",
        "    Returns:\n",
        "        Weather description string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"Weather in {location}: Sunny, 72Â°F with light winds\"\n",
        "\n",
        "\n",
        "@tool\n",
        "async def get_population(city: str) -> str:\n",
        "    \"\"\"\n",
        "    Get population information for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city: City name\n",
        "\n",
        "    Returns:\n",
        "        Population information string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"Population of {city}: Approximately 1 million people\"\n",
        "\n",
        "\n",
        "@tool\n",
        "async def calculate(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression.\n",
        "\n",
        "    Args:\n",
        "        expression: Mathematical expression to evaluate (e.g., \"2 + 2\")\n",
        "\n",
        "    Returns:\n",
        "        Result of the calculation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Safe evaluation of simple math expressions\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating: {str(e)}\"\n",
        "\n",
        "\n",
        "# List of all available tools\n",
        "tools = [get_weather, get_population, calculate]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# NODE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def input_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Get input from the user and add it to the conversation.\n",
        "\n",
        "    This node:\n",
        "    - Prompts the user for input\n",
        "    - Handles special commands (quit, exit, verbose, quiet)\n",
        "    - Adds user message to conversation history (for real messages only)\n",
        "    - Sets command field for special commands\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new user message or command\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: input_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Get user input\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    # Handle exit commands\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Exit command received\")\n",
        "        # Set command field, don't add to messages\n",
        "        return {\"command\": \"exit\"}\n",
        "\n",
        "    # Handle verbose toggle\n",
        "    if user_input.lower() == \"verbose\":\n",
        "        print(\"[SYSTEM] Verbose mode enabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"verbose\", \"verbose\": True}\n",
        "\n",
        "    if user_input.lower() == \"quiet\":\n",
        "        print(\"[SYSTEM] Verbose mode disabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"quiet\", \"verbose\": False}\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(f\"[DEBUG] User input: {user_input}\")\n",
        "\n",
        "    # Clear command field and add message\n",
        "    return {\"command\": None, \"messages\": [HumanMessage(content=user_input)]}\n",
        "\n",
        "\n",
        "def call_model(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Call the LLM with tools bound.\n",
        "\n",
        "    This node:\n",
        "    - Prepends system message if not already present\n",
        "    - Invokes the model with tool bindings\n",
        "    - Returns the model's response (may include tool_calls)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with model response\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: call_model\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"[DEBUG] Calling model with {len(state['messages'])} messages\")\n",
        "\n",
        "    messages = list(state[\"messages\"])\n",
        "\n",
        "    # Add system message if not present\n",
        "    # Check if first message is a SystemMessage\n",
        "    system_added = False\n",
        "    if not messages or not isinstance(messages[0], SystemMessage):\n",
        "        system_prompt = SystemMessage(\n",
        "            content=\"You are a helpful assistant. \"\n",
        "                   \"If a tool is able to solve a problem you are working on then \"\n",
        "                   \"always use it, even if you are able to solve it without using a tool.\"\n",
        "        )\n",
        "        messages = [system_prompt] + messages\n",
        "        system_added = True\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Added system prompt\")\n",
        "\n",
        "    # Initialize model with tools\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "    model_with_tools = model.bind_tools(tools)\n",
        "\n",
        "    # Invoke the model\n",
        "    response = model_with_tools.invoke(messages)\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
        "            print(f\"[DEBUG] Model requested {len(response.tool_calls)} tool call(s):\")\n",
        "            for tc in response.tool_calls:\n",
        "                print(f\"  - {tc['name']}({tc['args']})\")\n",
        "        else:\n",
        "            print(f\"[DEBUG] Model response (no tools): {response.content[:100]}...\")\n",
        "\n",
        "    # Return the system message if we added it, plus the response\n",
        "    # This ensures the system message is persisted in the conversation state\n",
        "    if system_added:\n",
        "        return {\"messages\": [messages[0], response]}  # system prompt + model response\n",
        "    else:\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def output_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Display the assistant's final response to the user.\n",
        "\n",
        "    This node:\n",
        "    - Extracts the last AI message from the conversation\n",
        "    - Prints it to the console\n",
        "    - Returns empty dict (no state changes)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Empty dict (no state modifications)\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: output_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Find the last AI message in the conversation\n",
        "    # (there may be tool messages mixed in)\n",
        "    last_ai_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, AIMessage) and msg.content:\n",
        "            last_ai_message = msg\n",
        "            break\n",
        "\n",
        "    if last_ai_message:\n",
        "        print(f\"\\nAssistant: {last_ai_message.content}\")\n",
        "    else:\n",
        "        print(\"\\n[WARNING] No assistant response found\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "def trim_history(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Manage conversation history length to prevent unlimited growth.\n",
        "\n",
        "    Strategy:\n",
        "    - Keep the system message (if present)\n",
        "    - Keep the most recent 100 messages\n",
        "    - This allows ~50 conversation turns (user + assistant pairs)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with trimmed message history (if needed)\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_messages = 100\n",
        "\n",
        "    # Only trim if we've exceeded the limit\n",
        "    if len(messages) > max_messages:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(f\"\\n[DEBUG] History length: {len(messages)} messages\")\n",
        "            print(f\"[DEBUG] Trimming to most recent {max_messages} messages\")\n",
        "\n",
        "        # Preserve system message if it exists at the start\n",
        "        if messages and isinstance(messages[0], SystemMessage):\n",
        "            # Keep system message + last (max_messages - 1) messages\n",
        "            trimmed = [messages[0]] + list(messages[-(max_messages - 1):])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Preserved system message + {max_messages - 1} recent messages\")\n",
        "        else:\n",
        "            # Just keep the last max_messages\n",
        "            trimmed = list(messages[-max_messages:])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Kept {max_messages} most recent messages\")\n",
        "\n",
        "        return {\"messages\": trimmed}\n",
        "\n",
        "    # No trimming needed\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTING LOGIC\n",
        "# ============================================================================\n",
        "\n",
        "def route_after_input(state: ConversationState) -> Literal[\"call_model\", \"end\", \"input\"]:\n",
        "    \"\"\"\n",
        "    Determine where to route after input based on command field.\n",
        "\n",
        "    Logic:\n",
        "    - If command is \"exit\", route to END\n",
        "    - If command is \"verbose\" or \"quiet\", route back to input\n",
        "    - Otherwise (command is None), route to call_model\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"end\" to terminate, \"input\" for verbose toggle, \"call_model\" to continue\n",
        "    \"\"\"\n",
        "    command = state.get(\"command\")\n",
        "\n",
        "    # Check for exit command\n",
        "    if command == \"exit\":\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to END (exit requested)\")\n",
        "        return \"end\"\n",
        "\n",
        "    # Check for verbose toggle commands - route back to input\n",
        "    if command in [\"verbose\", \"quiet\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing back to input (verbose toggle)\")\n",
        "        return \"input\"\n",
        "\n",
        "    # Normal message - route to model\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to call_model\")\n",
        "    return \"call_model\"\n",
        "\n",
        "\n",
        "def route_after_model(state: ConversationState) -> Literal[\"tools\", \"output\"]:\n",
        "    \"\"\"\n",
        "    Route after model call based on whether tools were requested.\n",
        "\n",
        "    Logic:\n",
        "    - If the model's response includes tool_calls, route to tools\n",
        "    - Otherwise, route to output to display the response\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"tools\" if tools requested, \"output\" otherwise\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    # Check if the last message has tool calls\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to tools\")\n",
        "        return \"tools\"\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to output\")\n",
        "    return \"output\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# GRAPH CONSTRUCTION\n",
        "# ============================================================================\n",
        "\n",
        "def create_conversation_graph():\n",
        "    \"\"\"\n",
        "    Build the conversation graph with manual tool calling using ToolNode.\n",
        "\n",
        "    Graph structure (single conversation with looping):\n",
        "\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚                                                      â”‚\n",
        "        â–¼                                                      â”‚\n",
        "      input_node â”€â”€(check command)â”€â”€> call_model              â”‚\n",
        "          â–²                              â”‚                     â”‚\n",
        "          â”‚                              â”œâ”€â”€(has tools)â”€â”€> tools\n",
        "          â”‚                              â”‚                    â”‚\n",
        "          â”‚                              â”‚                    â”‚\n",
        "          â”‚                              â””â”€â”€(no tools)â”€â”€> output_node\n",
        "          â”‚                                                    â”‚\n",
        "          â”‚                                                    â–¼\n",
        "          â””â”€â”€â”€(verbose/quiet)                           trim_history â”€â”€â”˜\n",
        "\n",
        "          â””â”€â”€â”€â”€â”€(exit)â”€â”€> END\n",
        "\n",
        "    Key features:\n",
        "    - Manual tool calling with ToolNode (no create_react_agent)\n",
        "    - Command field used for special commands (no sentinel messages!)\n",
        "    - Single conversation maintained in state.messages\n",
        "    - Graph loops back to input_node after each turn\n",
        "    - Tools route back to call_model for continued reasoning\n",
        "    - History automatically trimmed when it grows too long\n",
        "\n",
        "    Returns:\n",
        "        Compiled LangGraph application\n",
        "    \"\"\"\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the Conversation Graph\n",
        "    # ========================================================================\n",
        "\n",
        "    workflow = StateGraph(ConversationState)\n",
        "\n",
        "    # Create ToolNode to handle tool execution\n",
        "    # ToolNode automatically executes tools in parallel when possible\n",
        "    tool_node = ToolNode(tools)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"input\", input_node)\n",
        "    workflow.add_node(\"call_model\", call_model)\n",
        "    workflow.add_node(\"tools\", tool_node)  # ToolNode handles tool execution\n",
        "    workflow.add_node(\"output\", output_node)\n",
        "    workflow.add_node(\"trim_history\", trim_history)\n",
        "\n",
        "    # Set entry point - conversation always starts at input\n",
        "    workflow.set_entry_point(\"input\")\n",
        "\n",
        "    # Add conditional edge from input based on command field\n",
        "    # Check if user wants to exit, toggle verbose, or continue\n",
        "    workflow.add_conditional_edges(\n",
        "        \"input\",\n",
        "        route_after_input,\n",
        "        {\n",
        "            \"call_model\": \"call_model\",\n",
        "            \"input\": \"input\",  # Loop back for verbose/quiet\n",
        "            \"end\": END\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add conditional edge from call_model\n",
        "    # Check if tools were requested or if we have final response\n",
        "    workflow.add_conditional_edges(\n",
        "        \"call_model\",\n",
        "        route_after_model,\n",
        "        {\n",
        "            \"tools\": \"tools\",\n",
        "            \"output\": \"output\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # After tools execute, loop back to call_model\n",
        "    # This allows the model to see tool results and decide next action\n",
        "    workflow.add_edge(\"tools\", \"call_model\")\n",
        "\n",
        "    # After output, trim history and loop back to input\n",
        "    workflow.add_edge(\"output\", \"trim_history\")\n",
        "    workflow.add_edge(\"trim_history\", \"input\")  # This creates the conversation loop!\n",
        "\n",
        "    # Compile the graph\n",
        "    print(\"[SYSTEM] Conversation graph created successfully (using manual ToolNode)\")\n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graph(app):\n",
        "    \"\"\"\n",
        "    Generate Mermaid diagram of the conversation graph.\n",
        "\n",
        "    Creates:\n",
        "    - langchain_manual_tool_graph.png: The conversation loop with manual tool calling\n",
        "\n",
        "    Args:\n",
        "        app: Compiled conversation graph\n",
        "    \"\"\"\n",
        "    try:\n",
        "        graph_png = app.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_manual_tool_graph.png\", \"wb\") as f:\n",
        "            f.write(graph_png)\n",
        "        print(\"[SYSTEM] Graph visualization saved to 'langchain_manual_tool_graph.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate graph visualization: {e}\")\n",
        "        print(\"You may need to install: pip install pygraphviz or pip install grandalf\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    This function:\n",
        "    1. Creates the conversation graph\n",
        "    2. Visualizes the graph structure\n",
        "    3. Initializes the conversation state\n",
        "    4. Invokes the graph ONCE\n",
        "\n",
        "    The graph then runs indefinitely via internal looping (trim_history -> input)\n",
        "    until the user types 'quit' or 'exit'.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LangGraph Manual Tool Calling - Persistent Multi-Turn Conversation\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nThis system uses manual tool calling with ToolNode:\")\n",
        "    print(\"  - call_model: Invokes LLM with tool bindings\")\n",
        "    print(\"  - ToolNode: Executes requested tools in parallel\")\n",
        "    print(\"  - Loop: tools -> call_model (until no more tools needed)\")\n",
        "    print(\"  - Single persistent conversation across all turns\")\n",
        "    print(\"  - History managed automatically (trimmed after 100 messages)\")\n",
        "    print(\"\\nCommands:\")\n",
        "    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n",
        "    print(\"  - Type 'verbose' to enable detailed tracing\")\n",
        "    print(\"  - Type 'quiet' to disable detailed tracing\")\n",
        "    print(\"\\nAvailable tools:\")\n",
        "    print(\"  - get_weather(location): Get weather information\")\n",
        "    print(\"  - get_population(city): Get population data\")\n",
        "    print(\"  - calculate(expression): Evaluate math expressions\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the conversation graph\n",
        "    app = create_conversation_graph()\n",
        "\n",
        "    # Visualize the graph\n",
        "    visualize_graph(app)\n",
        "\n",
        "    # Initialize conversation state\n",
        "    # This state persists across all turns via graph looping\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"verbose\": True,\n",
        "        \"command\": None\n",
        "    }\n",
        "\n",
        "    print(\"\\n[SYSTEM] Starting conversation...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Invoke the graph ONCE\n",
        "        # The graph will loop internally until user exits\n",
        "        # Each iteration: input -> call_model -> [tools -> call_model]* -> output -> trim -> input\n",
        "        # Verbose commands: input -> input (direct loop!)\n",
        "        await app.ainvoke(initial_state)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n[SYSTEM] Interrupted by user (Ctrl+C)\")\n",
        "\n",
        "    print(\"\\n[SYSTEM] Conversation ended. Goodbye!\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vp4lunnuVSF",
        "outputId": "d7147d0a-fc29-4fa1-c86a-0086f57a74f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LangGraph ReAct Agent - Persistent Multi-Turn Conversation\n",
            "================================================================================\n",
            "\n",
            "This system uses create_react_agent with graph-based looping:\n",
            "  - Single persistent conversation across all turns\n",
            "  - History managed automatically (trimmed after 100 messages)\n",
            "  - Loops via graph edges (no Python loops or checkpointing)\n",
            "\n",
            "Commands:\n",
            "  - Type 'quit' or 'exit' to end the conversation\n",
            "  - Type 'verbose' to enable detailed tracing\n",
            "  - Type 'quiet' to disable detailed tracing\n",
            "\n",
            "Available tools:\n",
            "  - get_weather(location): Get weather information\n",
            "  - get_population(city): Get population data\n",
            "  - calculate(expression): Evaluate math expressions\n",
            "================================================================================\n",
            "[SYSTEM] ReAct agent created successfully\n",
            "[SYSTEM] ReAct agent graph saved to 'langchain_react_agent.png'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-745513083.py:375: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  react_agent = create_react_agent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SYSTEM] Conversation graph saved to 'langchain_conversation_graph.png'\n",
            "\n",
            "[SYSTEM] Starting conversation...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: Population in New York\n",
            "[DEBUG] User input: Population in New York\n",
            "[DEBUG] Routing to call_react_agent\n",
            "\n",
            "================================================================================\n",
            "NODE: call_react_agent\n",
            "================================================================================\n",
            "[DEBUG] Invoking ReAct agent with 1 messages in history\n",
            "[DEBUG] Agent generated 3 new messages\n",
            "[DEBUG] Tool calls: ['get_population']\n",
            "[DEBUG] Response preview: The population of New York is approximately 1 million people....\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: The population of New York is approximately 1 million people.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: exit\n",
            "[DEBUG] Exit command received\n",
            "[DEBUG] Routing to END (exit requested)\n",
            "\n",
            "[SYSTEM] Conversation ended. Goodbye!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "LangGraph ReAct Agent with Persistent Multi-Turn Conversation\n",
        "\n",
        "This program demonstrates a LangGraph application using create_react_agent with:\n",
        "- A single persistent conversation across multiple turns\n",
        "- Graph-based looping (no Python loops or checkpointing)\n",
        "- Automatic conversation history management (trimming after 100 messages)\n",
        "- Verbose debugging output\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "import time\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# ============================================================================\n",
        "# STATE DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationState(TypedDict):\n",
        "    \"\"\"\n",
        "    State schema for the conversation.\n",
        "\n",
        "    Attributes:\n",
        "        messages: Full conversation history with automatic message merging\n",
        "        verbose: Controls detailed tracing output\n",
        "        command: Special command from user (exit, verbose, quiet, or None)\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    verbose: bool\n",
        "    command: str  # \"exit\", \"verbose\", \"quiet\", or None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TOOL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"\n",
        "    Get current weather information for a specified location.\n",
        "\n",
        "    Args:\n",
        "        location: City name or location string\n",
        "\n",
        "    Returns:\n",
        "        Weather description string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    time.sleep(0.5)\n",
        "    return f\"Weather in {location}: Sunny, 72Â°F with light winds\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_population(city: str) -> str:\n",
        "    \"\"\"\n",
        "    Get population information for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city: City name\n",
        "\n",
        "    Returns:\n",
        "        Population information string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    time.sleep(0.5)\n",
        "    return f\"Population of {city}: Approximately 1 million people\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression.\n",
        "\n",
        "    Args:\n",
        "        expression: Mathematical expression to evaluate (e.g., \"2 + 2\")\n",
        "\n",
        "    Returns:\n",
        "        Result of the calculation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Safe evaluation of simple math expressions\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating: {str(e)}\"\n",
        "\n",
        "\n",
        "# List of all available tools\n",
        "tools = [get_weather, get_population, calculate]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# NODE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def input_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Get input from the user and add it to the conversation.\n",
        "\n",
        "    This node:\n",
        "    - Prompts the user for input\n",
        "    - Handles special commands (quit, exit, verbose, quiet)\n",
        "    - Adds user message to conversation history (for real messages only)\n",
        "    - Sets command field for special commands\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new user message or command\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: input_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Get user input\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    # Handle exit commands\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Exit command received\")\n",
        "        # Set command field, don't add to messages\n",
        "        return {\"command\": \"exit\"}\n",
        "\n",
        "    # Handle verbose toggle\n",
        "    if user_input.lower() == \"verbose\":\n",
        "        print(\"[SYSTEM] Verbose mode enabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"verbose\", \"verbose\": True}\n",
        "\n",
        "    if user_input.lower() == \"quiet\":\n",
        "        print(\"[SYSTEM] Verbose mode disabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"quiet\", \"verbose\": False}\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(f\"[DEBUG] User input: {user_input}\")\n",
        "\n",
        "    # Clear command field and add message\n",
        "    return {\"command\": None, \"messages\": [HumanMessage(content=user_input)]}\n",
        "\n",
        "\n",
        "def call_react_agent(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Invoke the ReAct agent with the current conversation history.\n",
        "\n",
        "    This node:\n",
        "    - Takes the full conversation history from state\n",
        "    - Invokes the ReAct agent (which handles tool calling internally)\n",
        "    - Returns only the NEW messages generated by the agent\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with agent's response messages\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: call_react_agent\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"[DEBUG] Invoking ReAct agent with {len(state['messages'])} messages in history\")\n",
        "\n",
        "    # Get the global react_agent\n",
        "    global react_agent\n",
        "\n",
        "    # Count messages before agent call\n",
        "    messages_before = len(state[\"messages\"])\n",
        "\n",
        "    # Invoke the ReAct agent with full conversation history\n",
        "    # The agent maintains context across all previous turns\n",
        "    result = react_agent.invoke({\"messages\": state[\"messages\"]})\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        messages_after = len(result[\"messages\"])\n",
        "        new_message_count = messages_after - messages_before\n",
        "        print(f\"[DEBUG] Agent generated {new_message_count} new messages\")\n",
        "\n",
        "        # Show what the agent did\n",
        "        for msg in result[\"messages\"][messages_before:]:\n",
        "            if isinstance(msg, AIMessage):\n",
        "                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "                    print(f\"[DEBUG] Tool calls: {[tc['name'] for tc in msg.tool_calls]}\")\n",
        "                elif msg.content:\n",
        "                    print(f\"[DEBUG] Response preview: {msg.content[:100]}...\")\n",
        "\n",
        "    # Return only the NEW messages (everything after what we sent)\n",
        "    new_messages = result[\"messages\"][messages_before:]\n",
        "    return {\"messages\": new_messages}\n",
        "\n",
        "\n",
        "def output_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Display the assistant's final response to the user.\n",
        "\n",
        "    This node:\n",
        "    - Extracts the last AI message from the conversation\n",
        "    - Prints it to the console\n",
        "    - Returns empty dict (no state changes)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Empty dict (no state modifications)\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: output_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Find the last AI message in the conversation\n",
        "    # (there may be tool messages mixed in)\n",
        "    last_ai_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, AIMessage) and msg.content:\n",
        "            last_ai_message = msg\n",
        "            break\n",
        "\n",
        "    if last_ai_message:\n",
        "        print(f\"\\nAssistant: {last_ai_message.content}\")\n",
        "    else:\n",
        "        print(\"\\n[WARNING] No assistant response found\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "def trim_history(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Manage conversation history length to prevent unlimited growth.\n",
        "\n",
        "    Strategy:\n",
        "    - Keep the system message (if present)\n",
        "    - Keep the most recent 100 messages\n",
        "    - This allows ~50 conversation turns (user + assistant pairs)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with trimmed message history (if needed)\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_messages = 100\n",
        "\n",
        "    # Only trim if we've exceeded the limit\n",
        "    if len(messages) > max_messages:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(f\"\\n[DEBUG] History length: {len(messages)} messages\")\n",
        "            print(f\"[DEBUG] Trimming to most recent {max_messages} messages\")\n",
        "\n",
        "        # Preserve system message if it exists at the start\n",
        "        if messages and isinstance(messages[0], SystemMessage):\n",
        "            # Keep system message + last (max_messages - 1) messages\n",
        "            trimmed = [messages[0]] + list(messages[-(max_messages - 1):])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Preserved system message + {max_messages - 1} recent messages\")\n",
        "        else:\n",
        "            # Just keep the last max_messages\n",
        "            trimmed = list(messages[-max_messages:])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Kept {max_messages} most recent messages\")\n",
        "\n",
        "        return {\"messages\": trimmed}\n",
        "\n",
        "    # No trimming needed\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTING LOGIC\n",
        "# ============================================================================\n",
        "\n",
        "def route_after_input(state: ConversationState) -> Literal[\"call_react_agent\", \"end\", \"input\"]:\n",
        "    \"\"\"\n",
        "    Determine where to route after input based on command field.\n",
        "\n",
        "    Logic:\n",
        "    - If command is \"exit\", route to END\n",
        "    - If command is \"verbose\" or \"quiet\", route back to input\n",
        "    - Otherwise (command is None), route to the ReAct agent\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"end\" to terminate, \"input\" for verbose toggle, \"call_react_agent\" to continue\n",
        "    \"\"\"\n",
        "    command = state.get(\"command\")\n",
        "\n",
        "    # Check for exit command\n",
        "    if command == \"exit\":\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to END (exit requested)\")\n",
        "        return \"end\"\n",
        "\n",
        "    # Check for verbose toggle commands - route back to input\n",
        "    if command in [\"verbose\", \"quiet\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing back to input (verbose toggle)\")\n",
        "        return \"input\"\n",
        "\n",
        "    # Normal message - route to agent\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to call_react_agent\")\n",
        "    return \"call_react_agent\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# GRAPH CONSTRUCTION\n",
        "# ============================================================================\n",
        "\n",
        "# Global variable to hold the ReAct agent\n",
        "react_agent = None\n",
        "\n",
        "def create_conversation_graph():\n",
        "    \"\"\"\n",
        "    Build the conversation graph with persistent multi-turn capability.\n",
        "\n",
        "    Graph structure (single conversation with looping):\n",
        "\n",
        "        Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â\n",
        "        Ã¢â€â€š                                                      Ã¢â€â€š\n",
        "        Ã¢â€“Â¼                                                      Ã¢â€â€š\n",
        "      input_node Ã¢â€â‚¬Ã¢â€â‚¬(check command)Ã¢â€â‚¬Ã¢â€â‚¬> call_react_agent        Ã¢â€â€š\n",
        "          Ã¢â€“Â²                              Ã¢â€â€š                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€“Â¼                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                         output_node                Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€â€š                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€“Â¼                     Ã¢â€â€š\n",
        "          Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬(verbose/quiet)       trim_history Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ\n",
        "\n",
        "          Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬(exit)Ã¢â€â‚¬Ã¢â€â‚¬> END\n",
        "\n",
        "    Key features:\n",
        "    - Single conversation maintained in state.messages\n",
        "    - Command field used for special commands (no sentinel messages!)\n",
        "    - Graph loops back to input_node after each turn\n",
        "    - Verbose/quiet commands route directly back to input\n",
        "    - History automatically trimmed when it grows too long\n",
        "    - No Python loops or checkpointing needed\n",
        "\n",
        "    Returns:\n",
        "        Compiled LangGraph application\n",
        "    \"\"\"\n",
        "    global react_agent\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the ReAct Agent\n",
        "    # ========================================================================\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # System message to encourage tool usage\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"If a tool is able to solve a problem you are working on then \"\n",
        "        \"always use it, even if you are able to solve it without using a tool.\"\n",
        "    )\n",
        "\n",
        "    # Create the ReAct agent using the built-in function\n",
        "    # This agent handles the thought/action/observation loop internally\n",
        "    react_agent = create_react_agent(\n",
        "        model=model,\n",
        "        tools=tools,\n",
        "        prompt=system_message\n",
        "    )\n",
        "\n",
        "    print(\"[SYSTEM] ReAct agent created successfully\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the Conversation Wrapper Graph\n",
        "    # ========================================================================\n",
        "\n",
        "    workflow = StateGraph(ConversationState)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"input\", input_node)\n",
        "    workflow.add_node(\"call_react_agent\", call_react_agent)\n",
        "    workflow.add_node(\"output\", output_node)\n",
        "    workflow.add_node(\"trim_history\", trim_history)\n",
        "\n",
        "    # Set entry point - conversation always starts at input\n",
        "    workflow.set_entry_point(\"input\")\n",
        "\n",
        "    # Add conditional edge from input based on command field\n",
        "    workflow.add_conditional_edges(\n",
        "        \"input\",\n",
        "        route_after_input,\n",
        "        {\n",
        "            \"call_react_agent\": \"call_react_agent\",\n",
        "            \"input\": \"input\",  # Loop back for verbose/quiet\n",
        "            \"end\": END\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add linear edges for the main conversation flow\n",
        "    # Agent -> Output -> Trim -> Input (loops back!)\n",
        "    workflow.add_edge(\"call_react_agent\", \"output\")\n",
        "    workflow.add_edge(\"output\", \"trim_history\")\n",
        "    workflow.add_edge(\"trim_history\", \"input\")  # This creates the loop!\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graphs(wrapper_app):\n",
        "    \"\"\"\n",
        "    Generate Mermaid diagrams for both graphs.\n",
        "\n",
        "    Creates:\n",
        "    - langchain_react_agent.png: Internal ReAct agent (thought/action/observation)\n",
        "    - langchain_conversation_graph.png: Conversation loop wrapper\n",
        "\n",
        "    Args:\n",
        "        wrapper_app: Compiled conversation graph\n",
        "    \"\"\"\n",
        "    global react_agent\n",
        "\n",
        "    # Visualize the ReAct agent\n",
        "    try:\n",
        "        react_png = react_agent.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_react_agent.png\", \"wb\") as f:\n",
        "            f.write(react_png)\n",
        "        print(\"[SYSTEM] ReAct agent graph saved to 'langchain_react_agent.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate ReAct agent visualization: {e}\")\n",
        "\n",
        "    # Visualize the conversation wrapper\n",
        "    try:\n",
        "        wrapper_png = wrapper_app.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_conversation_graph.png\", \"wb\") as f:\n",
        "            f.write(wrapper_png)\n",
        "        print(\"[SYSTEM] Conversation graph saved to 'langchain_conversation_graph.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate conversation graph visualization: {e}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    This function:\n",
        "    1. Creates the conversation graph\n",
        "    2. Visualizes the graph structure\n",
        "    3. Initializes the conversation state\n",
        "    4. Invokes the graph ONCE\n",
        "\n",
        "    The graph then runs indefinitely via internal looping (trim_history -> input)\n",
        "    until the user types 'quit' or 'exit'.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LangGraph ReAct Agent - Persistent Multi-Turn Conversation\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nThis system uses create_react_agent with graph-based looping:\")\n",
        "    print(\"  - Single persistent conversation across all turns\")\n",
        "    print(\"  - History managed automatically (trimmed after 100 messages)\")\n",
        "    print(\"  - Loops via graph edges (no Python loops or checkpointing)\")\n",
        "    print(\"\\nCommands:\")\n",
        "    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n",
        "    print(\"  - Type 'verbose' to enable detailed tracing\")\n",
        "    print(\"  - Type 'quiet' to disable detailed tracing\")\n",
        "    print(\"\\nAvailable tools:\")\n",
        "    print(\"  - get_weather(location): Get weather information\")\n",
        "    print(\"  - get_population(city): Get population data\")\n",
        "    print(\"  - calculate(expression): Evaluate math expressions\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the conversation graph\n",
        "    app = create_conversation_graph()\n",
        "\n",
        "    # Visualize both graphs\n",
        "    visualize_graphs(app)\n",
        "\n",
        "    # Initialize conversation state\n",
        "    # This state persists across all turns via graph looping\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"verbose\": True,\n",
        "        \"command\": None\n",
        "    }\n",
        "\n",
        "    print(\"\\n[SYSTEM] Starting conversation...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Invoke the graph ONCE\n",
        "        # The graph will loop internally until user exits\n",
        "        # Each iteration: input -> agent -> output -> trim -> input (loop!)\n",
        "        # Verbose commands: input -> input (direct loop!)\n",
        "        await app.ainvoke(initial_state)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n[SYSTEM] Interrupted by user (Ctrl+C)\")\n",
        "\n",
        "    print(\"\\n[SYSTEM] Conversation ended. Goodbye!\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "await main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
