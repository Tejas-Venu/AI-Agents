{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S0xX8KUqKGR"
      },
      "source": [
        "Part 1 - Running the code for toolnode_example.py and react_agent_example.py and analyzing the mermaid graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEHgrwIxx1z4",
        "outputId": "6fcc7f7a-caef-4308-f5ed-2bf7f1ae7c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.10-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.13 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.2.13)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.21.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (4.67.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=2.20.0->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain_openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.10-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-1.1.10\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-w3Fe2wsR85"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcfhFCkFqJIe",
        "outputId": "6c3d893c-ae07-4415-9b02-6adb56c28509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LangGraph Manual Tool Calling - Persistent Multi-Turn Conversation\n",
            "================================================================================\n",
            "\n",
            "This system uses manual tool calling with ToolNode:\n",
            "  - call_model: Invokes LLM with tool bindings\n",
            "  - ToolNode: Executes requested tools in parallel\n",
            "  - Loop: tools -> call_model (until no more tools needed)\n",
            "  - Single persistent conversation across all turns\n",
            "  - History managed automatically (trimmed after 100 messages)\n",
            "\n",
            "Commands:\n",
            "  - Type 'quit' or 'exit' to end the conversation\n",
            "  - Type 'verbose' to enable detailed tracing\n",
            "  - Type 'quiet' to disable detailed tracing\n",
            "\n",
            "Available tools:\n",
            "  - get_weather(location): Get weather information\n",
            "  - get_population(city): Get population data\n",
            "  - calculate(expression): Evaluate math expressions\n",
            "================================================================================\n",
            "[SYSTEM] Conversation graph created successfully (using manual ToolNode)\n",
            "[SYSTEM] Graph visualization saved to 'langchain_manual_tool_graph.png'\n",
            "\n",
            "[SYSTEM] Starting conversation...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: Weather in New York\n",
            "[DEBUG] User input: Weather in New York\n",
            "[DEBUG] Routing to call_model\n",
            "\n",
            "================================================================================\n",
            "NODE: call_model\n",
            "================================================================================\n",
            "[DEBUG] Calling model with 1 messages\n",
            "[DEBUG] Added system prompt\n",
            "[DEBUG] Model requested 1 tool call(s):\n",
            "  - get_weather({'location': 'New York'})\n",
            "[DEBUG] Routing to tools\n",
            "\n",
            "================================================================================\n",
            "NODE: call_model\n",
            "================================================================================\n",
            "[DEBUG] Calling model with 4 messages\n",
            "[DEBUG] Added system prompt\n",
            "[DEBUG] Model response (no tools): The current weather in New York is sunny, with a temperature of 72°F and light winds....\n",
            "[DEBUG] Routing to output\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: The current weather in New York is sunny, with a temperature of 72°F and light winds.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: exit\n",
            "[DEBUG] Exit command received\n",
            "[DEBUG] Routing to END (exit requested)\n",
            "\n",
            "[SYSTEM] Conversation ended. Goodbye!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "LangGraph Multi-Agent System with Manual ToolNode Implementation\n",
        "\n",
        "This program demonstrates a LangGraph application with manual tool calling:\n",
        "- A single persistent conversation across multiple turns\n",
        "- Manual tool calling loop with ToolNode (no create_react_agent)\n",
        "- Graph-based looping (no Python loops or checkpointing)\n",
        "- Automatic conversation history management (trimming after 100 messages)\n",
        "- Verbose debugging output\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# ============================================================================\n",
        "# STATE DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationState(TypedDict):\n",
        "    \"\"\"\n",
        "    State schema for the conversation.\n",
        "\n",
        "    Attributes:\n",
        "        messages: Full conversation history with automatic message merging\n",
        "        verbose: Controls detailed tracing output\n",
        "        command: Special command from user (exit, verbose, quiet, or None)\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    verbose: bool\n",
        "    command: str  # \"exit\", \"verbose\", \"quiet\", or None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TOOL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "async def get_weather(location: str) -> str:\n",
        "    \"\"\"\n",
        "    Get current weather information for a specified location.\n",
        "\n",
        "    Args:\n",
        "        location: City name or location string\n",
        "\n",
        "    Returns:\n",
        "        Weather description string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"Weather in {location}: Sunny, 72Â°F with light winds\"\n",
        "\n",
        "\n",
        "@tool\n",
        "async def get_population(city: str) -> str:\n",
        "    \"\"\"\n",
        "    Get population information for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city: City name\n",
        "\n",
        "    Returns:\n",
        "        Population information string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"Population of {city}: Approximately 1 million people\"\n",
        "\n",
        "\n",
        "@tool\n",
        "async def calculate(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression.\n",
        "\n",
        "    Args:\n",
        "        expression: Mathematical expression to evaluate (e.g., \"2 + 2\")\n",
        "\n",
        "    Returns:\n",
        "        Result of the calculation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Safe evaluation of simple math expressions\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating: {str(e)}\"\n",
        "\n",
        "\n",
        "# List of all available tools\n",
        "tools = [get_weather, get_population, calculate]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# NODE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def input_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Get input from the user and add it to the conversation.\n",
        "\n",
        "    This node:\n",
        "    - Prompts the user for input\n",
        "    - Handles special commands (quit, exit, verbose, quiet)\n",
        "    - Adds user message to conversation history (for real messages only)\n",
        "    - Sets command field for special commands\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new user message or command\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: input_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Get user input\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    # Handle exit commands\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Exit command received\")\n",
        "        # Set command field, don't add to messages\n",
        "        return {\"command\": \"exit\"}\n",
        "\n",
        "    # Handle verbose toggle\n",
        "    if user_input.lower() == \"verbose\":\n",
        "        print(\"[SYSTEM] Verbose mode enabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"verbose\", \"verbose\": True}\n",
        "\n",
        "    if user_input.lower() == \"quiet\":\n",
        "        print(\"[SYSTEM] Verbose mode disabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"quiet\", \"verbose\": False}\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(f\"[DEBUG] User input: {user_input}\")\n",
        "\n",
        "    # Clear command field and add message\n",
        "    return {\"command\": None, \"messages\": [HumanMessage(content=user_input)]}\n",
        "\n",
        "\n",
        "def call_model(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Call the LLM with tools bound.\n",
        "\n",
        "    This node:\n",
        "    - Prepends system message if not already present\n",
        "    - Invokes the model with tool bindings\n",
        "    - Returns the model's response (may include tool_calls)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with model response\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: call_model\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"[DEBUG] Calling model with {len(state['messages'])} messages\")\n",
        "\n",
        "    messages = list(state[\"messages\"])\n",
        "\n",
        "    # Add system message if not present\n",
        "    # Check if first message is a SystemMessage\n",
        "    system_added = False\n",
        "    if not messages or not isinstance(messages[0], SystemMessage):\n",
        "        system_prompt = SystemMessage(\n",
        "            content=\"You are a helpful assistant. \"\n",
        "                   \"If a tool is able to solve a problem you are working on then \"\n",
        "                   \"always use it, even if you are able to solve it without using a tool.\"\n",
        "        )\n",
        "        messages = [system_prompt] + messages\n",
        "        system_added = True\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Added system prompt\")\n",
        "\n",
        "    # Initialize model with tools\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "    model_with_tools = model.bind_tools(tools)\n",
        "\n",
        "    # Invoke the model\n",
        "    response = model_with_tools.invoke(messages)\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
        "            print(f\"[DEBUG] Model requested {len(response.tool_calls)} tool call(s):\")\n",
        "            for tc in response.tool_calls:\n",
        "                print(f\"  - {tc['name']}({tc['args']})\")\n",
        "        else:\n",
        "            print(f\"[DEBUG] Model response (no tools): {response.content[:100]}...\")\n",
        "\n",
        "    # Return the system message if we added it, plus the response\n",
        "    # This ensures the system message is persisted in the conversation state\n",
        "    if system_added:\n",
        "        return {\"messages\": [messages[0], response]}  # system prompt + model response\n",
        "    else:\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def output_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Display the assistant's final response to the user.\n",
        "\n",
        "    This node:\n",
        "    - Extracts the last AI message from the conversation\n",
        "    - Prints it to the console\n",
        "    - Returns empty dict (no state changes)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Empty dict (no state modifications)\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: output_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Find the last AI message in the conversation\n",
        "    # (there may be tool messages mixed in)\n",
        "    last_ai_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, AIMessage) and msg.content:\n",
        "            last_ai_message = msg\n",
        "            break\n",
        "\n",
        "    if last_ai_message:\n",
        "        print(f\"\\nAssistant: {last_ai_message.content}\")\n",
        "    else:\n",
        "        print(\"\\n[WARNING] No assistant response found\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "def trim_history(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Manage conversation history length to prevent unlimited growth.\n",
        "\n",
        "    Strategy:\n",
        "    - Keep the system message (if present)\n",
        "    - Keep the most recent 100 messages\n",
        "    - This allows ~50 conversation turns (user + assistant pairs)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with trimmed message history (if needed)\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_messages = 100\n",
        "\n",
        "    # Only trim if we've exceeded the limit\n",
        "    if len(messages) > max_messages:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(f\"\\n[DEBUG] History length: {len(messages)} messages\")\n",
        "            print(f\"[DEBUG] Trimming to most recent {max_messages} messages\")\n",
        "\n",
        "        # Preserve system message if it exists at the start\n",
        "        if messages and isinstance(messages[0], SystemMessage):\n",
        "            # Keep system message + last (max_messages - 1) messages\n",
        "            trimmed = [messages[0]] + list(messages[-(max_messages - 1):])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Preserved system message + {max_messages - 1} recent messages\")\n",
        "        else:\n",
        "            # Just keep the last max_messages\n",
        "            trimmed = list(messages[-max_messages:])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Kept {max_messages} most recent messages\")\n",
        "\n",
        "        return {\"messages\": trimmed}\n",
        "\n",
        "    # No trimming needed\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTING LOGIC\n",
        "# ============================================================================\n",
        "\n",
        "def route_after_input(state: ConversationState) -> Literal[\"call_model\", \"end\", \"input\"]:\n",
        "    \"\"\"\n",
        "    Determine where to route after input based on command field.\n",
        "\n",
        "    Logic:\n",
        "    - If command is \"exit\", route to END\n",
        "    - If command is \"verbose\" or \"quiet\", route back to input\n",
        "    - Otherwise (command is None), route to call_model\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"end\" to terminate, \"input\" for verbose toggle, \"call_model\" to continue\n",
        "    \"\"\"\n",
        "    command = state.get(\"command\")\n",
        "\n",
        "    # Check for exit command\n",
        "    if command == \"exit\":\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to END (exit requested)\")\n",
        "        return \"end\"\n",
        "\n",
        "    # Check for verbose toggle commands - route back to input\n",
        "    if command in [\"verbose\", \"quiet\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing back to input (verbose toggle)\")\n",
        "        return \"input\"\n",
        "\n",
        "    # Normal message - route to model\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to call_model\")\n",
        "    return \"call_model\"\n",
        "\n",
        "\n",
        "def route_after_model(state: ConversationState) -> Literal[\"tools\", \"output\"]:\n",
        "    \"\"\"\n",
        "    Route after model call based on whether tools were requested.\n",
        "\n",
        "    Logic:\n",
        "    - If the model's response includes tool_calls, route to tools\n",
        "    - Otherwise, route to output to display the response\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"tools\" if tools requested, \"output\" otherwise\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    # Check if the last message has tool calls\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to tools\")\n",
        "        return \"tools\"\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to output\")\n",
        "    return \"output\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# GRAPH CONSTRUCTION\n",
        "# ============================================================================\n",
        "\n",
        "def create_conversation_graph():\n",
        "    \"\"\"\n",
        "    Build the conversation graph with manual tool calling using ToolNode.\n",
        "\n",
        "    Graph structure (single conversation with looping):\n",
        "\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚                                                      â”‚\n",
        "        â–¼                                                      â”‚\n",
        "      input_node â”€â”€(check command)â”€â”€> call_model              â”‚\n",
        "          â–²                              â”‚                     â”‚\n",
        "          â”‚                              â”œâ”€â”€(has tools)â”€â”€> tools\n",
        "          â”‚                              â”‚                    â”‚\n",
        "          â”‚                              â”‚                    â”‚\n",
        "          â”‚                              â””â”€â”€(no tools)â”€â”€> output_node\n",
        "          â”‚                                                    â”‚\n",
        "          â”‚                                                    â–¼\n",
        "          â””â”€â”€â”€(verbose/quiet)                           trim_history â”€â”€â”˜\n",
        "\n",
        "          â””â”€â”€â”€â”€â”€(exit)â”€â”€> END\n",
        "\n",
        "    Key features:\n",
        "    - Manual tool calling with ToolNode (no create_react_agent)\n",
        "    - Command field used for special commands (no sentinel messages!)\n",
        "    - Single conversation maintained in state.messages\n",
        "    - Graph loops back to input_node after each turn\n",
        "    - Tools route back to call_model for continued reasoning\n",
        "    - History automatically trimmed when it grows too long\n",
        "\n",
        "    Returns:\n",
        "        Compiled LangGraph application\n",
        "    \"\"\"\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the Conversation Graph\n",
        "    # ========================================================================\n",
        "\n",
        "    workflow = StateGraph(ConversationState)\n",
        "\n",
        "    # Create ToolNode to handle tool execution\n",
        "    # ToolNode automatically executes tools in parallel when possible\n",
        "    tool_node = ToolNode(tools)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"input\", input_node)\n",
        "    workflow.add_node(\"call_model\", call_model)\n",
        "    workflow.add_node(\"tools\", tool_node)  # ToolNode handles tool execution\n",
        "    workflow.add_node(\"output\", output_node)\n",
        "    workflow.add_node(\"trim_history\", trim_history)\n",
        "\n",
        "    # Set entry point - conversation always starts at input\n",
        "    workflow.set_entry_point(\"input\")\n",
        "\n",
        "    # Add conditional edge from input based on command field\n",
        "    # Check if user wants to exit, toggle verbose, or continue\n",
        "    workflow.add_conditional_edges(\n",
        "        \"input\",\n",
        "        route_after_input,\n",
        "        {\n",
        "            \"call_model\": \"call_model\",\n",
        "            \"input\": \"input\",  # Loop back for verbose/quiet\n",
        "            \"end\": END\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add conditional edge from call_model\n",
        "    # Check if tools were requested or if we have final response\n",
        "    workflow.add_conditional_edges(\n",
        "        \"call_model\",\n",
        "        route_after_model,\n",
        "        {\n",
        "            \"tools\": \"tools\",\n",
        "            \"output\": \"output\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # After tools execute, loop back to call_model\n",
        "    # This allows the model to see tool results and decide next action\n",
        "    workflow.add_edge(\"tools\", \"call_model\")\n",
        "\n",
        "    # After output, trim history and loop back to input\n",
        "    workflow.add_edge(\"output\", \"trim_history\")\n",
        "    workflow.add_edge(\"trim_history\", \"input\")  # This creates the conversation loop!\n",
        "\n",
        "    # Compile the graph\n",
        "    print(\"[SYSTEM] Conversation graph created successfully (using manual ToolNode)\")\n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graph(app):\n",
        "    \"\"\"\n",
        "    Generate Mermaid diagram of the conversation graph.\n",
        "\n",
        "    Creates:\n",
        "    - langchain_manual_tool_graph.png: The conversation loop with manual tool calling\n",
        "\n",
        "    Args:\n",
        "        app: Compiled conversation graph\n",
        "    \"\"\"\n",
        "    try:\n",
        "        graph_png = app.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_manual_tool_graph.png\", \"wb\") as f:\n",
        "            f.write(graph_png)\n",
        "        print(\"[SYSTEM] Graph visualization saved to 'langchain_manual_tool_graph.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate graph visualization: {e}\")\n",
        "        print(\"You may need to install: pip install pygraphviz or pip install grandalf\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    This function:\n",
        "    1. Creates the conversation graph\n",
        "    2. Visualizes the graph structure\n",
        "    3. Initializes the conversation state\n",
        "    4. Invokes the graph ONCE\n",
        "\n",
        "    The graph then runs indefinitely via internal looping (trim_history -> input)\n",
        "    until the user types 'quit' or 'exit'.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LangGraph Manual Tool Calling - Persistent Multi-Turn Conversation\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nThis system uses manual tool calling with ToolNode:\")\n",
        "    print(\"  - call_model: Invokes LLM with tool bindings\")\n",
        "    print(\"  - ToolNode: Executes requested tools in parallel\")\n",
        "    print(\"  - Loop: tools -> call_model (until no more tools needed)\")\n",
        "    print(\"  - Single persistent conversation across all turns\")\n",
        "    print(\"  - History managed automatically (trimmed after 100 messages)\")\n",
        "    print(\"\\nCommands:\")\n",
        "    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n",
        "    print(\"  - Type 'verbose' to enable detailed tracing\")\n",
        "    print(\"  - Type 'quiet' to disable detailed tracing\")\n",
        "    print(\"\\nAvailable tools:\")\n",
        "    print(\"  - get_weather(location): Get weather information\")\n",
        "    print(\"  - get_population(city): Get population data\")\n",
        "    print(\"  - calculate(expression): Evaluate math expressions\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the conversation graph\n",
        "    app = create_conversation_graph()\n",
        "\n",
        "    # Visualize the graph\n",
        "    visualize_graph(app)\n",
        "\n",
        "    # Initialize conversation state\n",
        "    # This state persists across all turns via graph looping\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"verbose\": True,\n",
        "        \"command\": None\n",
        "    }\n",
        "\n",
        "    print(\"\\n[SYSTEM] Starting conversation...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Invoke the graph ONCE\n",
        "        # The graph will loop internally until user exits\n",
        "        # Each iteration: input -> call_model -> [tools -> call_model]* -> output -> trim -> input\n",
        "        # Verbose commands: input -> input (direct loop!)\n",
        "        await app.ainvoke(initial_state)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n[SYSTEM] Interrupted by user (Ctrl+C)\")\n",
        "\n",
        "    print(\"\\n[SYSTEM] Conversation ended. Goodbye!\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vp4lunnuVSF",
        "outputId": "d7147d0a-fc29-4fa1-c86a-0086f57a74f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LangGraph ReAct Agent - Persistent Multi-Turn Conversation\n",
            "================================================================================\n",
            "\n",
            "This system uses create_react_agent with graph-based looping:\n",
            "  - Single persistent conversation across all turns\n",
            "  - History managed automatically (trimmed after 100 messages)\n",
            "  - Loops via graph edges (no Python loops or checkpointing)\n",
            "\n",
            "Commands:\n",
            "  - Type 'quit' or 'exit' to end the conversation\n",
            "  - Type 'verbose' to enable detailed tracing\n",
            "  - Type 'quiet' to disable detailed tracing\n",
            "\n",
            "Available tools:\n",
            "  - get_weather(location): Get weather information\n",
            "  - get_population(city): Get population data\n",
            "  - calculate(expression): Evaluate math expressions\n",
            "================================================================================\n",
            "[SYSTEM] ReAct agent created successfully\n",
            "[SYSTEM] ReAct agent graph saved to 'langchain_react_agent.png'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-745513083.py:375: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  react_agent = create_react_agent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SYSTEM] Conversation graph saved to 'langchain_conversation_graph.png'\n",
            "\n",
            "[SYSTEM] Starting conversation...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: Population in New York\n",
            "[DEBUG] User input: Population in New York\n",
            "[DEBUG] Routing to call_react_agent\n",
            "\n",
            "================================================================================\n",
            "NODE: call_react_agent\n",
            "================================================================================\n",
            "[DEBUG] Invoking ReAct agent with 1 messages in history\n",
            "[DEBUG] Agent generated 3 new messages\n",
            "[DEBUG] Tool calls: ['get_population']\n",
            "[DEBUG] Response preview: The population of New York is approximately 1 million people....\n",
            "\n",
            "================================================================================\n",
            "NODE: output_node\n",
            "================================================================================\n",
            "\n",
            "Assistant: The population of New York is approximately 1 million people.\n",
            "\n",
            "================================================================================\n",
            "NODE: input_node\n",
            "================================================================================\n",
            "\n",
            "You: exit\n",
            "[DEBUG] Exit command received\n",
            "[DEBUG] Routing to END (exit requested)\n",
            "\n",
            "[SYSTEM] Conversation ended. Goodbye!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "LangGraph ReAct Agent with Persistent Multi-Turn Conversation\n",
        "\n",
        "This program demonstrates a LangGraph application using create_react_agent with:\n",
        "- A single persistent conversation across multiple turns\n",
        "- Graph-based looping (no Python loops or checkpointing)\n",
        "- Automatic conversation history management (trimming after 100 messages)\n",
        "- Verbose debugging output\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "import time\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# ============================================================================\n",
        "# STATE DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationState(TypedDict):\n",
        "    \"\"\"\n",
        "    State schema for the conversation.\n",
        "\n",
        "    Attributes:\n",
        "        messages: Full conversation history with automatic message merging\n",
        "        verbose: Controls detailed tracing output\n",
        "        command: Special command from user (exit, verbose, quiet, or None)\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    verbose: bool\n",
        "    command: str  # \"exit\", \"verbose\", \"quiet\", or None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TOOL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"\n",
        "    Get current weather information for a specified location.\n",
        "\n",
        "    Args:\n",
        "        location: City name or location string\n",
        "\n",
        "    Returns:\n",
        "        Weather description string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    time.sleep(0.5)\n",
        "    return f\"Weather in {location}: Sunny, 72Â°F with light winds\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_population(city: str) -> str:\n",
        "    \"\"\"\n",
        "    Get population information for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city: City name\n",
        "\n",
        "    Returns:\n",
        "        Population information string\n",
        "    \"\"\"\n",
        "    # Simulate API call delay\n",
        "    time.sleep(0.5)\n",
        "    return f\"Population of {city}: Approximately 1 million people\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression.\n",
        "\n",
        "    Args:\n",
        "        expression: Mathematical expression to evaluate (e.g., \"2 + 2\")\n",
        "\n",
        "    Returns:\n",
        "        Result of the calculation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Safe evaluation of simple math expressions\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating: {str(e)}\"\n",
        "\n",
        "\n",
        "# List of all available tools\n",
        "tools = [get_weather, get_population, calculate]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# NODE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def input_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Get input from the user and add it to the conversation.\n",
        "\n",
        "    This node:\n",
        "    - Prompts the user for input\n",
        "    - Handles special commands (quit, exit, verbose, quiet)\n",
        "    - Adds user message to conversation history (for real messages only)\n",
        "    - Sets command field for special commands\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new user message or command\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: input_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Get user input\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    # Handle exit commands\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Exit command received\")\n",
        "        # Set command field, don't add to messages\n",
        "        return {\"command\": \"exit\"}\n",
        "\n",
        "    # Handle verbose toggle\n",
        "    if user_input.lower() == \"verbose\":\n",
        "        print(\"[SYSTEM] Verbose mode enabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"verbose\", \"verbose\": True}\n",
        "\n",
        "    if user_input.lower() == \"quiet\":\n",
        "        print(\"[SYSTEM] Verbose mode disabled\")\n",
        "        # Set command field and update verbose flag\n",
        "        return {\"command\": \"quiet\", \"verbose\": False}\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(f\"[DEBUG] User input: {user_input}\")\n",
        "\n",
        "    # Clear command field and add message\n",
        "    return {\"command\": None, \"messages\": [HumanMessage(content=user_input)]}\n",
        "\n",
        "\n",
        "def call_react_agent(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Invoke the ReAct agent with the current conversation history.\n",
        "\n",
        "    This node:\n",
        "    - Takes the full conversation history from state\n",
        "    - Invokes the ReAct agent (which handles tool calling internally)\n",
        "    - Returns only the NEW messages generated by the agent\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with agent's response messages\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: call_react_agent\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"[DEBUG] Invoking ReAct agent with {len(state['messages'])} messages in history\")\n",
        "\n",
        "    # Get the global react_agent\n",
        "    global react_agent\n",
        "\n",
        "    # Count messages before agent call\n",
        "    messages_before = len(state[\"messages\"])\n",
        "\n",
        "    # Invoke the ReAct agent with full conversation history\n",
        "    # The agent maintains context across all previous turns\n",
        "    result = react_agent.invoke({\"messages\": state[\"messages\"]})\n",
        "\n",
        "    if state.get(\"verbose\", True):\n",
        "        messages_after = len(result[\"messages\"])\n",
        "        new_message_count = messages_after - messages_before\n",
        "        print(f\"[DEBUG] Agent generated {new_message_count} new messages\")\n",
        "\n",
        "        # Show what the agent did\n",
        "        for msg in result[\"messages\"][messages_before:]:\n",
        "            if isinstance(msg, AIMessage):\n",
        "                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "                    print(f\"[DEBUG] Tool calls: {[tc['name'] for tc in msg.tool_calls]}\")\n",
        "                elif msg.content:\n",
        "                    print(f\"[DEBUG] Response preview: {msg.content[:100]}...\")\n",
        "\n",
        "    # Return only the NEW messages (everything after what we sent)\n",
        "    new_messages = result[\"messages\"][messages_before:]\n",
        "    return {\"messages\": new_messages}\n",
        "\n",
        "\n",
        "def output_node(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Display the assistant's final response to the user.\n",
        "\n",
        "    This node:\n",
        "    - Extracts the last AI message from the conversation\n",
        "    - Prints it to the console\n",
        "    - Returns empty dict (no state changes)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Empty dict (no state modifications)\n",
        "    \"\"\"\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"NODE: output_node\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Find the last AI message in the conversation\n",
        "    # (there may be tool messages mixed in)\n",
        "    last_ai_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, AIMessage) and msg.content:\n",
        "            last_ai_message = msg\n",
        "            break\n",
        "\n",
        "    if last_ai_message:\n",
        "        print(f\"\\nAssistant: {last_ai_message.content}\")\n",
        "    else:\n",
        "        print(\"\\n[WARNING] No assistant response found\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "def trim_history(state: ConversationState) -> ConversationState:\n",
        "    \"\"\"\n",
        "    Manage conversation history length to prevent unlimited growth.\n",
        "\n",
        "    Strategy:\n",
        "    - Keep the system message (if present)\n",
        "    - Keep the most recent 100 messages\n",
        "    - This allows ~50 conversation turns (user + assistant pairs)\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with trimmed message history (if needed)\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_messages = 100\n",
        "\n",
        "    # Only trim if we've exceeded the limit\n",
        "    if len(messages) > max_messages:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(f\"\\n[DEBUG] History length: {len(messages)} messages\")\n",
        "            print(f\"[DEBUG] Trimming to most recent {max_messages} messages\")\n",
        "\n",
        "        # Preserve system message if it exists at the start\n",
        "        if messages and isinstance(messages[0], SystemMessage):\n",
        "            # Keep system message + last (max_messages - 1) messages\n",
        "            trimmed = [messages[0]] + list(messages[-(max_messages - 1):])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Preserved system message + {max_messages - 1} recent messages\")\n",
        "        else:\n",
        "            # Just keep the last max_messages\n",
        "            trimmed = list(messages[-max_messages:])\n",
        "            if state.get(\"verbose\", True):\n",
        "                print(f\"[DEBUG] Kept {max_messages} most recent messages\")\n",
        "\n",
        "        return {\"messages\": trimmed}\n",
        "\n",
        "    # No trimming needed\n",
        "    return {}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTING LOGIC\n",
        "# ============================================================================\n",
        "\n",
        "def route_after_input(state: ConversationState) -> Literal[\"call_react_agent\", \"end\", \"input\"]:\n",
        "    \"\"\"\n",
        "    Determine where to route after input based on command field.\n",
        "\n",
        "    Logic:\n",
        "    - If command is \"exit\", route to END\n",
        "    - If command is \"verbose\" or \"quiet\", route back to input\n",
        "    - Otherwise (command is None), route to the ReAct agent\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "\n",
        "    Returns:\n",
        "        \"end\" to terminate, \"input\" for verbose toggle, \"call_react_agent\" to continue\n",
        "    \"\"\"\n",
        "    command = state.get(\"command\")\n",
        "\n",
        "    # Check for exit command\n",
        "    if command == \"exit\":\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing to END (exit requested)\")\n",
        "        return \"end\"\n",
        "\n",
        "    # Check for verbose toggle commands - route back to input\n",
        "    if command in [\"verbose\", \"quiet\"]:\n",
        "        if state.get(\"verbose\", True):\n",
        "            print(\"[DEBUG] Routing back to input (verbose toggle)\")\n",
        "        return \"input\"\n",
        "\n",
        "    # Normal message - route to agent\n",
        "    if state.get(\"verbose\", True):\n",
        "        print(\"[DEBUG] Routing to call_react_agent\")\n",
        "    return \"call_react_agent\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# GRAPH CONSTRUCTION\n",
        "# ============================================================================\n",
        "\n",
        "# Global variable to hold the ReAct agent\n",
        "react_agent = None\n",
        "\n",
        "def create_conversation_graph():\n",
        "    \"\"\"\n",
        "    Build the conversation graph with persistent multi-turn capability.\n",
        "\n",
        "    Graph structure (single conversation with looping):\n",
        "\n",
        "        Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â\n",
        "        Ã¢â€â€š                                                      Ã¢â€â€š\n",
        "        Ã¢â€“Â¼                                                      Ã¢â€â€š\n",
        "      input_node Ã¢â€â‚¬Ã¢â€â‚¬(check command)Ã¢â€â‚¬Ã¢â€â‚¬> call_react_agent        Ã¢â€â€š\n",
        "          Ã¢â€“Â²                              Ã¢â€â€š                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€“Â¼                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                         output_node                Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€â€š                     Ã¢â€â€š\n",
        "          Ã¢â€â€š                              Ã¢â€“Â¼                     Ã¢â€â€š\n",
        "          Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬(verbose/quiet)       trim_history Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ\n",
        "\n",
        "          Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬(exit)Ã¢â€â‚¬Ã¢â€â‚¬> END\n",
        "\n",
        "    Key features:\n",
        "    - Single conversation maintained in state.messages\n",
        "    - Command field used for special commands (no sentinel messages!)\n",
        "    - Graph loops back to input_node after each turn\n",
        "    - Verbose/quiet commands route directly back to input\n",
        "    - History automatically trimmed when it grows too long\n",
        "    - No Python loops or checkpointing needed\n",
        "\n",
        "    Returns:\n",
        "        Compiled LangGraph application\n",
        "    \"\"\"\n",
        "    global react_agent\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the ReAct Agent\n",
        "    # ========================================================================\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # System message to encourage tool usage\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"If a tool is able to solve a problem you are working on then \"\n",
        "        \"always use it, even if you are able to solve it without using a tool.\"\n",
        "    )\n",
        "\n",
        "    # Create the ReAct agent using the built-in function\n",
        "    # This agent handles the thought/action/observation loop internally\n",
        "    react_agent = create_react_agent(\n",
        "        model=model,\n",
        "        tools=tools,\n",
        "        prompt=system_message\n",
        "    )\n",
        "\n",
        "    print(\"[SYSTEM] ReAct agent created successfully\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Create the Conversation Wrapper Graph\n",
        "    # ========================================================================\n",
        "\n",
        "    workflow = StateGraph(ConversationState)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"input\", input_node)\n",
        "    workflow.add_node(\"call_react_agent\", call_react_agent)\n",
        "    workflow.add_node(\"output\", output_node)\n",
        "    workflow.add_node(\"trim_history\", trim_history)\n",
        "\n",
        "    # Set entry point - conversation always starts at input\n",
        "    workflow.set_entry_point(\"input\")\n",
        "\n",
        "    # Add conditional edge from input based on command field\n",
        "    workflow.add_conditional_edges(\n",
        "        \"input\",\n",
        "        route_after_input,\n",
        "        {\n",
        "            \"call_react_agent\": \"call_react_agent\",\n",
        "            \"input\": \"input\",  # Loop back for verbose/quiet\n",
        "            \"end\": END\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add linear edges for the main conversation flow\n",
        "    # Agent -> Output -> Trim -> Input (loops back!)\n",
        "    workflow.add_edge(\"call_react_agent\", \"output\")\n",
        "    workflow.add_edge(\"output\", \"trim_history\")\n",
        "    workflow.add_edge(\"trim_history\", \"input\")  # This creates the loop!\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graphs(wrapper_app):\n",
        "    \"\"\"\n",
        "    Generate Mermaid diagrams for both graphs.\n",
        "\n",
        "    Creates:\n",
        "    - langchain_react_agent.png: Internal ReAct agent (thought/action/observation)\n",
        "    - langchain_conversation_graph.png: Conversation loop wrapper\n",
        "\n",
        "    Args:\n",
        "        wrapper_app: Compiled conversation graph\n",
        "    \"\"\"\n",
        "    global react_agent\n",
        "\n",
        "    # Visualize the ReAct agent\n",
        "    try:\n",
        "        react_png = react_agent.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_react_agent.png\", \"wb\") as f:\n",
        "            f.write(react_png)\n",
        "        print(\"[SYSTEM] ReAct agent graph saved to 'langchain_react_agent.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate ReAct agent visualization: {e}\")\n",
        "\n",
        "    # Visualize the conversation wrapper\n",
        "    try:\n",
        "        wrapper_png = wrapper_app.get_graph().draw_mermaid_png()\n",
        "        with open(\"langchain_conversation_graph.png\", \"wb\") as f:\n",
        "            f.write(wrapper_png)\n",
        "        print(\"[SYSTEM] Conversation graph saved to 'langchain_conversation_graph.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not generate conversation graph visualization: {e}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    This function:\n",
        "    1. Creates the conversation graph\n",
        "    2. Visualizes the graph structure\n",
        "    3. Initializes the conversation state\n",
        "    4. Invokes the graph ONCE\n",
        "\n",
        "    The graph then runs indefinitely via internal looping (trim_history -> input)\n",
        "    until the user types 'quit' or 'exit'.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LangGraph ReAct Agent - Persistent Multi-Turn Conversation\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nThis system uses create_react_agent with graph-based looping:\")\n",
        "    print(\"  - Single persistent conversation across all turns\")\n",
        "    print(\"  - History managed automatically (trimmed after 100 messages)\")\n",
        "    print(\"  - Loops via graph edges (no Python loops or checkpointing)\")\n",
        "    print(\"\\nCommands:\")\n",
        "    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n",
        "    print(\"  - Type 'verbose' to enable detailed tracing\")\n",
        "    print(\"  - Type 'quiet' to disable detailed tracing\")\n",
        "    print(\"\\nAvailable tools:\")\n",
        "    print(\"  - get_weather(location): Get weather information\")\n",
        "    print(\"  - get_population(city): Get population data\")\n",
        "    print(\"  - calculate(expression): Evaluate math expressions\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the conversation graph\n",
        "    app = create_conversation_graph()\n",
        "\n",
        "    # Visualize both graphs\n",
        "    visualize_graphs(app)\n",
        "\n",
        "    # Initialize conversation state\n",
        "    # This state persists across all turns via graph looping\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"verbose\": True,\n",
        "        \"command\": None\n",
        "    }\n",
        "\n",
        "    print(\"\\n[SYSTEM] Starting conversation...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Invoke the graph ONCE\n",
        "        # The graph will loop internally until user exits\n",
        "        # Each iteration: input -> agent -> output -> trim -> input (loop!)\n",
        "        # Verbose commands: input -> input (direct loop!)\n",
        "        await app.ainvoke(initial_state)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n[SYSTEM] Interrupted by user (Ctrl+C)\")\n",
        "\n",
        "    print(\"\\n[SYSTEM] Conversation ended. Goodbye!\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41T_Mg6uugbm"
      },
      "source": [
        "Part 2 - YouTube Transcript\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVIjX8voN3pk",
        "outputId": "b0002b70-ec8b-4bca-8785-1c932c2d6e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-1.2.4-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2026.1.4)\n",
            "Downloading youtube_transcript_api-1.2.4-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.2/485.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-1.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP_JimcVOBUs",
        "outputId": "75543637-cf5d-46fe-f912-f16ce9d11267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain.tools in /usr/local/lib/python3.12/dist-packages (0.1.34)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from langchain.tools) (1.2.10)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (from langchain.tools) (0.4.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (from langchain.tools) (1.1.10)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain->langchain.tools) (1.2.13)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain->langchain.tools) (1.0.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain->langchain.tools) (2.12.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (9.1.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (2.13.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (0.7.3)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community->langchain.tools) (2.0.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain.tools) (2.21.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain.tools) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->langchain.tools) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->langchain.tools) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->langchain.tools) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community->langchain.tools) (1.1.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (0.3.6)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai->langchain.tools) (4.67.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->langchain.tools) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->langchain.tools) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->langchain.tools) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->langchain.tools) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community->langchain.tools) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community->langchain.tools) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community->langchain.tools) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community->langchain.tools) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community->langchain.tools) (3.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai->langchain.tools) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community->langchain.tools) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain->langchain.tools) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain->langchain.tools) (1.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->langchain.tools) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain.tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "trAWtWpASYUn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlXajZTjRF7P",
        "outputId": "77966e23-7a37-4a3b-929b-4bba8cdf303e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.10)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.5)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.21.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.6)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.67.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (0.25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api langchain langchain-openai langgraph python-dotenv tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r_GNlczS0q2L"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B7gWkrRu06iR"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Mzhm1F5pR0pH"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def extract_video_id(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the 11-character YouTube video ID from a URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): A YouTube URL containing a video ID.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted video ID or error message if parsing fails.\n",
        "    \"\"\"\n",
        "\n",
        "    # Regex pattern to match video IDs\n",
        "    pattern = r'(?:v=|be/|embed/)([a-zA-Z0-9_-]{11})'\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else \"Error: Invalid YouTube URL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4ydBznIhSCIg",
        "outputId": "3676c193-8a81-45e8-c64c-e3006c59cb9f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hfIUstzHs9A'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_video_id.run(\"https://www.youtube.com/watch?v=hfIUstzHs9A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lIoWQ6Xj1CAH"
      },
      "outputs": [],
      "source": [
        "tools = []\n",
        "tools.append(extract_video_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qno1vnnZ1Q2R"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "\n",
        "@tool\n",
        "def fetch_transcript(video_id: str, language: str = \"en\") -> str:\n",
        "    \"\"\"\n",
        "    Fetches the transcript of a YouTube video.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n",
        "        language (str): Language code for the transcript (e.g., \"en\", \"es\").\n",
        "\n",
        "    Returns:\n",
        "        str: The transcript text or an error message.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        ytt_api = YouTubeTranscriptApi()\n",
        "        transcript = ytt_api.fetch(video_id, languages=[language])\n",
        "        return \" \".join([snippet.text for snippet in transcript.snippets])\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "GXjgitwi1Vby",
        "outputId": "7269baeb-58d7-4d84-a3b5-e16e7440350e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Over the past couple of months, large language models, or LLMs, such as chatGPT, have taken the world by storm. Whether it\\'s writing poetry or helping plan your upcoming vacation, we are seeing a step change in the performance of AI and its potential to drive enterprise value. My name is Kate Soule. I\\'m a senior manager of business strategy at IBM Research, and today I\\'m going to give a brief overview of this new field of AI that\\'s emerging and how it can be used in a business setting to drive value. Now, large language models are actually a part of a different class of models called foundation models. Now, the term \"foundation models\" was actually first coined by a team from Stanford when they saw that the field of AI was converging to a new paradigm. Where before AI applications were being built by training, maybe a library of different AI models, where each AI model was trained on very task-specific data to perform very specific task. They predicted that we were going to start moving to a new paradigm, where we would have a foundational capability, or a foundation model, that would drive all of these same use cases and applications. So the same exact applications that we were envisioning before with conventional AI, and the same model could drive any number of additional applications. The point is that this model could be transferred to any number of tasks. What gives this model the super power to be able to transfer to multiple different tasks and perform multiple different functions is that it\\'s been trained on a huge amount, in an unsupervised manner, on unstructured data. And what that means, in the language domain, is basically I\\'ll feed a bunch of sentences-- and I\\'m talking terabytes of data here --to train this model. And the start of my sentence might be \"no use crying over spilled\" and the end of my sentence might be \"milk\". And I\\'m trying to get my model to predict the last word of the sentence based off of the words that it saw before. And it\\'s this generative capability of the model-- predicting and generating the next word --based off of previous words that it\\'s seen beforehand, that is why that foundation models are actually a part of the field of AI called generative AI because we\\'re generating something new in this case, the next word in a sentence. And even though these models are trained to perform, at its core, a generation past, predicting the next word in the sentence, we actually can take these models, and if you introduce a small amount of labeled data to the equation, you can tune them to perform traditional NLP tasks-- things like classification, or named-entity recognition --things that you don\\'t normally associate as being a generative-based model or capability. And this process is called tuning. Where you can tune your foundation model by introducing a small amount of data, you update the parameters of your model and now perform a very specific natural language task. If you don\\'t have data, or have only very few data points, you can still take these foundation models and they actually work very well in low-labeled data domains. And in a process called prompting or prompt engineering, you can apply these models for some of those same exact tasks. So an example of prompting a model to perform a classification task might be you could give a model a sentence and then ask it a question: Does this sentence have a positive sentiment or negative sentiment? The model\\'s going to try and finish generating words in that sentence, and the next natural word in that sentence would be the answer to your classification problem, which would respond either positive or negative, depending on where it estimated the sentiment of the sentence would be. And these models work surprisingly well when applied to these new settings and domains. Now, this is a lot of where the advantages of foundation models come into play. So if we talk about the advantages, the chief advantage is the performance. These models have seen so much data. Again, data with a capital D-- terabytes of data --that by the time that they\\'re applied to small tasks, they can drastically outperform a model that was only trained on just a few data points. The second advantage of these models are the productivity gains. So just like I said earlier, through prompting or tuning, you need far less label data to get to task-specific model than if you had to start from scratch because your model is taking advantage of all the unlabeled data that it saw in its pre-training when we created this generative task. With these advantages, there are also some disadvantages that are important to keep in mind. And the first of those is the compute cost. So that penalty for having this model see so much data is that they\\'re very expensive to train, making it difficult for smaller enterprises to train a foundation model on their own. They\\'re also expensive-- by the time they get to a huge size, a couple billion parameters --they\\'re also very expensive to run inference. You might require multiple GPUs at a time just to host these models and run inference, making them a more costly method than traditional approaches. The second disadvantage of these models is on the trustworthiness side. So just like data is a huge advantage for these models, they\\'ve seen so much unstructured data, it also comes at a cost, especially in the domain like language. A lot of these models are trained basically off of language data that\\'s been scraped from the Internet. And there\\'s so much data that these models have been trained on. Even if you had a whole team of human annotators, you wouldn\\'t be able to go through and actually vet every single data point to make sure that it wasn\\'t biased and didn\\'t contain hate speech or other toxic information. And that\\'s just assuming you actually know what the data is. Often we don\\'t even know-- for a lot of these open source models that have been posted --what the exact datasets are that these models have been trained on leading to trustworthiness issues. So IBM recognizes the huge potential of these technologies. But my partners in IBM Research are working on multiple different innovations to try and improve also the efficiency of these models and the trustworthiness and reliability of these models to make them more relevant in a business setting. All of these examples that I\\'ve talked through so far have just been on the language side. But the reality is, there are a lot of other domains that foundation models can be applied towards. Famously, we\\'ve seen foundation models for vision --looking at models such as DALL-E 2, which takes text data, and that\\'s then used to generate a custom image. We\\'ve seen models for code with products like Copilot that can help complete code as it\\'s being authored. And IBM\\'s innovating across all of these domains. So whether it\\'s language models that we\\'re building into products like Watson Assistant and Watson Discovery, vision models that we\\'re building into products like Maximo Visual Inspection, or Ansible code models that we\\'re building with our partners at Red Hat under Project Wisdom. We\\'re innovating across all of these domains and more. We\\'re working on chemistry. So, for example, we just published and released molformer, which is a foundation model to promote molecule discovery or different targeted therapeutics. And we\\'re working on models for climate change, building Earth Science Foundation models using geospatial data to improve climate research. I hope you found this video both informative and helpful. If you\\'re interested in learning more, particularly how IBM is working to improve some of these disadvantages, making foundation models more trustworthy and more efficient, please take a look at the links below. Thank you.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fetch_transcript.run(\"hfIUstzHs9A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlpsQZ3W6Drl",
        "outputId": "6b834eb2-480c-4d5b-ced1-e68d20cd2f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing https://www.youtube.com/watch?v=x7X9w_GIm1s\n",
            "\n",
            "VIDEO ID: x7X9w_GIm1s\n",
            "\n",
            "--- SUMMARY ---\n",
            " Python is a high-level, interpreted programming language created by Guido van Rossum in 1991, named after Monty Python's Flying Circus. Known for its readability and simplicity, Python is popular among both beginners and advanced developers. It is widely used in server-side applications, big data analysis, and machine learning. Python supports multiple programming paradigms, including functional and object-oriented programming, and boasts a vast ecosystem of third-party libraries. Its syntax, which uses indentation for code structure, promotes efficient coding practices. The Zen of Python emphasizes readability, contributing to its widespread adoption.\n",
            "\n",
            "--- KEY CONCEPTS ---\n",
            "1. **Python Programming Language**: Python is a high-level, interpreted programming language known for its readability and simplicity, making it popular for both beginners and experienced developers.\n",
            "2. **Zen of Python**: A collection of aphorisms that capture the philosophy of Python, emphasizing readability and simplicity, such as \"Beautiful is better than ugly\" and \"Explicit is better than implicit.\"\n",
            "3. **Python Syntax**: Python uses indentation to define the scope of code blocks instead of curly braces or semicolons, which promotes clean and readable code.\n",
            "4. **Dynamic Typing**: Python is dynamically typed, meaning you don't need to declare variable types explicitly, but it is strongly typed, so variables don't change types unexpectedly.\n",
            "5. **Interactive Notebooks**: Python code can be organized into notebooks (e.g., Jupyter Notebooks) where code and documentation coexist, allowing for interactive data analysis and visualization.\n",
            "6. **Multiparadigm Language**: Python supports multiple programming paradigms, including functional programming with anonymous functions (lambdas) and object-oriented programming with classes and inheritance.\n",
            "7. **Third: party Libraries**: Python has a vast ecosystem of libraries, such as TensorFlow for deep learning and OpenCV for computer vision, which can be easily installed using the PIP package manager.\n",
            "8. **Pythonic Code**: Writing \"Pythonic\" code means adhering to Python's idioms and style guidelines, such as avoiding unnecessary semicolons and using list comprehensions for concise code.\n",
            "\n",
            "--- QUIZ ---\n",
            "\n",
            "Q1: Who created the Python programming language?\n",
            "  A. Guido van Rossum\n",
            "  B. James Gosling\n",
            "  C. Bjarne Stroustrup\n",
            "  D. Dennis Ritchie\n",
            "Answer: A\n",
            "Explanation: Python was created by Guido van Rossum and released in 1991.\n",
            "\n",
            "Q2: What is the Python programming language named after?\n",
            "  A. Monty Python's Flying Circus\n",
            "  B. A type of snake\n",
            "  C. A Greek mythological figure\n",
            "  D. A famous scientist\n",
            "Answer: A\n",
            "Explanation: Python is named after Monty Python's Flying Circus, a British comedy series.\n",
            "\n",
            "Q3: Which of the following is a key principle of the Zen of Python?\n",
            "  A. Beautiful is better than ugly\n",
            "  B. Complex is better than complicated\n",
            "  C. Fast is better than slow\n",
            "  D. Verbose is better than concise\n",
            "Answer: A\n",
            "Explanation: The Zen of Python includes the principle 'Beautiful is better than ugly'.\n",
            "\n",
            "Q4: How does Python determine the scope of a line of code?\n",
            "  A. Indentation\n",
            "  B. Curly braces\n",
            "  C. Semicolons\n",
            "  D. Parentheses\n",
            "Answer: A\n",
            "Explanation: Python uses indentation to determine the scope of a line of code, unlike many other languages that use curly braces or semicolons.\n",
            "\n",
            "Q5: Which package manager is commonly used to install third-party libraries in Python?\n",
            "  A. PIP\n",
            "  B. NPM\n",
            "  C. Yarn\n",
            "  D. Composer\n",
            "Answer: A\n",
            "Explanation: PIP is the package manager commonly used to install third-party libraries in Python.\n",
            "\n",
            "Q6: What type of programming paradigms does Python support?\n",
            "  A. Multi-paradigm\n",
            "  B. Procedural only\n",
            "  C. Functional only\n",
            "  D. Object-oriented only\n",
            "Answer: A\n",
            "Explanation: Python is a multi-paradigm language, supporting functional, procedural, and object-oriented programming patterns.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "TEMPERATURE = 0.0\n",
        "CHUNK_CHAR_SIZE = 3500\n",
        "SUMMARY_MAX_TOKENS = 400\n",
        "KEY_CONCEPTS_COUNT = 8\n",
        "QUIZ_QUESTIONS = 6\n",
        "\n",
        "def get_llm():\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"OPENAI_API_KEY not set in environment. Set it before calling get_llm().\")\n",
        "    return ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\n",
        "\n",
        "def chunk_text(text: str, max_chars: int = CHUNK_CHAR_SIZE) -> List[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(text)\n",
        "    while start < n:\n",
        "        end = min(start + max_chars, n)\n",
        "        if end < n:\n",
        "            last_period = text.rfind(\". \", start, end)\n",
        "            last_newline = text.rfind(\"\\n\", start, end)\n",
        "            cut = max(last_period + 2, last_newline + 1, -1)\n",
        "            if cut >= start:\n",
        "                end = cut\n",
        "        chunks.append(text[start:end].strip())\n",
        "        start = end\n",
        "    return chunks\n",
        "\n",
        "def call_llm_prompt(prompt_messages: List[dict]) -> str:\n",
        "    llm = get_llm()\n",
        "    messages = []\n",
        "    for m in prompt_messages:\n",
        "        role = m.get(\"role\", \"user\")\n",
        "        content = m.get(\"content\", \"\")\n",
        "        if role == \"system\":\n",
        "            messages.append(SystemMessage(content=content))\n",
        "        else:\n",
        "            messages.append(HumanMessage(content=content))\n",
        "    resp = llm.invoke(messages)\n",
        "    if hasattr(resp, \"content\") and resp.content:\n",
        "        return resp.content\n",
        "    if isinstance(resp, dict) and \"content\" in resp:\n",
        "        return resp[\"content\"]\n",
        "    return str(resp)\n",
        "\n",
        "def summarize_transcript(transcript: str) -> str:\n",
        "    if not transcript:\n",
        "        return \"No transcript available.\"\n",
        "    chunks = chunk_text(transcript)\n",
        "    chunk_summaries = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        prompt = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a concise summarization assistant.\"},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"Summarize the following portion of a lecture/transcript in 3-5 short bullet points. \"\n",
        "                    \"Be concise and focus on the main points.\\n\\n\"\n",
        "                    f\"PORTION (chunk {i+1}/{len(chunks)}):\\n{chunk}\"\n",
        "                ),\n",
        "            },\n",
        "        ]\n",
        "        out = call_llm_prompt(prompt)\n",
        "        chunk_summaries.append(out.strip())\n",
        "    merged_text = \"\\n\\n\".join(chunk_summaries)\n",
        "    merge_prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that produces a short coherent summary.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Given the following bullet-style summaries from different parts of a lecture, \"\n",
        "                f\"produce a single concise summary (around {SUMMARY_MAX_TOKENS//10}–{SUMMARY_MAX_TOKENS//5} words) \"\n",
        "                \"that captures the overall topic, the main arguments/steps, and the important conclusions. \"\n",
        "                \"Use 4–8 short sentences.\\n\\n\"\n",
        "                f\"INPUT:\\n{merged_text}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "    final_summary = call_llm_prompt(merge_prompt).strip()\n",
        "    return final_summary\n",
        "\n",
        "def extract_key_concepts(transcript: str, n_concepts:int = KEY_CONCEPTS_COUNT) -> List[Dict[str,str]]:\n",
        "    prompt = [\n",
        "        {\"role\":\"system\", \"content\":\"You are a concise educational assistant who extracts important concepts.\"},\n",
        "        {\"role\":\"user\", \"content\": (\n",
        "            f\"From the following transcript, list the {n_concepts} most important concepts or terms a student should learn. \"\n",
        "            \"For each concept, give a one-sentence plain-English explanation and (optionally) a short example or formula.\\n\\n\"\n",
        "            f\"TRANSCRIPT:\\n{(transcript[:20000] + '...') if len(transcript) > 20000 else transcript}\"\n",
        "        )},\n",
        "    ]\n",
        "    out = call_llm_prompt(prompt)\n",
        "    lines = [ln.strip() for ln in out.splitlines() if ln.strip()]\n",
        "    concepts = []\n",
        "    for ln in lines:\n",
        "        if len(concepts) >= n_concepts:\n",
        "            break\n",
        "        parts = re.split(r\"\\s*[:\\-–—]\\s*\", ln, maxsplit=1)\n",
        "        if len(parts) == 2:\n",
        "            concept = parts[0].strip(\"0123456789. )\")\n",
        "            explanation = parts[1].strip()\n",
        "            concepts.append({\"concept\": concept, \"explanation\": explanation})\n",
        "        else:\n",
        "            concepts.append({\"concept\": ln[:60], \"explanation\": ln[60:].strip() or \"See transcript.\"})\n",
        "    return concepts[:n_concepts]\n",
        "\n",
        "def _synthesize_quiz_from_concepts(concepts: List[Dict[str,str]], n_questions: int):\n",
        "    items = []\n",
        "    for i in range(min(n_questions, len(concepts))):\n",
        "        c = concepts[i]\n",
        "        q = f\"What is the best short description of \\\"{c['concept']}\\\"?\"\n",
        "        correct = c['explanation']\n",
        "        distractors = []\n",
        "        for j in range(len(concepts)):\n",
        "            if j != i and len(distractors) < 3:\n",
        "                distractors.append(concepts[j]['explanation'].split('.')[0])\n",
        "        while len(distractors) < 3:\n",
        "            distractors.append(\"None of the above\")\n",
        "        options = [correct] + distractors\n",
        "        import random\n",
        "        random.shuffle(options)\n",
        "        answer_index = options.index(correct)\n",
        "        items.append({\n",
        "            \"question\": q,\n",
        "            \"options\": options,\n",
        "            \"answer_index\": answer_index,\n",
        "            \"explanation\": correct\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def generate_quiz(transcript: str, n_questions:int = QUIZ_QUESTIONS) -> List[Dict[str,Any]]:\n",
        "    prompt = [\n",
        "        {\"role\":\"system\",\"content\":\"You are an instructor creating accurate multiple-choice quiz questions. Return ONLY valid JSON.\"},\n",
        "        {\"role\":\"user\",\"content\": (\n",
        "            f\"Create {n_questions} multiple-choice questions (4 options each) based strictly on the transcript below.\\n\"\n",
        "            \"Return EXACTLY a JSON array. Each element must be:\\n\"\n",
        "            '{\"question\": \"...\", \"options\": [\"A text\",\"B text\",\"C text\",\"D text\"], \"answer_index\": 0, \"explanation\": \"...\"}\\n'\n",
        "            \"answer_index must match the correct option.\\n\\n\"\n",
        "            f\"TRANSCRIPT:\\n{(transcript[:20000] + '...') if len(transcript) > 20000 else transcript}\"\n",
        "        )}\n",
        "    ]\n",
        "    out = call_llm_prompt(prompt)\n",
        "    #print(\"RAW QUIZ OUTPUT:\\n\", out[:4000])\n",
        "    # try to locate a JSON substring in the output\n",
        "    json_text = None\n",
        "    try:\n",
        "        json_text = out.strip()\n",
        "        parsed = json.loads(json_text)\n",
        "    except Exception:\n",
        "        # try to extract first JSON array substring\n",
        "        m = re.search(r\"(\\[.*\\])\", out, flags=re.S)\n",
        "        if m:\n",
        "            candidate = m.group(1)\n",
        "            try:\n",
        "                parsed = json.loads(candidate)\n",
        "                json_text = candidate\n",
        "            except Exception:\n",
        "                parsed = None\n",
        "        else:\n",
        "            parsed = None\n",
        "\n",
        "    if parsed and isinstance(parsed, list):\n",
        "        items = []\n",
        "        for item in parsed[:n_questions]:\n",
        "            if (\n",
        "                isinstance(item, dict) and\n",
        "                \"question\" in item and\n",
        "                \"options\" in item and\n",
        "                \"answer_index\" in item and\n",
        "                \"explanation\" in item and\n",
        "                isinstance(item[\"options\"], list) and\n",
        "                len(item[\"options\"]) == 4\n",
        "            ):\n",
        "                items.append({\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"options\": item[\"options\"],\n",
        "                    \"answer_index\": int(item[\"answer_index\"]),\n",
        "                    \"explanation\": item[\"explanation\"]\n",
        "                })\n",
        "        if items:\n",
        "            return items\n",
        "\n",
        "    # Retry once with a stricter JSON-only instruction\n",
        "    retry_prompt = [\n",
        "        {\"role\":\"system\",\"content\":\"You must return ONLY a pure JSON array and nothing else.\"},\n",
        "        {\"role\":\"user\",\"content\": (\n",
        "            f\"The previous output was not parseable. Now return EXACTLY a JSON array with {n_questions} objects of the form:\\n\"\n",
        "            '{\"question\":\"...\",\"options\":[\"A\",\"B\",\"C\",\"D\"],\"answer_index\":0,\"explanation\":\"...\"}\\n\\n'\n",
        "            f\"TRANSCRIPT:\\n{(transcript[:20000] + '...') if len(transcript) > 20000 else transcript}\"\n",
        "        )}\n",
        "    ]\n",
        "    out2 = call_llm_prompt(retry_prompt)\n",
        "    print(\"RAW QUIZ OUTPUT (retry):\\n\", out2[:4000])\n",
        "    try:\n",
        "        parsed2 = json.loads(out2)\n",
        "        items2 = []\n",
        "        for item in parsed2[:n_questions]:\n",
        "            if (\n",
        "                isinstance(item, dict) and\n",
        "                \"question\" in item and\n",
        "                \"options\" in item and\n",
        "                \"answer_index\" in item and\n",
        "                \"explanation\" in item and\n",
        "                isinstance(item[\"options\"], list) and\n",
        "                len(item[\"options\"]) == 4\n",
        "            ):\n",
        "                items2.append({\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"options\": item[\"options\"],\n",
        "                    \"answer_index\": int(item[\"answer_index\"]),\n",
        "                    \"explanation\": item[\"explanation\"]\n",
        "                })\n",
        "        if items2:\n",
        "            return items2\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # As a last resort, synthesize quiz from key concepts\n",
        "    try:\n",
        "        concepts = extract_key_concepts(transcript, n_concepts= max(3, n_questions))\n",
        "        fallback = _synthesize_quiz_from_concepts(concepts, n_questions)\n",
        "        if fallback:\n",
        "            print(\"FALLBACK: generated quiz from key concepts.\")\n",
        "            return fallback\n",
        "    except Exception as e:\n",
        "        print(\"Fallback synthesis failed:\", e)\n",
        "\n",
        "    return []\n",
        "\n",
        "def analyze_video(url: str, language:str=\"en\", n_concepts:int=KEY_CONCEPTS_COUNT, n_quiz:int=QUIZ_QUESTIONS) -> Dict[str,Any]:\n",
        "    vid_id = extract_video_id.run(url)\n",
        "    if vid_id.startswith(\"Error\"):\n",
        "        raise ValueError(f\"Could not extract video id: {vid_id}\")\n",
        "    transcript = fetch_transcript.run(vid_id, language=language)\n",
        "    if transcript.startswith(\"Error\"):\n",
        "        raise RuntimeError(f\"Error fetching transcript: {transcript}\")\n",
        "    summary = summarize_transcript(transcript)\n",
        "    concepts = extract_key_concepts(transcript, n_concepts)\n",
        "    quiz = generate_quiz(transcript, n_quiz)\n",
        "    return {\n",
        "        \"video_id\": vid_id,\n",
        "        \"transcript\": transcript,\n",
        "        \"summary\": summary,\n",
        "        \"key_concepts\": concepts,\n",
        "        \"quiz\": quiz\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
        "        print(\"ERROR: OPENAI_API_KEY not set. In Colab do:\\n\"\n",
        "              \"from google.colab import userdata\\n\"\n",
        "              \"import os\\n\"\n",
        "              \"os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\\n\")\n",
        "        sys.exit(1)\n",
        "    test_url = \"https://www.youtube.com/watch?v=x7X9w_GIm1s\"\n",
        "    print(\"Analyzing\", test_url)\n",
        "    result = analyze_video(test_url)\n",
        "    print(\"\\nVIDEO ID:\", result[\"video_id\"])\n",
        "    print(\"\\n--- SUMMARY ---\\n\", result[\"summary\"])\n",
        "    print(\"\\n--- KEY CONCEPTS ---\")\n",
        "    for i,c in enumerate(result[\"key_concepts\"],1):\n",
        "        print(f\"{i}. {c['concept']}: {c['explanation']}\")\n",
        "    print(\"\\n--- QUIZ ---\")\n",
        "    for i,q in enumerate(result[\"quiz\"],1):\n",
        "        print(f\"\\nQ{i}: {q['question']}\")\n",
        "        for idx,opt in enumerate(q['options']):\n",
        "            print(f\"  {['A','B','C','D'][idx]}. {opt}\")\n",
        "        print(\"Answer:\", ['A','B','C','D'][q['answer_index']])\n",
        "        print(\"Explanation:\", q[\"explanation\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
