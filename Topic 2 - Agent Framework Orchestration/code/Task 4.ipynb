{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T22:10:27.904621Z",
     "iopub.status.busy": "2026-02-21T22:10:27.904314Z",
     "iopub.status.idle": "2026-02-21T22:11:31.788611Z",
     "shell.execute_reply": "2026-02-21T22:11:31.787797Z",
     "shell.execute_reply.started": "2026-02-21T22:10:27.904593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LangGraph Single-Model Routing Agent (LLaMA or Qwen based on input)\n",
      "============================================================\n",
      "Using CUDA (NVIDIA GPU)\n",
      "Loading Llama: meta-llama/Llama-3.2-1B-Instruct ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f97cf7ee5849cdad53b48688538e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama loaded.\n",
      "Loading Qwen: Qwen/Qwen2.5-1.5B-Instruct ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60e7752473349f4aba6f5d3e1a8b4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen loaded.\n",
      "\n",
      "==================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "==================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey Qwen, how are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ§  Qwen Response\n",
      "============================================================\n",
      "User: , how are you?\n",
      "Assistant: As an AI, I don't have feelings or emotions like humans do. However, I'm here to help you with any questions or information you need! How can I assist you today? Let me know if there's anything specific you'd like to discuss or learn about. I'll do my best to provide accurate and helpful responses based on the data available to me. Is there something particular you're curious about? I may not be able to predict what you might ask, but I'll try to give a thoughtful response when possible. What would you like to find out?\n",
      "\n",
      "==================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "==================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is soccer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ¦™ LLaMA Response\n",
      "============================================================\n",
      "User: What is soccer?\n",
      "Assistant: Soccer, also known as association football, is a team sport played between two teams of 11 players using a round ball with the objective of scoring more goals than the opposing team by kicking or heading the ball into the opponent's goal.\n",
      "\n",
      "Would you like me to explain any specific aspect of soccer?\n",
      "\n",
      "==================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "==================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# langgraph_single_model_routing_fixed.py\n",
    "#\n",
    "# LangGraph agent that routes each user input to exactly one LLM:\n",
    "#  - If input starts with \"Hey Qwen\" (case-insensitive) -> route to Qwen\n",
    "#  - Otherwise -> route to Llama\n",
    "# Fixes:\n",
    "#  - Clears previous responses on new input to avoid stale output\n",
    "#  - Uses robust startswith matching for trigger phrase\n",
    "#  - Strips the trigger phrase before sending remainder to Qwen\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Tuple\n",
    "\n",
    "# -------------------------\n",
    "# Device selection\n",
    "# -------------------------\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA (NVIDIA GPU)\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS (Apple Silicon)\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return \"cpu\"\n",
    "\n",
    "# -------------------------\n",
    "# State\n",
    "# -------------------------\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    should_exit: bool\n",
    "    llama_response: str\n",
    "    qwen_response: str\n",
    "    verbose: bool\n",
    "\n",
    "# -------------------------\n",
    "# LLM loader with safe langchain fix\n",
    "# -------------------------\n",
    "def load_llm_wrapped(model_id: str, device: str):\n",
    "    \"\"\"\n",
    "    Load a HF model and wrap in an adapter exposing .invoke(prompt) -> str.\n",
    "    Raises RuntimeError on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "            device_map=device if device == \"cuda\" else None,\n",
    "        )\n",
    "        if device == \"mps\":\n",
    "            model = model.to(device)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Defensive langchain attributes to avoid AttributeError\n",
    "        try:\n",
    "            import langchain\n",
    "            if not hasattr(langchain, \"verbose\"):\n",
    "                langchain.verbose = False\n",
    "            if not hasattr(langchain, \"debug\"):\n",
    "                langchain.debug = False\n",
    "            if not hasattr(langchain, \"llm_cache\"):\n",
    "                langchain.llm_cache = None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        wrapped = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "        class Adapter:\n",
    "            def __init__(self, llm):\n",
    "                self.llm = llm\n",
    "            def invoke(self, prompt: str) -> str:\n",
    "                out = self.llm.invoke(prompt)\n",
    "                if isinstance(out, str):\n",
    "                    return out\n",
    "                try:\n",
    "                    if isinstance(out, list) and len(out) > 0 and isinstance(out[0], dict):\n",
    "                        if \"generated_text\" in out[0]:\n",
    "                            return out[0][\"generated_text\"]\n",
    "                    if isinstance(out, dict) and \"generated_text\" in out:\n",
    "                        return out[\"generated_text\"]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return str(out)\n",
    "\n",
    "        return Adapter(wrapped)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load {model_id}: {e}\") from e\n",
    "\n",
    "# -------------------------\n",
    "# Create models (Llama + Qwen/ fallback)\n",
    "# -------------------------\n",
    "def create_models() -> Tuple[object, object]:\n",
    "    device = get_device()\n",
    "\n",
    "    # Attempt to load Llama\n",
    "    llama_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    llama_llm = None\n",
    "    try:\n",
    "        print(f\"Loading Llama: {llama_id} ...\")\n",
    "        llama_llm = load_llm_wrapped(llama_id, device)\n",
    "        print(\"Llama loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load Llama ({llama_id}): {e}\")\n",
    "        llama_llm = None\n",
    "\n",
    "    # Attempt to load Qwen; fall back to small local model if needed\n",
    "    qwen_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    qwen_llm = None\n",
    "    try:\n",
    "        print(f\"Loading Qwen: {qwen_id} ...\")\n",
    "        qwen_llm = load_llm_wrapped(qwen_id, device)\n",
    "        print(\"Qwen loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load Qwen ({qwen_id}): {e}\")\n",
    "        print(\"Falling back to lightweight local model (gpt2) for Qwen branch (no tokens required).\")\n",
    "        fallback_id = \"gpt2\"\n",
    "        try:\n",
    "            qwen_llm = load_llm_wrapped(fallback_id, device)\n",
    "            print(\"Fallback (gpt2) loaded for Qwen branch.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to load fallback model {fallback_id}: {e2}\")\n",
    "            class QwenFallback:\n",
    "                def invoke(self, prompt: str) -> str:\n",
    "                    return \"[QWEN-FALLBACK] No local model available. Install transformers and a small model like 'gpt2'.\"\n",
    "            qwen_llm = QwenFallback()\n",
    "\n",
    "    # If Llama failed to load, use qwen/fallback as a substitute so graph still runs\n",
    "    if llama_llm is None:\n",
    "        print(\"Using the Qwen/fallback model for the Llama branch as well (Llama unavailable).\")\n",
    "        llama_llm = qwen_llm\n",
    "\n",
    "    return llama_llm, qwen_llm\n",
    "\n",
    "# -------------------------\n",
    "# Create graph with single-model routing (fixed)\n",
    "# -------------------------\n",
    "def create_graph(llama_llm, qwen_llm):\n",
    "    def get_user_input(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering get_user_input\")\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"> \", end=\"\")\n",
    "        user_input = input()\n",
    "\n",
    "        lowered = user_input.strip().lower()\n",
    "        if lowered == \"verbose\":\n",
    "            print(\"Verbose tracing enabled.\")\n",
    "            # clear previous responses on a new input\n",
    "            return {\"user_input\": user_input, \"should_exit\": False, \"verbose\": True, \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "        if lowered == \"quiet\":\n",
    "            print(\"Verbose tracing disabled.\")\n",
    "            return {\"user_input\": user_input, \"should_exit\": False, \"verbose\": False, \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "        if lowered in (\"quit\", \"exit\", \"q\"):\n",
    "            print(\"Goodbye!\")\n",
    "            return {\"user_input\": user_input, \"should_exit\": True, \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "        # For any other input, clear previous model outputs to avoid stale prints\n",
    "        return {\"user_input\": user_input, \"should_exit\": False, \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "    def route_after_input(state: AgentState) -> str:\n",
    "        # 3-way branch: END, back-to-input (empty), or call appropriate model\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input:\", {\"should_exit\": state.get(\"should_exit\"), \"user_input\": repr(state.get(\"user_input\"))})\n",
    "        if state.get(\"should_exit\", False):\n",
    "            return END\n",
    "\n",
    "        raw = str(state.get(\"user_input\", \"\"))\n",
    "        if raw.strip() == \"\":\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] empty input -> looping back to get_user_input\")\n",
    "            print(\"[NOTICE] Empty input received â€” please type something.\")\n",
    "            return \"get_user_input\"\n",
    "\n",
    "        # Robust trigger detection: startswith after lstrip\n",
    "        stripped_leading = raw.lstrip()\n",
    "        if stripped_leading.lower().startswith(\"hey qwen\"):\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] route_after_input -> call_qwen (matched 'Hey Qwen')\")\n",
    "            return \"call_qwen\"\n",
    "        else:\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] route_after_input -> call_llama (default)\")\n",
    "            return \"call_llama\"\n",
    "\n",
    "    def call_llama(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] call_llama invoked\")\n",
    "        prompt = f\"User: {state['user_input']}\\nAssistant:\"\n",
    "        response = llama_llm.invoke(prompt)\n",
    "        return {\"llama_response\": response}\n",
    "\n",
    "    def call_qwen(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] call_qwen invoked\")\n",
    "        # Strip the trigger phrase \"Hey Qwen\" (case-insensitive) before passing to model\n",
    "        raw = str(state.get(\"user_input\", \"\"))\n",
    "        stripped_leading = raw.lstrip()\n",
    "        lower_leading = stripped_leading.lower()\n",
    "        if lower_leading.startswith(\"hey qwen\"):\n",
    "            # remove only the leading phrase 'Hey Qwen' (length 8), then strip leading separators\n",
    "            remainder = stripped_leading[8:].lstrip()\n",
    "        else:\n",
    "            remainder = stripped_leading\n",
    "\n",
    "        # If remainder is empty (user only typed the trigger), still call model with original raw input\n",
    "        model_input = remainder if remainder.strip() != \"\" else raw\n",
    "\n",
    "        prompt = f\"User: {model_input}\\nAssistant:\"\n",
    "        response = qwen_llm.invoke(prompt)\n",
    "        return {\"qwen_response\": response}\n",
    "\n",
    "    def print_both_responses(state: AgentState) -> dict:\n",
    "        \"\"\"\n",
    "        Print only the model sections that actually have text.\n",
    "        If both are empty, print a short notice instead.\n",
    "        \"\"\"\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] print_both_responses invoked\")\n",
    "    \n",
    "        llama_text = (state.get(\"llama_response\") or \"\").strip()\n",
    "        qwen_text  = (state.get(\"qwen_response\")  or \"\").strip()\n",
    "    \n",
    "        if not llama_text and not qwen_text:\n",
    "            print(\"\\n[NOTICE] No model produced output this turn.\")\n",
    "            return {\"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "    \n",
    "        if llama_text:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ðŸ¦™ LLaMA Response\")\n",
    "            print(\"=\" * 60)\n",
    "            print(llama_text)\n",
    "    \n",
    "        if qwen_text:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ðŸ§  Qwen Response\")\n",
    "            print(\"=\" * 60)\n",
    "            print(qwen_text)\n",
    "\n",
    "        # Clear both responses so the next turn starts fresh\n",
    "        return {\"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "    # Build graph\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"get_user_input\", get_user_input)\n",
    "    graph.add_node(\"call_llama\", call_llama)\n",
    "    graph.add_node(\"call_qwen\", call_qwen)\n",
    "    graph.add_node(\"print_both_responses\", print_both_responses)\n",
    "\n",
    "    graph.add_edge(START, \"get_user_input\")\n",
    "\n",
    "    graph.add_conditional_edges(\n",
    "        \"get_user_input\",\n",
    "        route_after_input,\n",
    "        {\n",
    "            \"get_user_input\": \"get_user_input\",  # empty input -> loop\n",
    "            \"call_qwen\": \"call_qwen\",            # Hey Qwen -> qwen\n",
    "            \"call_llama\": \"call_llama\",          # default -> llama\n",
    "            END: END,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Both model nodes converge to the print node\n",
    "    graph.add_edge(\"call_llama\", \"print_both_responses\")\n",
    "    graph.add_edge(\"call_qwen\", \"print_both_responses\")\n",
    "\n",
    "    # Loop back to input\n",
    "    graph.add_edge(\"print_both_responses\", \"get_user_input\")\n",
    "\n",
    "    return graph.compile()\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LangGraph Single-Model Routing Agent (LLaMA or Qwen based on input)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llama_llm, qwen_llm = create_models()\n",
    "\n",
    "    graph = create_graph(llama_llm, qwen_llm)\n",
    "\n",
    "    initial_state: AgentState = {\n",
    "        \"user_input\": \"\",\n",
    "        \"should_exit\": False,\n",
    "        \"llama_response\": \"\",\n",
    "        \"qwen_response\": \"\",\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    graph.invoke(initial_state)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
