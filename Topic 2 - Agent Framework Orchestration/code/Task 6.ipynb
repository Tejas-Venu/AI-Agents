{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T02:53:36.393972Z",
     "iopub.status.busy": "2026-02-22T02:53:36.393290Z",
     "iopub.status.idle": "2026-02-22T02:54:54.117794Z",
     "shell.execute_reply": "2026-02-22T02:54:54.116861Z",
     "shell.execute_reply.started": "2026-02-22T02:53:36.393940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Dual-LLM LangGraph Agent with Shared History & Sanitization (Llama <-> Qwen)\n",
      "================================================================================\n",
      "Using CUDA (NVIDIA GPU)\n",
      "Loading Llama (meta-llama/Llama-3.2-1B-Instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcc1e85796c4d4b96544fc33c62f1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama loaded.\n",
      "Attempting to load Qwen (Qwen/Qwen2.5-1.5B-Instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc92d8b2cc504c2aa56ed8a77d9266c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen loaded.\n",
      "\n",
      "============================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "Special commands: history, reset\n",
      "============================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the best ice cream flavor?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ¦™ LLaMA Response\n",
      "======================================================================\n",
      "[System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\n",
      "[System] You are an assistant participating in a multi-agent chat: Human, Llama, Qwen.\n",
      "User: Human: What is the best ice cream flavor?\n",
      "Assistant: Since you're looking for the best ice cream flavor, I'd recommend trying unique and exotic flavors like matcha green tea, cardamom pistachio, or black sesame. If you're in the mood for something classic, you can't go wrong with vanilla or chocolate. What's your personal favorite?\n",
      "\n",
      "============================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "Special commands: history, reset\n",
      "============================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey Qwen, what do you think?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ§  Qwen Response\n",
      "======================================================================\n",
      "[System] You are Qwen (assistant). Participants: Human and Llama. When others speak, their name will be prefixed (e.g. 'Llama: ...'). Answer helpfully and concisely.\n",
      "[System] You are an assistant participating in a multi-agent chat: Human, Llama, Qwen.\n",
      "User: Human: What is the best ice cream flavor?\n",
      "User: Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\n",
      "[System] You are an assistant participating in a multi-agent chat: Human, Llama, Qwen.\n",
      "User: Human: What is the best ice cream flavor?\n",
      "Assistant: Since you're looking for the best ice cream flavor, I'd recommend trying unique and exotic flavors like matcha green tea, cardamom pistachio, or black sesame. If you're in the mood for something classic, you can't go wrong with vanilla or chocolate. What's your personal favorite?\n",
      "User: Human: , what do you think?\n",
      "Assistant: As an AI language model, I don't have personal preferences, but I can provide some general insights. Vanilla and chocolate are universally loved due to their sweetness and rich flavors. However, if we consider uniqueness, perhaps you might try flavors inspired by cultural traditions such as coconut macarons, lavender honey, or pistachio caramel. These options could offer a fresh taste experience that sets them apart from more common flavors. Do any of these sound appealing?\n",
      "\n",
      "============================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "Special commands: history, reset\n",
      "============================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    }
   ],
   "source": [
    "# langgraph_dual_llm_history_sanitized.py\n",
    "#\n",
    "# Dual-LLM agent with shared canonical history, per-target Message-API translation,\n",
    "# and sanitization of model outputs to avoid repeated speaker prefixes.\n",
    "#\n",
    "# Interactive commands:\n",
    "#   verbose / quiet   -- toggle tracing\n",
    "#   history           -- print canonical history\n",
    "#   reset             -- reset canonical history to only the initial system message\n",
    "#   quit / exit / q   -- exit\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import traceback\n",
    "from typing import TypedDict, List, Dict, Any, Tuple\n",
    "\n",
    "# Defensive imports (torch / transformers). If unavailable the script will fall back.\n",
    "TRANSFORMERS_OK = True\n",
    "try:\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    TRANSFORMERS_OK = False\n",
    "    print(\"[WARN] torch import failed; running in fallback mode.\")\n",
    "    traceback.print_exception(type(e), e, e.__traceback__, file=sys.stdout)\n",
    "\n",
    "if TRANSFORMERS_OK:\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    except Exception as e:\n",
    "        TRANSFORMERS_OK = False\n",
    "        print(\"[WARN] transformers import failed; running in fallback mode.\")\n",
    "        traceback.print_exception(type(e), e, e.__traceback__, file=sys.stdout)\n",
    "\n",
    "# langgraph\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, START, END\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Could not import langgraph.graph. Install langgraph or run in an environment with it available.\")\n",
    "    traceback.print_exception(type(e), e, e.__traceback__, file=sys.stdout)\n",
    "    raise\n",
    "\n",
    "# -------------------------\n",
    "# Types\n",
    "# -------------------------\n",
    "SpeakerMessage = Dict[str, str]  # {\"speaker\": \"Human\"|\"Llama\"|\"Qwen\"|\"Tool\"|\"System\", \"content\": \"...\"}\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    should_exit: bool\n",
    "    last_model: str\n",
    "    verbose: bool\n",
    "    messages: List[SpeakerMessage]\n",
    "    llama_response: str\n",
    "    qwen_response: str\n",
    "\n",
    "# -------------------------\n",
    "# Sanitization utilities\n",
    "# -------------------------\n",
    "def sanitize_model_output(text: Any, speaker: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean model output so it doesn't contain embedded \"Llama:\"/\"Qwen:\"/ \"Assistant:\" labels\n",
    "    or repeated speaker prefixes. Returns a stripped, normalized string.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if not isinstance(text, str):\n",
    "        try:\n",
    "            text = str(text)\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove repeated leading speaker labels like \"Llama: Llama: Hello\" or \"Qwen: Assistant: Hello\"\n",
    "    # Build pattern for speaker and generic Assistant\n",
    "    sp = re.escape(speaker)\n",
    "    # Remove repeated leading occurrences of the speaker label or \"Assistant:\"\n",
    "    text = re.sub(rf'^(?:\\s*(?:{sp}|Assistant)\\s*:)+\\s*', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove trailing stray labels like \" ... Llama:\" or \" ... Assistant:\"\n",
    "    text = re.sub(rf'(?:\\s*(?:{sp}|Assistant)\\s*:\\s*)+$', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Collapse multiple empty lines\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def ensure_speaker_prefix(content: str, speaker_label: str) -> str:\n",
    "    \"\"\"\n",
    "    When building role messages, prefix content with \"Speaker: \" only if it's not already present.\n",
    "    Example: if content already begins with \"Llama:\" (case-insensitive) do not add another prefix.\n",
    "    \"\"\"\n",
    "    if content is None:\n",
    "        content = \"\"\n",
    "    content = content.strip()\n",
    "    if content == \"\":\n",
    "        return content\n",
    "    # If already prefixed \"Llama:\" or \"Qwen:\" or \"Human:\", return as-is\n",
    "    if re.match(rf'^\\s*{re.escape(speaker_label)}\\s*:', content, flags=re.IGNORECASE):\n",
    "        return content\n",
    "    if re.match(r'^\\s*Human\\s*:', content, flags=re.IGNORECASE):\n",
    "        return content\n",
    "    return f\"{speaker_label}: {content}\"\n",
    "\n",
    "# -------------------------\n",
    "# Device selection\n",
    "# -------------------------\n",
    "def get_device() -> str:\n",
    "    if TRANSFORMERS_OK:\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                print(\"Using CUDA (NVIDIA GPU)\")\n",
    "                return \"cuda\"\n",
    "            elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "                print(\"Using MPS (Apple Silicon)\")\n",
    "                return \"mps\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"Using CPU\")\n",
    "    return \"cpu\"\n",
    "\n",
    "# -------------------------\n",
    "# Fallback model for testing when HF stack unavailable\n",
    "# -------------------------\n",
    "class SimpleFallbackModel:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        preview = (prompt.replace(\"\\n\", \" \")[:240] + \"...\") if prompt else \"[no prompt]\"\n",
    "        return f\"[{self.name.upper()}-FALLBACK] No HF model available. Prompt preview: {preview}\"\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline adapter for HF pipeline outputs\n",
    "# -------------------------\n",
    "class PipelineAdapter:\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        try:\n",
    "            out = self.pipe(prompt)\n",
    "            if isinstance(out, list) and len(out) > 0 and isinstance(out[0], dict):\n",
    "                txt = out[0].get(\"generated_text\")\n",
    "                if isinstance(txt, str):\n",
    "                    return txt\n",
    "            if isinstance(out, str):\n",
    "                return out\n",
    "            return str(out)\n",
    "        except Exception as e:\n",
    "            return f\"[MODEL-ERROR] pipeline invocation failed: {e}\"\n",
    "\n",
    "# -------------------------\n",
    "# Load HF model (safe)\n",
    "# -------------------------\n",
    "def load_and_wrap(model_id: str, device: str):\n",
    "    if not TRANSFORMERS_OK:\n",
    "        raise RuntimeError(\"Transformers not available\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(None if device == \"cpu\" else None),\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    )\n",
    "    if device == \"mps\":\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "        except Exception:\n",
    "            pass\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=getattr(tokenizer, \"eos_token_id\", None),\n",
    "    )\n",
    "    return PipelineAdapter(pipe)\n",
    "\n",
    "# -------------------------\n",
    "# Create models (Llama + Qwen) with fallbacks\n",
    "# -------------------------\n",
    "def create_models() -> Tuple[Any, Any]:\n",
    "    device = get_device()\n",
    "    # Llama\n",
    "    llama_llm = None\n",
    "    if TRANSFORMERS_OK:\n",
    "        try:\n",
    "            print(\"Loading Llama (meta-llama/Llama-3.2-1B-Instruct) ...\")\n",
    "            llama_llm = load_and_wrap(\"meta-llama/Llama-3.2-1B-Instruct\", device)\n",
    "            print(\"Llama loaded.\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load Llama, falling back to gpt2 or SimpleFallbackModel:\", e)\n",
    "            traceback.print_exc()\n",
    "            try:\n",
    "                llama_llm = load_and_wrap(\"gpt2\", device)\n",
    "                print(\"gpt2 loaded as Llama fallback.\")\n",
    "            except Exception:\n",
    "                llama_llm = SimpleFallbackModel(\"llama\")\n",
    "    else:\n",
    "        llama_llm = SimpleFallbackModel(\"llama\")\n",
    "\n",
    "    # Qwen (attempt, but treat None as disabled)\n",
    "    qwen_llm = None\n",
    "    if TRANSFORMERS_OK:\n",
    "        try:\n",
    "            print(\"Attempting to load Qwen (Qwen/Qwen2.5-1.5B-Instruct) ...\")\n",
    "            qwen_llm = load_and_wrap(\"Qwen/Qwen2.5-1.5B-Instruct\", device)\n",
    "            print(\"Qwen loaded.\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Could not load Qwen (it will be treated as disabled):\", e)\n",
    "            traceback.print_exc()\n",
    "            qwen_llm = None\n",
    "    else:\n",
    "        qwen_llm = None\n",
    "\n",
    "    return llama_llm, qwen_llm\n",
    "\n",
    "# -------------------------\n",
    "# Convert canonical messages into role-based messages for target\n",
    "# -------------------------\n",
    "def build_role_messages_for_target(target: str, canonical_messages: List[SpeakerMessage]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Convert canonical history into role-messages for a given target ('Llama' or 'Qwen').\n",
    "    - System -> role: system\n",
    "    - Tool -> role: tool\n",
    "    - Human -> role: user with content \"Human: <content>\"\n",
    "    - Prior LLM utterance -> assistant if it was from target; otherwise user with \"<Speaker>: <content>\"\n",
    "    Uses ensure_speaker_prefix to avoid double-labeling.\n",
    "    \"\"\"\n",
    "    role_msgs: List[Dict[str, str]] = []\n",
    "    # system messages first (preserve order)\n",
    "    for m in canonical_messages:\n",
    "        sp = m.get(\"speaker\", \"\")\n",
    "        content = m.get(\"content\", \"\") or \"\"\n",
    "        if sp.lower() == \"system\":\n",
    "            role_msgs.append({\"role\": \"system\", \"content\": content})\n",
    "    # then others in original order\n",
    "    for m in canonical_messages:\n",
    "        sp = m.get(\"speaker\", \"\")\n",
    "        content = m.get(\"content\", \"\") or \"\"\n",
    "        sp_norm = sp.lower()\n",
    "        if sp_norm == \"system\":\n",
    "            continue\n",
    "        if sp_norm == \"tool\":\n",
    "            role_msgs.append({\"role\": \"tool\", \"content\": content})\n",
    "            continue\n",
    "        if sp_norm == \"human\":\n",
    "            # Human messages become user messages prefixed with \"Human: \"\n",
    "            cont = content\n",
    "            if not re.match(r'^\\s*Human\\s*:', cont, flags=re.IGNORECASE):\n",
    "                cont = f\"Human: {cont}\"\n",
    "            role_msgs.append({\"role\": \"user\", \"content\": cont})\n",
    "            continue\n",
    "        # LLM speaker (Llama or Qwen)\n",
    "        speaker_label = sp  # e.g., \"Llama\" or \"Qwen\"\n",
    "        # prevent double-prefixing: ensure content does not already start with \"Llama:\" etc.\n",
    "        prefixed = ensure_speaker_prefix(content, speaker_label)\n",
    "        if speaker_label.lower() == target.lower():\n",
    "            role_msgs.append({\"role\": \"assistant\", \"content\": prefixed})\n",
    "        else:\n",
    "            role_msgs.append({\"role\": \"user\", \"content\": prefixed})\n",
    "    return role_msgs\n",
    "\n",
    "def prompt_from_role_messages(role_messages: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Convert role messages into a single textual prompt.\n",
    "    Format: system lines first as [System], then 'User: ...' / '[Tool] ...' / 'Assistant: ...'\n",
    "    Ends with 'Assistant:' cue.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "    for rm in role_messages:\n",
    "        if rm[\"role\"] == \"system\":\n",
    "            lines.append(f\"[System] {rm['content']}\")\n",
    "    for rm in role_messages:\n",
    "        role = rm[\"role\"]\n",
    "        content = rm[\"content\"]\n",
    "        if role == \"system\":\n",
    "            continue\n",
    "        if role == \"user\":\n",
    "            lines.append(f\"User: {content}\")\n",
    "        elif role == \"tool\":\n",
    "            lines.append(f\"[Tool] {content}\")\n",
    "        elif role == \"assistant\":\n",
    "            lines.append(f\"Assistant: {content}\")\n",
    "    lines.append(\"Assistant:\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def system_message_for_target(target: str) -> SpeakerMessage:\n",
    "    if target.lower() == \"llama\":\n",
    "        return {\"speaker\": \"System\", \"content\": \"You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\"}\n",
    "    else:\n",
    "        return {\"speaker\": \"System\", \"content\": \"You are Qwen (assistant). Participants: Human and Llama. When others speak, their name will be prefixed (e.g. 'Llama: ...'). Answer helpfully and concisely.\"}\n",
    "\n",
    "# -------------------------\n",
    "# Graph nodes\n",
    "# -------------------------\n",
    "def create_graph(llama_llm, qwen_llm):\n",
    "    \"\"\"\n",
    "    Graph nodes:\n",
    "      - get_user_input: gather input, append Human message to canonical messages\n",
    "      - route_after_input: empty -> loop, quit -> END, otherwise -> call_model\n",
    "      - call_model: build per-target prompt, call model (handle Qwen disabled), sanitize output, append to canonical messages\n",
    "      - print_response: print the last model reply (from canonical messages or snippet fields)\n",
    "    \"\"\"\n",
    "\n",
    "    def get_user_input(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering get_user_input\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\")\n",
    "        print(\"Special commands: history, reset\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"> \", end=\"\")\n",
    "        raw = input()\n",
    "\n",
    "        low = raw.strip().lower()\n",
    "        if low == \"verbose\":\n",
    "            return {\"user_input\": raw, \"should_exit\": False, \"verbose\": True}\n",
    "        if low == \"quiet\":\n",
    "            return {\"user_input\": raw, \"should_exit\": False, \"verbose\": False}\n",
    "        if low in (\"quit\", \"exit\", \"q\"):\n",
    "            return {\"user_input\": raw, \"should_exit\": True}\n",
    "        if low == \"history\":\n",
    "            msgs = state.get(\"messages\", [])\n",
    "            print(\"\\n[HISTORY] canonical messages (most recent last):\")\n",
    "            for m in msgs:\n",
    "                print(f\"  {m.get('speaker')}: {m.get('content')}\")\n",
    "            return {\"user_input\": \"\", \"should_exit\": False}\n",
    "        if low == \"reset\":\n",
    "            sys_msg = {\"speaker\": \"System\", \"content\": \"You are an assistant participating in a multi-agent chat: Human, Llama, Qwen.\"}\n",
    "            print(\"[NOTICE] History reset.\")\n",
    "            return {\"user_input\": \"\", \"should_exit\": False, \"messages\": [sys_msg], \"last_model\": \"\"}\n",
    "\n",
    "        # Normal input: append human message to canonical history (we don't strip Hey Qwen here)\n",
    "        msgs = list(state.get(\"messages\", []))\n",
    "        msgs.append({\"speaker\": \"Human\", \"content\": raw})\n",
    "        return {\"user_input\": raw, \"should_exit\": False, \"messages\": msgs, \"last_model\": \"\"}\n",
    "\n",
    "    def route_after_input(state: AgentState) -> str:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input - user_input:\", repr(state.get(\"user_input\")))\n",
    "        if state.get(\"should_exit\", False):\n",
    "            return END\n",
    "        raw = str(state.get(\"user_input\", \"\") or \"\")\n",
    "        if raw.strip() == \"\":\n",
    "            print(\"[NOTICE] Empty input received â€” please type something.\")\n",
    "            return \"get_user_input\"\n",
    "        return \"call_model\"\n",
    "\n",
    "    def call_model(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] call_model invoked\")\n",
    "\n",
    "        raw = str(state.get(\"user_input\", \"\") or \"\")\n",
    "        canonical_msgs = list(state.get(\"messages\", []))\n",
    "\n",
    "        # Determine target\n",
    "        target_initial = \"Qwen\" if raw.lstrip().lower().startswith(\"hey qwen\") else \"Llama\"\n",
    "        target = target_initial\n",
    "\n",
    "        # If Qwen selected but unavailable, append a Tool notice and route to Llama\n",
    "        model_obj = qwen_llm if target.lower() == \"qwen\" else llama_llm\n",
    "        if target.lower() == \"qwen\" and model_obj is None:\n",
    "            # Add tool message and switch target to Llama\n",
    "            canonical_msgs.append({\"speaker\": \"Tool\", \"content\": \"Qwen is disabled in this runtime; routed to Llama instead.\"})\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] Qwen is disabled; appended Tool message and routing to Llama\")\n",
    "            target = \"Llama\"\n",
    "            model_obj = llama_llm\n",
    "\n",
    "        # Build role messages for the chosen target\n",
    "        role_msgs = build_role_messages_for_target(target, canonical_msgs)\n",
    "        sys_msg = system_message_for_target(target)\n",
    "        role_msgs_with_sys = [{\"role\": \"system\", \"content\": sys_msg[\"content\"]}] + role_msgs\n",
    "\n",
    "        # If the original human message began with \"Hey Qwen\", remove that cue from the prompt (but keep canonical history unchanged)\n",
    "        if raw.lstrip().lower().startswith(\"hey qwen\"):\n",
    "            # make a modifiable copy and strip \"Hey Qwen\" from the last human entry in role_msgs\n",
    "            role_msgs_mod = [dict(rm) for rm in role_msgs]\n",
    "            for i in range(len(role_msgs_mod) - 1, -1, -1):\n",
    "                rm = role_msgs_mod[i]\n",
    "                if rm[\"role\"] == \"user\" and rm[\"content\"].lower().startswith(\"human:\"):\n",
    "                    after = rm[\"content\"][len(\"Human:\"):].lstrip()\n",
    "                    if after.lower().startswith(\"hey qwen\"):\n",
    "                        new_after = after[len(\"hey qwen\"):].lstrip()\n",
    "                        rm[\"content\"] = f\"Human: {new_after}\" if new_after != \"\" else rm[\"content\"]\n",
    "                    break\n",
    "            role_msgs_with_sys = [{\"role\": \"system\", \"content\": sys_msg[\"content\"]}] + role_msgs_mod\n",
    "\n",
    "        prompt_text = prompt_from_role_messages(role_msgs_with_sys)\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Prompt for target\", target, \"(truncated):\\n\", prompt_text[:1200])\n",
    "\n",
    "        # Call the model safely\n",
    "        try:\n",
    "            response_text = model_obj.invoke(prompt_text)\n",
    "        except Exception as e:\n",
    "            response_text = f\"[MODEL-ERROR] {e}\"\n",
    "            print(\"[ERROR] Model invocation failed:\", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Sanitize model output before storing\n",
    "        response_text_clean = sanitize_model_output(response_text, target)\n",
    "\n",
    "        # Append sanitized reply to canonical messages\n",
    "        canonical_msgs.append({\"speaker\": target, \"content\": response_text_clean})\n",
    "\n",
    "        # Build snippet fields\n",
    "        llama_snip = response_text_clean if target.lower() == \"llama\" else \"\"\n",
    "        qwen_snip = response_text_clean if target.lower() == \"qwen\" else \"\"\n",
    "\n",
    "        return {\n",
    "            \"messages\": canonical_msgs,\n",
    "            \"last_model\": target,\n",
    "            \"llama_response\": llama_snip,\n",
    "            \"qwen_response\": qwen_snip\n",
    "        }\n",
    "\n",
    "    def print_response(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] print_response invoked; last_model=\", state.get(\"last_model\"))\n",
    "\n",
    "        last_model = state.get(\"last_model\", \"\")\n",
    "        llama_text = (state.get(\"llama_response\") or \"\").strip()\n",
    "        qwen_text = (state.get(\"qwen_response\") or \"\").strip()\n",
    "\n",
    "        printed = False\n",
    "        if last_model.lower() == \"llama\" and llama_text:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ¦™ LLaMA Response\")\n",
    "            print(\"=\" * 70)\n",
    "            print(llama_text)\n",
    "            printed = True\n",
    "        elif last_model.lower() == \"qwen\" and qwen_text:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ§  Qwen Response\")\n",
    "            print(\"=\" * 70)\n",
    "            print(qwen_text)\n",
    "            printed = True\n",
    "        else:\n",
    "            # Fallback: print last canonical LLM message\n",
    "            msgs = state.get(\"messages\", []) or []\n",
    "            if msgs:\n",
    "                last_msg = msgs[-1]\n",
    "                sp = last_msg.get(\"speaker\", \"\")\n",
    "                cont = (last_msg.get(\"content\") or \"\").strip()\n",
    "                if sp.lower() in (\"llama\", \"qwen\") and cont:\n",
    "                    header = \"ðŸ¦™ LLaMA Response\" if sp.lower() == \"llama\" else \"ðŸ§  Qwen Response\"\n",
    "                    print(\"\\n\" + \"=\" * 70)\n",
    "                    print(header)\n",
    "                    print(\"=\" * 70)\n",
    "                    print(cont)\n",
    "                    printed = True\n",
    "\n",
    "        if not printed:\n",
    "            print(\"\\n[NOTICE] No model produced output this turn.\")\n",
    "            print(\"[DIAGNOSTIC] last_model:\", repr(last_model))\n",
    "            msgs = state.get(\"messages\", [])\n",
    "            print(\"[DIAGNOSTIC] messages count:\", len(msgs))\n",
    "            if len(msgs) > 0:\n",
    "                print(\"[DIAGNOSTIC] last messages (most recent last):\")\n",
    "                for m in msgs[-6:]:\n",
    "                    print(f\"  {m.get('speaker')}: {m.get('content')!s}\")\n",
    "\n",
    "        # Clear snippet fields for next turn\n",
    "        return {\"last_model\": \"\", \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "    # Build graph\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"get_user_input\", get_user_input)\n",
    "    graph.add_node(\"call_model\", call_model)\n",
    "    graph.add_node(\"print_response\", print_response)\n",
    "\n",
    "    graph.add_edge(START, \"get_user_input\")\n",
    "    graph.add_conditional_edges(\n",
    "        \"get_user_input\",\n",
    "        route_after_input,\n",
    "        {\n",
    "            \"get_user_input\": \"get_user_input\",\n",
    "            \"call_model\": \"call_model\",\n",
    "            END: END,\n",
    "        },\n",
    "    )\n",
    "    graph.add_edge(\"call_model\", \"print_response\")\n",
    "    graph.add_edge(\"print_response\", \"get_user_input\")\n",
    "\n",
    "    return graph.compile()\n",
    "\n",
    "# -------------------------\n",
    "# Sample demonstration (optional)\n",
    "# -------------------------\n",
    "SAMPLE_CONVERSATIONS = [\n",
    "    {\n",
    "        \"desc\": \"Human asks Llama then queries Qwen\",\n",
    "        \"steps\": [\n",
    "            {\"speaker\": \"Human\", \"content\": \"What is the best ice cream flavor?\"},\n",
    "            {\"speaker\": \"Llama\", \"content\": \"There is no single best flavor; vanilla is most versatile.\"},\n",
    "            {\"speaker\": \"Human\", \"content\": \"Hey Qwen, what do you think?\"},\n",
    "            {\"speaker\": \"Qwen\", \"content\": \"Chocolate is my favorite.\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "def print_sample_prompts():\n",
    "    for conv in SAMPLE_CONVERSATIONS:\n",
    "        msgs = [{\"speaker\": \"System\", \"content\": \"You are an assistant in a multi-agent chat: Human, Llama, Qwen.\"}]\n",
    "        for s in conv[\"steps\"]:\n",
    "            msgs.append({\"speaker\": s[\"speaker\"], \"content\": s[\"content\"]})\n",
    "        print(\"\\n=== Sample prompts ===\")\n",
    "        print(\"Prompt for Qwen (role-mapped):\")\n",
    "        r_q = build_role_messages_for_target(\"Qwen\", msgs)\n",
    "        r_q = [{\"role\": \"system\", \"content\": system_message_for_target(\"Qwen\")[\"content\"]}] + r_q\n",
    "        print(prompt_from_role_messages(r_q)[:1200])\n",
    "        print(\"\\nPrompt for Llama (role-mapped):\")\n",
    "        r_l = build_role_messages_for_target(\"Llama\", msgs)\n",
    "        r_l = [{\"role\": \"system\", \"content\": system_message_for_target(\"Llama\")[\"content\"]}] + r_l\n",
    "        print(prompt_from_role_messages(r_l)[:1200])\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Dual-LLM LangGraph Agent with Shared History & Sanitization (Llama <-> Qwen)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    llama_llm, qwen_llm = create_models()\n",
    "    graph = create_graph(llama_llm, qwen_llm)\n",
    "\n",
    "    # initial system message\n",
    "    system_msg = {\"speaker\": \"System\", \"content\": \"You are an assistant participating in a multi-agent chat: Human, Llama, Qwen.\"}\n",
    "    initial_state: AgentState = {\n",
    "        \"user_input\": \"\",\n",
    "        \"should_exit\": False,\n",
    "        \"last_model\": \"\",\n",
    "        \"verbose\": False,\n",
    "        \"messages\": [system_msg],\n",
    "        \"llama_response\": \"\",\n",
    "        \"qwen_response\": \"\",\n",
    "    }\n",
    "\n",
    "    if not TRANSFORMERS_OK or qwen_llm is None:\n",
    "        print(\"\\n[NOTICE] Running with fallbacks or Qwen disabled. You can still test routing & history.\\n\")\n",
    "        print_sample_prompts()\n",
    "\n",
    "    graph.invoke(initial_state)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
