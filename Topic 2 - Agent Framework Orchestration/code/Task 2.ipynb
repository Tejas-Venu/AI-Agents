{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:35:57.292232Z",
     "iopub.status.busy": "2026-02-21T21:35:57.291485Z",
     "iopub.status.idle": "2026-02-21T21:38:51.258282Z",
     "shell.execute_reply": "2026-02-21T21:38:51.257386Z",
     "shell.execute_reply.started": "2026-02-21T21:35:57.292200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LangGraph Simple Agent with Llama-3.2-1B-Instruct\n",
      "==================================================\n",
      "\n",
      "Using CUDA (NVIDIA GPU) for inference\n",
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "This may take a moment on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdec4ba20d084de988751feadb9d0fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Creating LangGraph...\n",
      "Graph created successfully!\n",
      "\n",
      "Saving graph visualization...\n",
      "Graph image saved to lg_graph.png\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your input...\n",
      "\n",
      "--------------------------------------------------\n",
      "LLM Response:\n",
      "--------------------------------------------------\n",
      "User: \n",
      "Assistant: \n",
      "\n",
      "You are currently in a peaceful forest, surrounded by tall trees and a gentle stream. The sun is shining down on you, casting dappled shadows on the forest floor. A gentle breeze rustles the leaves, and the sound of birds chirping fills the air.\n",
      "\n",
      "You are feeling quite relaxed and content, and you have just finished a nice meal of wild berries and nuts. You are sitting on a large rock, and you notice that you are not alone. A small rabbit appears from behind a nearby bush, twitching its whiskers and looking around cautiously.\n",
      "\n",
      "What do you want to do?\n",
      "\n",
      "A) Invite the rabbit to join you on the rock\n",
      "B) Keep the rabbit at a distance, as you're not sure what it might be looking for\n",
      "C) Try to catch the rabbit to eat it\n",
      "D) Ignore the rabbit and continue to relax\n",
      "\n",
      "What is your choice?\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your input...\n",
      "\n",
      "--------------------------------------------------\n",
      "LLM Response:\n",
      "--------------------------------------------------\n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "\n",
      "Note: Since we are in a text-based environment, I will be responding with a brief message. The conversation will continue after each message.\n",
      "\n",
      "You are a librarian who has just received a call from a local high school. They are asking if they can borrow the library's collection for a school event. The event is a book fair, and they want to borrow 500 books from the library's collection for their students. What do you say to the librarian?\n",
      "\n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant: \n",
      "User: \n",
      "Assistant\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# langgraph_simple_agent.py\n",
    "# Simple LangGraph agent using a Hugging Face LLM.\n",
    "# Runtime tracing toggle: type \"verbose\" to enable tracing, \"quiet\" to disable.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Pick best available device: cuda > mps > cpu\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA (NVIDIA GPU) for inference\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS (Apple Silicon) for inference\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU for inference\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATE DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State that flows through nodes.\n",
    "    \"\"\"\n",
    "    user_input: str\n",
    "    should_exit: bool\n",
    "    llm_response: str\n",
    "    verbose: bool\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LLM CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"\n",
    "    Load Llama-3.2-1B-Instruct via Hugging Face, wrap with HuggingFacePipeline.\n",
    "    Apply defensive fixes to top-level langchain module attributes that langchain_core expects.\n",
    "    \"\"\"\n",
    "\n",
    "    device = get_device()\n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    print(\"This may take a moment on first run...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "        device_map=device if device == \"cuda\" else None,\n",
    "    )\n",
    "\n",
    "    if device == \"mps\":\n",
    "        # Move model to MPS explicitly\n",
    "        model = model.to(device)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Defensive compatibility fixes for langchain_core\n",
    "    # Some installations of `langchain` do not expose top-level attributes\n",
    "    # that langchain_core expects (verbose, debug, llm_cache). Ensure they exist.\n",
    "    # ------------------------------------------------------------------\n",
    "    try:\n",
    "        import langchain\n",
    "\n",
    "        # ensure verbose/debug exist\n",
    "        if not hasattr(langchain, \"verbose\"):\n",
    "            langchain.verbose = False\n",
    "        if not hasattr(langchain, \"debug\"):\n",
    "            langchain.debug = False\n",
    "\n",
    "        # ensure llm_cache exists (langchain_core may check this)\n",
    "        if not hasattr(langchain, \"llm_cache\"):\n",
    "            # Default to None (no global cache). This is safe.\n",
    "            langchain.llm_cache = None\n",
    "\n",
    "    except Exception:\n",
    "        # If anything goes wrong while trying to set attributes, ignore and continue.\n",
    "        # The worst case is the subsequent LangChain calls may raise errors; those will show up later.\n",
    "        pass\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GRAPH CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_graph(llm):\n",
    "    \"\"\"\n",
    "    Build the LangGraph state graph with nodes:\n",
    "      - get_user_input\n",
    "      - call_llm\n",
    "      - print_response\n",
    "    and a conditional route after get_user_input.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: get_user_input\n",
    "    # --------------------------\n",
    "    def get_user_input(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: get_user_input\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Enter your text (or 'quit' to exit):\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"\\n> \", end=\"\")\n",
    "\n",
    "        user_input = input()\n",
    "        lowered = user_input.strip().lower()\n",
    "\n",
    "        # Toggle tracing\n",
    "        if lowered == \"verbose\":\n",
    "            print(\"Verbose tracing enabled.\")\n",
    "            return {\"user_input\": user_input, \"should_exit\": False, \"verbose\": True}\n",
    "\n",
    "        if lowered == \"quiet\":\n",
    "            print(\"Verbose tracing disabled.\")\n",
    "            return {\"user_input\": user_input, \"should_exit\": False, \"verbose\": False}\n",
    "\n",
    "        # Exit commands\n",
    "        if lowered in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            return {\"user_input\": user_input, \"should_exit\": True}\n",
    "\n",
    "        # Default: continue to LLM; preserve existing verbose flag if present\n",
    "        return {\"user_input\": user_input, \"should_exit\": False}\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: call_llm\n",
    "    # --------------------------\n",
    "    def call_llm(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: call_llm\")\n",
    "            print(f\"[TRACE] State input user_input={repr(state.get('user_input'))}\")\n",
    "\n",
    "        user_input = state[\"user_input\"]\n",
    "        prompt = f\"User: {user_input}\\nAssistant:\"\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(f\"[TRACE] Prepared prompt (truncated to 120 chars): {repr(prompt[:120])}\")\n",
    "            print(\"[TRACE] Invoking LLM...\")\n",
    "\n",
    "        print(\"\\nProcessing your input...\")\n",
    "\n",
    "        # Use the wrapped HuggingFacePipeline LLM\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] LLM returned response (truncated to 120 chars):\")\n",
    "            print(repr(response[:120]))\n",
    "\n",
    "        return {\"llm_response\": response}\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: print_response\n",
    "    # --------------------------\n",
    "    def print_response(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: print_response\")\n",
    "            print(f\"[TRACE] llm_response length = {len(state.get('llm_response', ''))}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"LLM Response:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(state[\"llm_response\"])\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Exiting node: print_response\")\n",
    "\n",
    "        return {}\n",
    "\n",
    "    # --------------------------\n",
    "    # Router after input\n",
    "    # --------------------------\n",
    "    def route_after_input(state: AgentState) -> str:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input evaluating...\")\n",
    "            print(f\"[TRACE] should_exit={state.get('should_exit', False)} user_input={repr(state.get('user_input'))}\")\n",
    "\n",
    "        if state.get(\"should_exit\", False):\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] route_after_input -> END\")\n",
    "            return END\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input -> call_llm\")\n",
    "        return \"call_llm\"\n",
    "\n",
    "    # Build graph\n",
    "    graph_builder = StateGraph(AgentState)\n",
    "\n",
    "    graph_builder.add_node(\"get_user_input\", get_user_input)\n",
    "    graph_builder.add_node(\"call_llm\", call_llm)\n",
    "    graph_builder.add_node(\"print_response\", print_response)\n",
    "\n",
    "    graph_builder.add_edge(START, \"get_user_input\")\n",
    "\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"get_user_input\",\n",
    "        route_after_input,\n",
    "        {\n",
    "            \"call_llm\": \"call_llm\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    graph_builder.add_edge(\"call_llm\", \"print_response\")\n",
    "    graph_builder.add_edge(\"print_response\", \"get_user_input\")\n",
    "\n",
    "    graph = graph_builder.compile()\n",
    "    return graph\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE GRAPH IMAGE (Mermaid)\n",
    "# =============================================================================\n",
    "\n",
    "def save_graph_image(graph, filename=\"lg_graph.png\"):\n",
    "    try:\n",
    "        png_data = graph.get_graph(xray=True).draw_mermaid_png()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(png_data)\n",
    "        print(f\"Graph image saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save graph image: {e}\")\n",
    "        print(\"You may need to install additional dependencies: pip install grandalf\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"LangGraph Simple Agent with Llama-3.2-1B-Instruct\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "    # Create LLM (with defensive langchain fixes)\n",
    "    llm = create_llm()\n",
    "\n",
    "    # Build graph\n",
    "    print(\"\\nCreating LangGraph...\")\n",
    "    graph = create_graph(llm)\n",
    "    print(\"Graph created successfully!\")\n",
    "\n",
    "    # Save visualization\n",
    "    print(\"\\nSaving graph visualization...\")\n",
    "    save_graph_image(graph)\n",
    "\n",
    "    # Initial state\n",
    "    initial_state: AgentState = {\n",
    "        \"user_input\": \"\",\n",
    "        \"should_exit\": False,\n",
    "        \"llm_response\": \"\",\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    # Invoke the graph (it loops internally until user requests exit)\n",
    "    graph.invoke(initial_state)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:41:27.603521Z",
     "iopub.status.busy": "2026-02-21T21:41:27.602931Z",
     "iopub.status.idle": "2026-02-21T21:41:52.604703Z",
     "shell.execute_reply": "2026-02-21T21:41:52.603876Z",
     "shell.execute_reply.started": "2026-02-21T21:41:27.603487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LangGraph Simple Agent with Llama-3.2-1B-Instruct\n",
      "==================================================\n",
      "\n",
      "Using CUDA (NVIDIA GPU) for inference\n",
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "This may take a moment on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858d72d2a90e4d94abbfc88bc4bcafea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Creating LangGraph...\n",
      "Graph created successfully!\n",
      "\n",
      "Saving graph visualization...\n",
      "Graph image saved to lg_graph.png\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Empty input received — please type something (press Enter to try again).\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, how are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your input...\n",
      "\n",
      "--------------------------------------------------\n",
      "LLM Response:\n",
      "--------------------------------------------------\n",
      "User: Hello, how are you?\n",
      "Assistant: I'm doing well, thank you for asking! However, I'm a large language model, I don't have feelings like humans do. I'm functioning properly and ready to help with any questions or topics you'd like to discuss. How can I assist you today?\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# langgraph_simple_agent.py\n",
    "# Simple LangGraph agent using a Hugging Face LLM.\n",
    "# Runtime tracing toggle: type \"verbose\" to enable tracing, \"quiet\" to disable.\n",
    "# Now: empty input is never passed to the LLM. get_user_input has a 3-way branch:\n",
    "#   - quit/exit/q -> END\n",
    "#   - empty input -> loop back to get_user_input\n",
    "#   - non-empty input -> call_llm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Pick best available device: cuda > mps > cpu\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA (NVIDIA GPU) for inference\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS (Apple Silicon) for inference\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU for inference\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATE DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State that flows through nodes.\n",
    "    \"\"\"\n",
    "    user_input: str\n",
    "    should_exit: bool\n",
    "    llm_response: str\n",
    "    verbose: bool\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LLM CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"\n",
    "    Load Llama-3.2-1B-Instruct via Hugging Face, wrap with HuggingFacePipeline.\n",
    "    Apply defensive fixes to top-level langchain module attributes that langchain_core expects.\n",
    "    \"\"\"\n",
    "\n",
    "    device = get_device()\n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    print(\"This may take a moment on first run...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "        device_map=device if device == \"cuda\" else None,\n",
    "    )\n",
    "\n",
    "    if device == \"mps\":\n",
    "        # Move model to MPS explicitly\n",
    "        model = model.to(device)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Defensive compatibility fixes for langchain_core\n",
    "    # Some installations of `langchain` do not expose top-level attributes\n",
    "    # that langchain_core expects (verbose, debug, llm_cache). Ensure they exist.\n",
    "    # ------------------------------------------------------------------\n",
    "    try:\n",
    "        import langchain\n",
    "\n",
    "        # ensure verbose/debug exist\n",
    "        if not hasattr(langchain, \"verbose\"):\n",
    "            langchain.verbose = False\n",
    "        if not hasattr(langchain, \"debug\"):\n",
    "            langchain.debug = False\n",
    "\n",
    "        # ensure llm_cache exists (langchain_core may check this)\n",
    "        if not hasattr(langchain, \"llm_cache\"):\n",
    "            # Default to None (no global cache). This is safe.\n",
    "            langchain.llm_cache = None\n",
    "\n",
    "    except Exception:\n",
    "        # If anything goes wrong while trying to set attributes, ignore and continue.\n",
    "        pass\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GRAPH CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_graph(llm):\n",
    "    \"\"\"\n",
    "    Build the LangGraph state graph with nodes:\n",
    "      - get_user_input\n",
    "      - call_llm\n",
    "      - print_response\n",
    "    and a 3-way conditional route after get_user_input:\n",
    "      - END (quit)\n",
    "      - get_user_input (empty input -> loop back)\n",
    "      - call_llm (non-empty input -> proceed)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: get_user_input\n",
    "    # --------------------------\n",
    "    def get_user_input(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: get_user_input\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Enter your text (or 'quit' to exit):\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"\\n> \", end=\"\")\n",
    "\n",
    "        # Read raw input (do not strip yet; we use stripped value for emptiness check)\n",
    "        raw_input = input()\n",
    "        stripped = raw_input.strip()\n",
    "        lowered = stripped.lower()\n",
    "\n",
    "        # Toggle tracing commands (these are non-empty so will route to call_llm;\n",
    "        # if you prefer toggles to *not* be sent to the LLM, we can change that behavior)\n",
    "        if lowered == \"verbose\":\n",
    "            print(\"Verbose tracing enabled.\")\n",
    "            # store user_input as the literal the user typed (keeps history consistent)\n",
    "            # set verbose True so subsequent nodes trace\n",
    "            return {\"user_input\": raw_input, \"should_exit\": False, \"verbose\": True}\n",
    "\n",
    "        if lowered == \"quiet\":\n",
    "            print(\"Verbose tracing disabled.\")\n",
    "            return {\"user_input\": raw_input, \"should_exit\": False, \"verbose\": False}\n",
    "\n",
    "        # Exit commands\n",
    "        if lowered in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            return {\"user_input\": raw_input, \"should_exit\": True}\n",
    "\n",
    "        # Empty input: inform user and return state that will cause router to loop back to this node\n",
    "        if stripped == \"\":\n",
    "            # Friendly notice (so user knows why nothing happened)\n",
    "            print(\"[NOTICE] Empty input received — please type something (press Enter to try again).\")\n",
    "            # set user_input to the raw (empty) value; router will detect emptiness and route back\n",
    "            # do not change verbose flag here (preserve previous)\n",
    "            return {\"user_input\": raw_input, \"should_exit\": False}\n",
    "\n",
    "        # Non-empty input: proceed to LLM\n",
    "        return {\"user_input\": raw_input, \"should_exit\": False}\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: call_llm\n",
    "    # --------------------------\n",
    "    def call_llm(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: call_llm\")\n",
    "            print(f\"[TRACE] State input user_input={repr(state.get('user_input'))}\")\n",
    "\n",
    "        user_input = state[\"user_input\"]\n",
    "        prompt = f\"User: {user_input}\\nAssistant:\"\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(f\"[TRACE] Prepared prompt (truncated to 120 chars): {repr(prompt[:120])}\")\n",
    "            print(\"[TRACE] Invoking LLM...\")\n",
    "\n",
    "        print(\"\\nProcessing your input...\")\n",
    "\n",
    "        # Use the wrapped HuggingFacePipeline LLM\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] LLM returned response (truncated to 120 chars):\")\n",
    "            print(repr(response[:120]))\n",
    "\n",
    "        return {\"llm_response\": response}\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: print_response\n",
    "    # --------------------------\n",
    "    def print_response(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: print_response\")\n",
    "            print(f\"[TRACE] llm_response length = {len(state.get('llm_response', ''))}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"LLM Response:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(state[\"llm_response\"])\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Exiting node: print_response\")\n",
    "\n",
    "        return {}\n",
    "\n",
    "    # --------------------------\n",
    "    # Router after input (3-way)\n",
    "    # --------------------------\n",
    "    def route_after_input(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Return one of:\n",
    "          - END           (user wants to quit)\n",
    "          - \"get_user_input\" (empty input -> loop back)\n",
    "          - \"call_llm\"    (non-empty input -> proceed)\n",
    "        \"\"\"\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input evaluating...\")\n",
    "            print(f\"[TRACE] should_exit={state.get('should_exit', False)} user_input={repr(state.get('user_input'))}\")\n",
    "\n",
    "        # If user requested exit\n",
    "        if state.get(\"should_exit\", False):\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] route_after_input -> END\")\n",
    "            return END\n",
    "\n",
    "        # If the latest user_input is empty/whitespace -> loop back to input node\n",
    "        # We use .strip() to treat whitespace-only as empty\n",
    "        if str(state.get(\"user_input\", \"\")).strip() == \"\":\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] route_after_input -> get_user_input (empty input)\")\n",
    "            return \"get_user_input\"\n",
    "\n",
    "        # Otherwise proceed to LLM\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input -> call_llm\")\n",
    "        return \"call_llm\"\n",
    "\n",
    "    # Build graph with a conditional that maps three outcomes\n",
    "    graph_builder = StateGraph(AgentState)\n",
    "\n",
    "    graph_builder.add_node(\"get_user_input\", get_user_input)\n",
    "    graph_builder.add_node(\"call_llm\", call_llm)\n",
    "    graph_builder.add_node(\"print_response\", print_response)\n",
    "\n",
    "    graph_builder.add_edge(START, \"get_user_input\")\n",
    "\n",
    "    # Here the mapping keys correspond to the strings returned by route_after_input()\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"get_user_input\",\n",
    "        route_after_input,\n",
    "        {\n",
    "            # When route_after_input returns \"get_user_input\", execution goes back to input node\n",
    "            \"get_user_input\": \"get_user_input\",\n",
    "            # When route_after_input returns \"call_llm\", proceed to the LLM node\n",
    "            \"call_llm\": \"call_llm\",\n",
    "            # When route_after_input returns END, finish execution\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    graph_builder.add_edge(\"call_llm\", \"print_response\")\n",
    "    graph_builder.add_edge(\"print_response\", \"get_user_input\")\n",
    "\n",
    "    graph = graph_builder.compile()\n",
    "    return graph\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE GRAPH IMAGE (Mermaid)\n",
    "# =============================================================================\n",
    "\n",
    "def save_graph_image(graph, filename=\"lg_graph.png\"):\n",
    "    try:\n",
    "        png_data = graph.get_graph(xray=True).draw_mermaid_png()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(png_data)\n",
    "        print(f\"Graph image saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save graph image: {e}\")\n",
    "        print(\"You may need to install additional dependencies: pip install grandalf\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"LangGraph Simple Agent with Llama-3.2-1B-Instruct\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "    # Create LLM (with defensive langchain fixes)\n",
    "    llm = create_llm()\n",
    "\n",
    "    # Build graph\n",
    "    print(\"\\nCreating LangGraph...\")\n",
    "    graph = create_graph(llm)\n",
    "    print(\"Graph created successfully!\")\n",
    "\n",
    "    # Save visualization\n",
    "    print(\"\\nSaving graph visualization...\")\n",
    "    save_graph_image(graph)\n",
    "\n",
    "    # Initial state\n",
    "    initial_state: AgentState = {\n",
    "        \"user_input\": \"\",\n",
    "        \"should_exit\": False,\n",
    "        \"llm_response\": \"\",\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    # Invoke the graph (it loops internally until user requests exit)\n",
    "    graph.invoke(initial_state)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
