{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T22:15:25.833016Z",
     "iopub.status.busy": "2026-02-21T22:15:25.832454Z",
     "iopub.status.idle": "2026-02-21T22:18:59.322210Z",
     "shell.execute_reply": "2026-02-21T22:18:59.321466Z",
     "shell.execute_reply.started": "2026-02-21T22:15:25.832987Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LangGraph Single-Model Routing Agent with Message History (Qwen disabled)\n",
      "============================================================\n",
      "Using CUDA (NVIDIA GPU)\n",
      "Loading Llama: meta-llama/Llama-3.2-1B-Instruct ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e08a4751cc493b8f40ced5e11efcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama loaded.\n",
      "\n",
      "==================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "==================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is soccer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ¦™ LLaMA Response\n",
      "============================================================\n",
      "[System] You are a helpful assistant. Answer concisely.\n",
      "User: What is soccer?\n",
      "Assistant: Soccer, also known as football, is a team sport played with a round ball and involving two teams of eleven players each, with the objective of scoring more goals than the opposing team by kicking or heading the ball into the opponent's goal. The game is typically played on a rectangular field with goals at each end.\n",
      "\n",
      "==================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "==================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the best team?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ¦™ LLaMA Response\n",
      "============================================================\n",
      "[System] You are a helpful assistant. Answer concisely.\n",
      "User: What is soccer?\n",
      "Assistant: [System] You are a helpful assistant. Answer concisely.\n",
      "User: What is soccer?\n",
      "Assistant: Soccer, also known as football, is a team sport played with a round ball and involving two teams of eleven players each, with the objective of scoring more goals than the opposing team by kicking or heading the ball into the opponent's goal. The game is typically played on a rectangular field with goals at each end.\n",
      "User: What is the best team?\n",
      "Assistant: [System] There is no definitive answer, as the best team can vary depending on the specific season, competition, and criteria used to evaluate teams. However, some of the top-performing teams in recent years include Barcelona, Manchester City, and Real Madrid.\n",
      "\n",
      "==================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "==================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    }
   ],
   "source": [
    "# langgraph_single_model_with_history.py\n",
    "#\n",
    "# LangGraph agent with chat message history (Message API) and single-model routing.\n",
    "# - Maintains messages: roles supported: system, human, ai, tool\n",
    "# - If input begins with \"Hey Qwen\" -> Qwen is disabled, a notice is added and the input is routed to Llama\n",
    "# - Otherwise -> Llama\n",
    "# - Empty input loops back to input node (not sent to model)\n",
    "# - verbose / quiet toggles supported\n",
    "# - No Qwen loading attempted (disabled)\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Tuple, List, Dict, Any\n",
    "\n",
    "# -------------------------\n",
    "# Device selection\n",
    "# -------------------------\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA (NVIDIA GPU)\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS (Apple Silicon)\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return \"cpu\"\n",
    "\n",
    "# -------------------------\n",
    "# Message / State types\n",
    "# -------------------------\n",
    "Message = Dict[str, str]  # {\"role\": \"system\"|\"human\"|\"ai\"|\"tool\", \"content\": \"...\"}\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    should_exit: bool\n",
    "    llama_response: str\n",
    "    qwen_response: str   # kept for compatibility but not used since Qwen disabled\n",
    "    verbose: bool\n",
    "    messages: List[Message]\n",
    "\n",
    "# -------------------------\n",
    "# Build prompt from messages\n",
    "# -------------------------\n",
    "def build_prompt_from_messages(messages: List[Message]) -> str:\n",
    "    \"\"\"\n",
    "    Convert messages (list of {role, content}) into a single string prompt for a text-generation model.\n",
    "    Roles: system, human, ai, tool.\n",
    "\n",
    "    The format is a simple readable transcript:\n",
    "        [System] ...\n",
    "        User: ...\n",
    "        Tool: ...\n",
    "        Assistant: ...\n",
    "    Ends with 'Assistant:' as a cue for the model to respond.\n",
    "    \"\"\"\n",
    "    out_lines: List[str] = []\n",
    "    # Put system messages first (there may be multiple, we keep the order)\n",
    "    for msg in messages:\n",
    "        role = msg.get(\"role\", \"\").lower()\n",
    "        content = msg.get(\"content\", \"\")\n",
    "        if role == \"system\":\n",
    "            out_lines.append(f\"[System] {content}\")\n",
    "    # Then the other messages in order\n",
    "    for msg in messages:\n",
    "        role = msg.get(\"role\", \"\").lower()\n",
    "        content = msg.get(\"content\", \"\")\n",
    "        if role == \"human\" or role == \"user\":\n",
    "            out_lines.append(f\"User: {content}\")\n",
    "        elif role == \"tool\" or role == \"function\":\n",
    "            out_lines.append(f\"[Tool] {content}\")\n",
    "        elif role == \"ai\" or role == \"assistant\":\n",
    "            out_lines.append(f\"Assistant: {content}\")\n",
    "        # system already emitted\n",
    "    # Add assistant cue for model to continue\n",
    "    out_lines.append(\"Assistant:\")\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "# -------------------------\n",
    "# LLM loader with safe langchain fix\n",
    "# -------------------------\n",
    "def load_llm_wrapped(model_id: str, device: str):\n",
    "    \"\"\"\n",
    "    Load a HF model and wrap in an adapter exposing .invoke(prompt) -> str.\n",
    "    Raises RuntimeError on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "            device_map=device if device == \"cuda\" else None,\n",
    "        )\n",
    "        if device == \"mps\":\n",
    "            model = model.to(device)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Defensive langchain attributes to avoid AttributeError in some installs\n",
    "        try:\n",
    "            import langchain\n",
    "            if not hasattr(langchain, \"verbose\"):\n",
    "                langchain.verbose = False\n",
    "            if not hasattr(langchain, \"debug\"):\n",
    "                langchain.debug = False\n",
    "            if not hasattr(langchain, \"llm_cache\"):\n",
    "                langchain.llm_cache = None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        wrapped = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "        class Adapter:\n",
    "            def __init__(self, llm):\n",
    "                self.llm = llm\n",
    "            def invoke(self, prompt: str) -> str:\n",
    "                out = self.llm.invoke(prompt)\n",
    "                # If out is a string, return it. Otherwise try to extract generated_text\n",
    "                if isinstance(out, str):\n",
    "                    return out\n",
    "                try:\n",
    "                    if isinstance(out, list) and len(out) > 0 and isinstance(out[0], dict):\n",
    "                        if \"generated_text\" in out[0]:\n",
    "                            return out[0][\"generated_text\"]\n",
    "                    if isinstance(out, dict) and \"generated_text\" in out:\n",
    "                        return out[\"generated_text\"]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return str(out)\n",
    "\n",
    "        return Adapter(wrapped)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load {model_id}: {e}\") from e\n",
    "\n",
    "# -------------------------\n",
    "# Create models (only Llama is loaded; Qwen disabled)\n",
    "# -------------------------\n",
    "def create_models() -> Tuple[object, None]:\n",
    "    device = get_device()\n",
    "\n",
    "    # Attempt to load Llama\n",
    "    llama_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    llama_llm = None\n",
    "    try:\n",
    "        print(f\"Loading Llama: {llama_id} ...\")\n",
    "        llama_llm = load_llm_wrapped(llama_id, device)\n",
    "        print(\"Llama loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load Llama ({llama_id}): {e}\")\n",
    "        # try small fallback (gpt2) so the agent can still run without large model\n",
    "        fallback_id = \"gpt2\"\n",
    "        try:\n",
    "            print(f\"Loading fallback model: {fallback_id}\")\n",
    "            llama_llm = load_llm_wrapped(fallback_id, device)\n",
    "            print(\"Fallback loaded and used as Llama.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to load fallback {fallback_id}: {e2}\")\n",
    "            class FinalFallback:\n",
    "                def invoke(self, prompt: str) -> str:\n",
    "                    return \"[LLAMA-FALLBACK] No model available. Install transformers and a small model like 'gpt2'.\"\n",
    "            llama_llm = FinalFallback()\n",
    "\n",
    "    # Qwen is disabled: return None in place of qwen_llm\n",
    "    qwen_llm = None\n",
    "\n",
    "    return llama_llm, qwen_llm\n",
    "\n",
    "# -------------------------\n",
    "# Graph & nodes\n",
    "# -------------------------\n",
    "def create_graph(llama_llm, qwen_llm):\n",
    "    \"\"\"\n",
    "    Single-model routing with message history.\n",
    "    Qwen is disabled; 'Hey Qwen' will be treated as normal input and a tool message will be stored.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_user_input(state: AgentState) -> dict:\n",
    "        # On new input, clear previous model responses (avoid stale output)\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering get_user_input\")\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"> \", end=\"\")\n",
    "        user_input = input()\n",
    "\n",
    "        lowered = user_input.strip().lower()\n",
    "        if lowered == \"verbose\":\n",
    "            # clear outputs and leave messages intact (we keep the chat history)\n",
    "            return {\"user_input\": user_input, \"should_exit\": False, \"verbose\": True, \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "        if lowered == \"quiet\":\n",
    "            return {\"user_input\": user_input, \"should_exit\": False, \"verbose\": False, \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "        if lowered in (\"quit\", \"exit\", \"q\"):\n",
    "            # clear last outputs before exiting\n",
    "            return {\"user_input\": user_input, \"should_exit\": True, \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "        # Normal input: append a human message to the message history and clear model outputs\n",
    "        # We'll modify the state's messages in the call to route (LangGraph merges returned dict)\n",
    "        return {\"user_input\": user_input, \"should_exit\": False, \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "    def route_after_input(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Three-way:\n",
    "         - END if should_exit\n",
    "         - get_user_input if input empty\n",
    "         - call_llama otherwise (Qwen disabled)\n",
    "        If input begins with 'Hey Qwen', we insert a tool notice into messages and route to Llama.\n",
    "        \"\"\"\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input:\", {\"should_exit\": state.get(\"should_exit\"), \"user_input\": repr(state.get(\"user_input\"))})\n",
    "\n",
    "        if state.get(\"should_exit\", False):\n",
    "            return END\n",
    "\n",
    "        raw = str(state.get(\"user_input\", \"\"))\n",
    "        if raw.strip() == \"\":\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] empty input -> looping back\")\n",
    "            print(\"[NOTICE] Empty input received â€” please type something.\")\n",
    "            return \"get_user_input\"\n",
    "\n",
    "        # If user tried to address Qwen, record a tool message noting Qwen disabled and route to Llama\n",
    "        stripped_leading = raw.lstrip()\n",
    "        if stripped_leading.lower().startswith(\"hey qwen\"):\n",
    "            # add a tool message to explain Qwen is disabled\n",
    "            tool_msg = {\"role\": \"tool\", \"content\": \"Qwen is disabled in this agent. Input will be handled by Llama instead.\"}\n",
    "            # Merge messages into state: append tool message + append human message (we will append human message below)\n",
    "            # Note: return mapping from route function only determines next node; we need to append messages now.\n",
    "            # LangGraph will merge dict returned by the preceding node (get_user_input) â€” but route function cannot modify state.\n",
    "            # So we rely on get_user_input having appended the human message earlier; to ensure the tool message is recorded,\n",
    "            # we'll have the call_llama node detect the 'Hey Qwen' prefix and append the tool message before calling Llama.\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] Detected 'Hey Qwen' prefix; will route to Llama with tool notice\")\n",
    "            return \"call_llama\"\n",
    "\n",
    "        # Default: route to Llama\n",
    "        return \"call_llama\"\n",
    "\n",
    "    def call_llama(state: AgentState) -> dict:\n",
    "        \"\"\"\n",
    "        This node:\n",
    "          - ensures the latest human message is appended to messages\n",
    "          - if the input began with 'Hey Qwen' it appends a tool message explaining Qwen is disabled\n",
    "          - builds prompt from messages and calls the Llama model\n",
    "          - appends the AI response to messages and returns llama_response\n",
    "        \"\"\"\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] call_llama invoked\")\n",
    "\n",
    "        raw = str(state.get(\"user_input\", \"\"))\n",
    "\n",
    "        # Ensure messages exists\n",
    "        messages: List[Message] = state.get(\"messages\", [])\n",
    "        if messages is None:\n",
    "            messages = []\n",
    "\n",
    "        # Append human message (keep original exact user_input)\n",
    "        human_msg = {\"role\": \"human\", \"content\": raw}\n",
    "        messages.append(human_msg)\n",
    "\n",
    "        # If user started with Hey Qwen, add a tool message informing Qwen is disabled\n",
    "        stripped_leading = raw.lstrip()\n",
    "        if stripped_leading.lower().startswith(\"hey qwen\"):\n",
    "            tool_msg = {\"role\": \"tool\", \"content\": \"Qwen is disabled in this agent. Routed to Llama.\"}\n",
    "            messages.append(tool_msg)\n",
    "            # Also remove the leading trigger from the human content in messages? we keep original human message,\n",
    "            # but for model input we'll pass the remainder (strip trigger) as the human content via prompt builder.\n",
    "            # So we modify a temporary messages_for_prompt below.\n",
    "\n",
    "        # Build messages_for_prompt such that if 'Hey Qwen' was used, the latest human message content is replaced\n",
    "        # with the remainder after removing the trigger, so model does not see the trigger text.\n",
    "        messages_for_prompt = []\n",
    "        for m in messages:\n",
    "            # copy to avoid mutating state stored messages\n",
    "            messages_for_prompt.append({\"role\": m[\"role\"], \"content\": m[\"content\"]})\n",
    "\n",
    "        # If last human message begins with Hey Qwen, replace its content for the prompt with remainder\n",
    "        if messages_for_prompt and messages_for_prompt[-1][\"role\"] in (\"human\", \"user\"):\n",
    "            last_content = messages_for_prompt[-1][\"content\"]\n",
    "            if last_content.lstrip().lower().startswith(\"hey qwen\"):\n",
    "                # remove leading 'Hey Qwen' (first 8 characters after lstrip) then left-strip separators\n",
    "                remainder = last_content.lstrip()[8:].lstrip()\n",
    "                messages_for_prompt[-1][\"content\"] = remainder if remainder != \"\" else last_content\n",
    "\n",
    "        # Build prompt text for the single-turn text generation model\n",
    "        prompt_text = build_prompt_from_messages(messages_for_prompt)\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Prompt to Llama (truncated):\")\n",
    "            print(prompt_text[:1000])\n",
    "\n",
    "        # Call the Llama LLM\n",
    "        response_text = llama_llm.invoke(prompt_text)\n",
    "\n",
    "        # Append AI response to messages\n",
    "        ai_msg = {\"role\": \"ai\", \"content\": response_text}\n",
    "        messages.append(ai_msg)\n",
    "\n",
    "        # Return updated messages and llama_response for printing\n",
    "        return {\"llama_response\": response_text, \"messages\": messages}\n",
    "\n",
    "    def print_both_responses(state: AgentState) -> dict:\n",
    "        \"\"\"\n",
    "        Print only the model sections that have content. Because Qwen is disabled,\n",
    "        qwen_response will typically be empty.\n",
    "        \"\"\"\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] print_both_responses invoked\")\n",
    "\n",
    "        llama_text = (state.get(\"llama_response\") or \"\").strip()\n",
    "        qwen_text = (state.get(\"qwen_response\") or \"\").strip()\n",
    "\n",
    "        if not llama_text and not qwen_text:\n",
    "            print(\"\\n[NOTICE] No model produced output this turn.\")\n",
    "            return {\"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "        if llama_text:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ðŸ¦™ LLaMA Response\")\n",
    "            print(\"=\" * 60)\n",
    "            print(llama_text)\n",
    "\n",
    "        if qwen_text:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ðŸ§  Qwen Response\")\n",
    "            print(\"=\" * 60)\n",
    "            print(qwen_text)\n",
    "\n",
    "        # Clear responses for next iteration\n",
    "        return {\"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "    # Build the graph\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"get_user_input\", get_user_input)\n",
    "    graph.add_node(\"call_llama\", call_llama)\n",
    "    graph.add_node(\"print_both_responses\", print_both_responses)\n",
    "\n",
    "    graph.add_edge(START, \"get_user_input\")\n",
    "\n",
    "    graph.add_conditional_edges(\n",
    "        \"get_user_input\",\n",
    "        route_after_input,\n",
    "        {\n",
    "            \"get_user_input\": \"get_user_input\",  # empty -> loop\n",
    "            \"call_llama\": \"call_llama\",          # default -> llama (including Hey Qwen)\n",
    "            END: END,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    graph.add_edge(\"call_llama\", \"print_both_responses\")\n",
    "    graph.add_edge(\"print_both_responses\", \"get_user_input\")\n",
    "\n",
    "    return graph.compile()\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LangGraph Single-Model Routing Agent with Message History (Qwen disabled)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llama_llm, qwen_llm = create_models()\n",
    "\n",
    "    graph = create_graph(llama_llm, qwen_llm)\n",
    "\n",
    "    # Initial system message that sets assistant behavior; customize as needed\n",
    "    system_msg = {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer concisely.\"}\n",
    "\n",
    "    initial_state: AgentState = {\n",
    "        \"user_input\": \"\",\n",
    "        \"should_exit\": False,\n",
    "        \"llama_response\": \"\",\n",
    "        \"qwen_response\": \"\",\n",
    "        \"verbose\": False,\n",
    "        \"messages\": [system_msg],\n",
    "    }\n",
    "\n",
    "    graph.invoke(initial_state)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
