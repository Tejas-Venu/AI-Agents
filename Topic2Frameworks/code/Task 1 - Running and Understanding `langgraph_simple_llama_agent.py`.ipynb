{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:34:33.041300Z",
     "iopub.status.busy": "2026-02-21T21:34:33.040725Z",
     "iopub.status.idle": "2026-02-21T21:35:18.278986Z",
     "shell.execute_reply": "2026-02-21T21:35:18.278257Z",
     "shell.execute_reply.started": "2026-02-21T21:34:33.041268Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LangGraph Simple Agent with Llama-3.2-1B-Instruct\n",
      "==================================================\n",
      "\n",
      "Using CUDA (NVIDIA GPU) for inference\n",
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "This may take a moment on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18931ff44b144b068885f6b6bf4c75be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Creating LangGraph...\n",
      "Graph created successfully!\n",
      "\n",
      "Saving graph visualization...\n",
      "Graph image saved to lg_graph.png\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " verbose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbose tracing enabled.\n",
      "[TRACE] route_after_input evaluating...\n",
      "[TRACE] should_exit=False user_input='verbose'\n",
      "[TRACE] route_after_input -> call_llm\n",
      "[TRACE] Entering node: call_llm\n",
      "[TRACE] State input user_input='verbose'\n",
      "[TRACE] Prepared prompt (truncated to 120 chars): 'User: verbose\\nAssistant:'\n",
      "[TRACE] Invoking LLM...\n",
      "\n",
      "Processing your input...\n",
      "[TRACE] LLM returned response (truncated to 120 chars):\n",
      "\"User: verbose\\nAssistant: verbose\\n\\nI'd like to inquire about the process of obtaining a patent for a new invention. Here \"\n",
      "[TRACE] Entering node: print_response\n",
      "[TRACE] llm_response length = 1300\n",
      "\n",
      "--------------------------------------------------\n",
      "LLM Response:\n",
      "--------------------------------------------------\n",
      "User: verbose\n",
      "Assistant: verbose\n",
      "\n",
      "I'd like to inquire about the process of obtaining a patent for a new invention. Here is a brief summary of what I've found so far:\n",
      "\n",
      "1. Determine if your invention meets the basic requirements of patentability under the Patent Act.\n",
      "2. Conduct a preliminary patent search to identify existing patents and prior art that may be relevant to your invention.\n",
      "3. Prepare a detailed patent application, which includes:\n",
      "\t* A clear and concise description of the invention\n",
      "\t* Drawings and diagrams that illustrate the invention\n",
      "\t* Claims that define the scope of protection for your invention\n",
      "\t* Abstract or summary of the invention\n",
      "\t* A statement of the problem or need that your invention solves\n",
      "4. File a provisional patent application (if desired) to file a non-exclusive application to be eligible for patent filing.\n",
      "5. File a non-disclosure agreement (NDA) with potential partners or collaborators to protect your intellectual property.\n",
      "6. File a utility patent application with the United States Patent and Trademark Office (USPTO) to claim your invention as a patent.\n",
      "7. Respond to any office actions or rejections from the USPTO, and revise your application as necessary.\n",
      "\n",
      "Am I correct in assuming that the process involves multiple steps and considerations, such as\n",
      "[TRACE] Exiting node: print_response\n",
      "[TRACE] Entering node: get_user_input\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quiet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbose tracing disabled.\n",
      "\n",
      "Processing your input...\n",
      "\n",
      "--------------------------------------------------\n",
      "LLM Response:\n",
      "--------------------------------------------------\n",
      "User: quiet\n",
      "Assistant: I'm here to help you, quiet. What's on your mind? Is there something you'd like to talk about or ask?\n",
      "\n",
      "==================================================\n",
      "Enter your text (or 'quit' to exit):\n",
      "==================================================\n",
      "\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# langgraph_simple_agent.py\n",
    "# Simple LangGraph agent using a Hugging Face LLM.\n",
    "# Runtime tracing toggle: type \"verbose\" to enable tracing, \"quiet\" to disable.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Pick best available device: cuda > mps > cpu\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA (NVIDIA GPU) for inference\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS (Apple Silicon) for inference\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU for inference\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATE DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State that flows through nodes.\n",
    "    \"\"\"\n",
    "    user_input: str\n",
    "    should_exit: bool\n",
    "    llm_response: str\n",
    "    verbose: bool\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LLM CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"\n",
    "    Load Llama-3.2-1B-Instruct via Hugging Face, wrap with HuggingFacePipeline.\n",
    "    Apply defensive fixes to top-level langchain module attributes that langchain_core expects.\n",
    "    \"\"\"\n",
    "\n",
    "    device = get_device()\n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    print(\"This may take a moment on first run...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "        device_map=device if device == \"cuda\" else None,\n",
    "    )\n",
    "\n",
    "    if device == \"mps\":\n",
    "        # Move model to MPS explicitly\n",
    "        model = model.to(device)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Defensive compatibility fixes for langchain_core\n",
    "    # Some installations of `langchain` do not expose top-level attributes\n",
    "    # that langchain_core expects (verbose, debug, llm_cache). Ensure they exist.\n",
    "    # ------------------------------------------------------------------\n",
    "    try:\n",
    "        import langchain\n",
    "\n",
    "        # ensure verbose/debug exist\n",
    "        if not hasattr(langchain, \"verbose\"):\n",
    "            langchain.verbose = False\n",
    "        if not hasattr(langchain, \"debug\"):\n",
    "            langchain.debug = False\n",
    "\n",
    "        # ensure llm_cache exists (langchain_core may check this)\n",
    "        if not hasattr(langchain, \"llm_cache\"):\n",
    "            # Default to None (no global cache). This is safe.\n",
    "            langchain.llm_cache = None\n",
    "\n",
    "    except Exception:\n",
    "        # If anything goes wrong while trying to set attributes, ignore and continue.\n",
    "        # The worst case is the subsequent LangChain calls may raise errors; those will show up later.\n",
    "        pass\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GRAPH CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_graph(llm):\n",
    "    \"\"\"\n",
    "    Build the LangGraph state graph with nodes:\n",
    "      - get_user_input\n",
    "      - call_llm\n",
    "      - print_response\n",
    "    and a conditional route after get_user_input.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: get_user_input\n",
    "    # --------------------------\n",
    "    def get_user_input(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: get_user_input\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Enter your text (or 'quit' to exit):\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"\\n> \", end=\"\")\n",
    "\n",
    "        user_input = input()\n",
    "        lowered = user_input.strip().lower()\n",
    "\n",
    "        # Toggle tracing\n",
    "        if lowered == \"verbose\":\n",
    "            print(\"Verbose tracing enabled.\")\n",
    "            return {\"user_input\": user_input, \"should_exit\": False, \"verbose\": True}\n",
    "\n",
    "        if lowered == \"quiet\":\n",
    "            print(\"Verbose tracing disabled.\")\n",
    "            return {\"user_input\": user_input, \"should_exit\": False, \"verbose\": False}\n",
    "\n",
    "        # Exit commands\n",
    "        if lowered in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            return {\"user_input\": user_input, \"should_exit\": True}\n",
    "\n",
    "        # Default: continue to LLM; preserve existing verbose flag if present\n",
    "        return {\"user_input\": user_input, \"should_exit\": False}\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: call_llm\n",
    "    # --------------------------\n",
    "    def call_llm(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: call_llm\")\n",
    "            print(f\"[TRACE] State input user_input={repr(state.get('user_input'))}\")\n",
    "\n",
    "        user_input = state[\"user_input\"]\n",
    "        prompt = f\"User: {user_input}\\nAssistant:\"\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(f\"[TRACE] Prepared prompt (truncated to 120 chars): {repr(prompt[:120])}\")\n",
    "            print(\"[TRACE] Invoking LLM...\")\n",
    "\n",
    "        print(\"\\nProcessing your input...\")\n",
    "\n",
    "        # Use the wrapped HuggingFacePipeline LLM\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] LLM returned response (truncated to 120 chars):\")\n",
    "            print(repr(response[:120]))\n",
    "\n",
    "        return {\"llm_response\": response}\n",
    "\n",
    "    # --------------------------\n",
    "    # Node: print_response\n",
    "    # --------------------------\n",
    "    def print_response(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering node: print_response\")\n",
    "            print(f\"[TRACE] llm_response length = {len(state.get('llm_response', ''))}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"LLM Response:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(state[\"llm_response\"])\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Exiting node: print_response\")\n",
    "\n",
    "        return {}\n",
    "\n",
    "    # --------------------------\n",
    "    # Router after input\n",
    "    # --------------------------\n",
    "    def route_after_input(state: AgentState) -> str:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input evaluating...\")\n",
    "            print(f\"[TRACE] should_exit={state.get('should_exit', False)} user_input={repr(state.get('user_input'))}\")\n",
    "\n",
    "        if state.get(\"should_exit\", False):\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] route_after_input -> END\")\n",
    "            return END\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input -> call_llm\")\n",
    "        return \"call_llm\"\n",
    "\n",
    "    # Build graph\n",
    "    graph_builder = StateGraph(AgentState)\n",
    "\n",
    "    graph_builder.add_node(\"get_user_input\", get_user_input)\n",
    "    graph_builder.add_node(\"call_llm\", call_llm)\n",
    "    graph_builder.add_node(\"print_response\", print_response)\n",
    "\n",
    "    graph_builder.add_edge(START, \"get_user_input\")\n",
    "\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"get_user_input\",\n",
    "        route_after_input,\n",
    "        {\n",
    "            \"call_llm\": \"call_llm\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    graph_builder.add_edge(\"call_llm\", \"print_response\")\n",
    "    graph_builder.add_edge(\"print_response\", \"get_user_input\")\n",
    "\n",
    "    graph = graph_builder.compile()\n",
    "    return graph\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE GRAPH IMAGE (Mermaid)\n",
    "# =============================================================================\n",
    "\n",
    "def save_graph_image(graph, filename=\"lg_graph.png\"):\n",
    "    try:\n",
    "        png_data = graph.get_graph(xray=True).draw_mermaid_png()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(png_data)\n",
    "        print(f\"Graph image saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save graph image: {e}\")\n",
    "        print(\"You may need to install additional dependencies: pip install grandalf\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"LangGraph Simple Agent with Llama-3.2-1B-Instruct\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "    # Create LLM (with defensive langchain fixes)\n",
    "    llm = create_llm()\n",
    "\n",
    "    # Build graph\n",
    "    print(\"\\nCreating LangGraph...\")\n",
    "    graph = create_graph(llm)\n",
    "    print(\"Graph created successfully!\")\n",
    "\n",
    "    # Save visualization\n",
    "    print(\"\\nSaving graph visualization...\")\n",
    "    save_graph_image(graph)\n",
    "\n",
    "    # Initial state\n",
    "    initial_state: AgentState = {\n",
    "        \"user_input\": \"\",\n",
    "        \"should_exit\": False,\n",
    "        \"llm_response\": \"\",\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    # Invoke the graph (it loops internally until user requests exit)\n",
    "    graph.invoke(initial_state)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
