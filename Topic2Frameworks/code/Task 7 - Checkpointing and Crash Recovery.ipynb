{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T03:09:49.119959Z",
     "iopub.status.busy": "2026-02-22T03:09:49.119378Z",
     "iopub.status.idle": "2026-02-22T03:10:35.358251Z",
     "shell.execute_reply": "2026-02-22T03:10:35.356983Z",
     "shell.execute_reply.started": "2026-02-22T03:09:49.119918Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Dual-LLM LangGraph Agent with Checkpointing (Llama <-> Qwen)\n",
      "================================================================================\n",
      "Using CUDA (NVIDIA GPU)\n",
      "Loading Llama (meta-llama/Llama-3.2-1B-Instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896d37214da8411283dd6d38427ad13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama loaded.\n",
      "Attempting to load Qwen (Qwen/Qwen2.5-1.5B-Instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98422282bb4246de866c6fd8d30abd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen loaded.\n",
      "\n",
      "============================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "Special commands: history, reset, save_and_exit\n",
      "============================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ¦™ LLaMA Response\n",
      "======================================================================\n",
      "[System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\n",
      "User: Human: Hey\n",
      "Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\n",
      "User: Human: Hey\n",
      "Assistant: Hello! How can I help you today?\n",
      "User: Human: Hey Qwen, what about you?\n",
      "User: Qwen: [System] You are Qwen (assistant). Participants: Human and Llama. When others speak, their name will be prefixed (e.g. 'Llama: ...'). Answer helpfully and concisely.\n",
      "User: Human: Hey\n",
      "User: Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\n",
      "User: Human: Hey\n",
      "Assistant: Hello! How can I help you today?\n",
      "User: Human: , what about you?\n",
      "Assistant: As an AI language model, my purpose is to assist with various tasks such as answering questions, providing information, or engaging in conversations on a wide range of topics. Is there something specific you would like assistance with? Feel free to ask me anything.\n",
      "User: Human: What is your favourite soccer team\n",
      "Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\n",
      "User: Human: Hey\n",
      "Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\n",
      "User: Human: Hey\n",
      "Assistant: Hello! How can I help you today?\n",
      "User: Human: Hey Qwen, what about you?\n",
      "User: Qwen: [System] You are Qwen (assistant). Participants: Human and Llama. When others speak, their name will be prefixed (e.g. 'Llama: ...'). Answer helpfully and concisely.\n",
      "User: Human: Hey\n",
      "User: Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\n",
      "User: Human: Hey\n",
      "Assistant: Hello! How can I help you today?\n",
      "User: Human: , what about you?\n",
      "Assistant: As an AI language model, my purpose is to assist with various tasks such as answering questions, providing information, or engaging in conversations on a wide range of topics. Is there something specific you would like assistance with? Feel free to ask me anything.\n",
      "User: Human: What is your favourite soccer team\n",
      "Assistant: A popular question! As a neutral AI, I don't have personal preferences, but I can tell you about some of the most successful and beloved soccer teams. Would you like to know more about a specific team or aspect of the sport?\n",
      "User: Human: Qwen:, what about you?\n",
      "User: Qwen: [System] You are Qwen (assistant). Participants: Human and Llama. When others speak, their name will be prefixed (e.g. 'Llama:...'). Answer helpfully and concisely.\n",
      "User: Human: What is your favourite soccer team\n",
      "User: Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen:...'). Answer helpfully and concisely.\n",
      "User: Human: Qwen, what about you?\n",
      "User: Qwen:, what about you?\n",
      "User: Human: What is your favourite soccer team\n",
      "User: Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen:...'). Answer helpfully and concisely.\n",
      "User: Human: Qwen\n",
      "User: Human: Hi\n",
      "Assistant: Hello! How can I help you today?\n",
      "User: Human: Qwen, what about you?\n",
      "User: Qwen:, what about you?\n",
      "User: Human: What is your favourite soccer team\n",
      "User: Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen:...'). Answer helpfully and concisely.\n",
      "User: Human: Qwen\n",
      "User: Human: Hi\n",
      "Assistant: Hello! How can I help you today?\n",
      "User: Human: Qwen, what about you?\n",
      "User: Qwen:, what about you?\n",
      "User: Human: What is your favourite soccer team\n",
      "User: Llama: [System] You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen:...'). Answer helpfully and concisely.\n",
      "\n",
      "============================================================\n",
      "Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\n",
      "Special commands: history, reset, save_and_exit\n",
      "============================================================\n",
      "> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    }
   ],
   "source": [
    "# langgraph_dual_llm_history_with_checkpoint.py\n",
    "#\n",
    "# Dual-LLM agent (Llama <-> Qwen) with shared canonical history, sanitization,\n",
    "# and persistent checkpointing for crash recovery.\n",
    "#\n",
    "# Interactive commands:\n",
    "#   verbose / quiet   -- toggle tracing\n",
    "#   history           -- print canonical history\n",
    "#   reset             -- clear history (keeps initial system message)\n",
    "#   save_and_exit     -- save checkpoint and exit cleanly\n",
    "#   quit / exit / q   -- exit (state saved automatically)\n",
    "#\n",
    "# NOTE: SIGINT and SIGTERM are trapped and will trigger a checkpoint save before exit.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import signal\n",
    "import atexit\n",
    "import tempfile\n",
    "import traceback\n",
    "from typing import TypedDict, List, Dict, Any, Tuple\n",
    "\n",
    "# Defensive imports (torch / transformers). If unavailable the script runs with fallback models.\n",
    "TRANSFORMERS_OK = True\n",
    "try:\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    TRANSFORMERS_OK = False\n",
    "    print(\"[WARN] torch import failed; running in fallback mode.\")\n",
    "    traceback.print_exception(type(e), e, e.__traceback__, file=sys.stdout)\n",
    "\n",
    "if TRANSFORMERS_OK:\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    except Exception as e:\n",
    "        TRANSFORMERS_OK = False\n",
    "        print(\"[WARN] transformers import failed; running in fallback mode.\")\n",
    "        traceback.print_exception(type(e), e, e.__traceback__, file=sys.stdout)\n",
    "\n",
    "# langgraph\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, START, END\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Could not import langgraph.graph. Install langgraph or run in an environment with it available.\")\n",
    "    traceback.print_exception(type(e), e, e.__traceback__, file=sys.stdout)\n",
    "    raise\n",
    "\n",
    "# -------------------------\n",
    "# Types & checkpoint config\n",
    "# -------------------------\n",
    "SpeakerMessage = Dict[str, str]  # {\"speaker\": \"Human\"|\"Llama\"|\"Qwen\"|\"Tool\"|\"System\", \"content\": \"...\"}\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    should_exit: bool\n",
    "    last_model: str\n",
    "    verbose: bool\n",
    "    messages: List[SpeakerMessage]\n",
    "    llama_response: str\n",
    "    qwen_response: str\n",
    "\n",
    "CHECKPOINT_PATH = os.environ.get(\"LG_CHECKPOINT_PATH\", \"lg_checkpoint.json\")\n",
    "\n",
    "# -------------------------\n",
    "# Checkpoint helpers\n",
    "# -------------------------\n",
    "def atomic_write_json(path: str, data: dict):\n",
    "    \"\"\"Write JSON atomically to avoid partial files.\"\"\"\n",
    "    dirnm = os.path.dirname(os.path.abspath(path)) or \".\"\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", dir=dirnm, delete=False, encoding=\"utf-8\") as tf:\n",
    "        json.dump(data, tf, ensure_ascii=False, indent=2)\n",
    "        tmpname = tf.name\n",
    "    # Rename to target atomically\n",
    "    os.replace(tmpname, path)\n",
    "\n",
    "def save_checkpoint(state: AgentState, path: str = CHECKPOINT_PATH):\n",
    "    \"\"\"Persist minimal state needed for recovery: messages, verbose, last_model.\"\"\"\n",
    "    try:\n",
    "        data = {\n",
    "            \"messages\": state.get(\"messages\", []),\n",
    "            \"verbose\": bool(state.get(\"verbose\", False)),\n",
    "            \"last_model\": state.get(\"last_model\", \"\"),\n",
    "        }\n",
    "        atomic_write_json(path, data)\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(f\"[TRACE] Checkpoint saved to {path} (messages={len(data['messages'])})\")\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Failed to save checkpoint:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "def load_checkpoint(path: str = CHECKPOINT_PATH) -> Dict[str, Any]:\n",
    "    \"\"\"Load checkpoint if available; return dict with keys messages, verbose, last_model.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return {\"messages\": None, \"verbose\": False, \"last_model\": \"\"}\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # Validate shape\n",
    "        messages = data.get(\"messages\", [])\n",
    "        verbose = bool(data.get(\"verbose\", False))\n",
    "        last_model = data.get(\"last_model\", \"\")\n",
    "        return {\"messages\": messages, \"verbose\": verbose, \"last_model\": last_model}\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Failed to load checkpoint:\", e)\n",
    "        traceback.print_exc()\n",
    "        return {\"messages\": None, \"verbose\": False, \"last_model\": \"\"}\n",
    "\n",
    "# Ensure save on process exit\n",
    "def _on_exit_save(state: AgentState):\n",
    "    try:\n",
    "        # state may be mutated; just attempt to persist what's currently set\n",
    "        save_checkpoint(state)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# We'll register signal handlers below once we have the running state.\n",
    "\n",
    "# -------------------------\n",
    "# Sanitization utilities\n",
    "# -------------------------\n",
    "def sanitize_model_output(text: Any, speaker: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if not isinstance(text, str):\n",
    "        try:\n",
    "            text = str(text)\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "    text = text.strip()\n",
    "    sp = re.escape(speaker)\n",
    "    text = re.sub(rf'^(?:\\s*(?:{sp}|Assistant)\\s*:)+\\s*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(rf'(?:\\s*(?:{sp}|Assistant)\\s*:\\s*)+$', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "    return text\n",
    "\n",
    "def ensure_speaker_prefix(content: str, speaker_label: str) -> str:\n",
    "    if content is None:\n",
    "        content = \"\"\n",
    "    content = content.strip()\n",
    "    if content == \"\":\n",
    "        return content\n",
    "    if re.match(rf'^\\s*{re.escape(speaker_label)}\\s*:', content, flags=re.IGNORECASE):\n",
    "        return content\n",
    "    if re.match(r'^\\s*Human\\s*:', content, flags=re.IGNORECASE):\n",
    "        return content\n",
    "    return f\"{speaker_label}: {content}\"\n",
    "\n",
    "# -------------------------\n",
    "# Device selection, fallback models, HF loader\n",
    "# -------------------------\n",
    "def get_device() -> str:\n",
    "    if TRANSFORMERS_OK:\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                print(\"Using CUDA (NVIDIA GPU)\")\n",
    "                return \"cuda\"\n",
    "            elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "                print(\"Using MPS (Apple Silicon)\")\n",
    "                return \"mps\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"Using CPU\")\n",
    "    return \"cpu\"\n",
    "\n",
    "class SimpleFallbackModel:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        preview = (prompt.replace(\"\\n\", \" \")[:240] + \"...\") if prompt else \"[no prompt]\"\n",
    "        return f\"[{self.name.upper()}-FALLBACK] No HF model available. Prompt preview: {preview}\"\n",
    "\n",
    "class PipelineAdapter:\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        try:\n",
    "            out = self.pipe(prompt)\n",
    "            if isinstance(out, list) and len(out) > 0 and isinstance(out[0], dict):\n",
    "                txt = out[0].get(\"generated_text\")\n",
    "                if isinstance(txt, str):\n",
    "                    return txt\n",
    "            if isinstance(out, str):\n",
    "                return out\n",
    "            return str(out)\n",
    "        except Exception as e:\n",
    "            return f\"[MODEL-ERROR] pipeline invocation failed: {e}\"\n",
    "\n",
    "def load_and_wrap(model_id: str, device: str):\n",
    "    if not TRANSFORMERS_OK:\n",
    "        raise RuntimeError(\"Transformers not available\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(None if device == \"cpu\" else None),\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    )\n",
    "    if device == \"mps\":\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "        except Exception:\n",
    "            pass\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=getattr(tokenizer, \"eos_token_id\", None),\n",
    "    )\n",
    "    return PipelineAdapter(pipe)\n",
    "\n",
    "def create_models() -> Tuple[Any, Any]:\n",
    "    device = get_device()\n",
    "    # Llama\n",
    "    if TRANSFORMERS_OK:\n",
    "        try:\n",
    "            print(\"Loading Llama (meta-llama/Llama-3.2-1B-Instruct) ...\")\n",
    "            llama_llm = load_and_wrap(\"meta-llama/Llama-3.2-1B-Instruct\", device)\n",
    "            print(\"Llama loaded.\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load Llama, falling back to gpt2 or SimpleFallbackModel:\", e)\n",
    "            traceback.print_exc()\n",
    "            try:\n",
    "                llama_llm = load_and_wrap(\"gpt2\", device)\n",
    "                print(\"gpt2 loaded as Llama fallback.\")\n",
    "            except Exception:\n",
    "                llama_llm = SimpleFallbackModel(\"llama\")\n",
    "    else:\n",
    "        llama_llm = SimpleFallbackModel(\"llama\")\n",
    "\n",
    "    # Qwen - attempt but allow disabled (None)\n",
    "    qwen_llm = None\n",
    "    if TRANSFORMERS_OK:\n",
    "        try:\n",
    "            print(\"Attempting to load Qwen (Qwen/Qwen2.5-1.5B-Instruct) ...\")\n",
    "            qwen_llm = load_and_wrap(\"Qwen/Qwen2.5-1.5B-Instruct\", device)\n",
    "            print(\"Qwen loaded.\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Could not load Qwen (it will be treated as disabled):\", e)\n",
    "            traceback.print_exc()\n",
    "            qwen_llm = None\n",
    "    else:\n",
    "        qwen_llm = None\n",
    "\n",
    "    return llama_llm, qwen_llm\n",
    "\n",
    "# -------------------------\n",
    "# Role-message conversion & prompt building\n",
    "# -------------------------\n",
    "# --- REPLACEMENT: build_role_messages_for_target ---\n",
    "def build_role_messages_for_target(target: str, canonical_messages: List[SpeakerMessage]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Convert canonical history into role-messages for a given target ('Llama' or 'Qwen').\n",
    "\n",
    "    Differences from prior version:\n",
    "     - DO NOT include canonical 'System' messages here; the caller will prepend exactly one\n",
    "       target-specific system prompt (system_message_for_target). This prevents duplicate system lines.\n",
    "     - Human -> role:user with content \"Human: <content>\"\n",
    "     - Prior LLM utterance -> assistant if it was from target; otherwise user with \"<Speaker>: <content>\"\n",
    "     - Uses ensure_speaker_prefix to avoid double-labeling.\n",
    "    \"\"\"\n",
    "    role_msgs: List[Dict[str, str]] = []\n",
    "\n",
    "    # iterate canonical messages but skip system messages (they will be added separately)\n",
    "    for m in canonical_messages:\n",
    "        sp = m.get(\"speaker\", \"\")\n",
    "        content = m.get(\"content\", \"\") or \"\"\n",
    "        sp_norm = sp.lower()\n",
    "\n",
    "        if sp_norm == \"system\":\n",
    "            # Skip canonical system entries here; the per-target system prompt is injected by the caller.\n",
    "            continue\n",
    "\n",
    "        if sp_norm == \"tool\":\n",
    "            role_msgs.append({\"role\": \"tool\", \"content\": content})\n",
    "            continue\n",
    "\n",
    "        if sp_norm == \"human\":\n",
    "            # Human messages become user messages prefixed with \"Human: \"\n",
    "            cont = content\n",
    "            if not re.match(r'^\\s*Human\\s*:', cont, flags=re.IGNORECASE):\n",
    "                cont = f\"Human: {cont}\"\n",
    "            role_msgs.append({\"role\": \"user\", \"content\": cont})\n",
    "            continue\n",
    "\n",
    "        # For prior LLM utterances (Llama/Qwen), avoid double prefixes\n",
    "        speaker_label = sp  # e.g., \"Llama\" or \"Qwen\"\n",
    "        prefixed = ensure_speaker_prefix(content, speaker_label)\n",
    "        if speaker_label.lower() == target.lower():\n",
    "            # prior utterance by same model -> assistant role\n",
    "            role_msgs.append({\"role\": \"assistant\", \"content\": prefixed})\n",
    "        else:\n",
    "            # prior utterance by other model -> user role with speaker prefix\n",
    "            role_msgs.append({\"role\": \"user\", \"content\": prefixed})\n",
    "\n",
    "    return role_msgs\n",
    "\n",
    "\n",
    "# --- REPLACEMENT: prompt_from_role_messages ---\n",
    "def prompt_from_role_messages(role_messages: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt text from role-based messages.\n",
    "    - system messages should already have been placed first by the caller (if any).\n",
    "    - For prior assistant messages: include their content verbatim (they are already prefixed\n",
    "      like \"Llama: ...\" or \"Qwen: ...\") â€” DO NOT prefix with \"Assistant: \" again.\n",
    "    - For user/tool messages: format as \"User: ...\" / \"[Tool] ...\".\n",
    "    - End the prompt with a single \"Assistant:\" cue for the model to complete.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "\n",
    "    # system lines first (if present)\n",
    "    for rm in role_messages:\n",
    "        if rm[\"role\"] == \"system\":\n",
    "            lines.append(f\"[System] {rm['content']}\")\n",
    "\n",
    "    # then the rest in order\n",
    "    for rm in role_messages:\n",
    "        role = rm[\"role\"]\n",
    "        content = rm[\"content\"]\n",
    "        if role == \"system\":\n",
    "            continue\n",
    "        if role == \"user\":\n",
    "            lines.append(f\"User: {content}\")\n",
    "        elif role == \"tool\":\n",
    "            lines.append(f\"[Tool] {content}\")\n",
    "        elif role == \"assistant\":\n",
    "            # IMPORTANT: do not prefix with \"Assistant:\" here because content already carries speaker prefix.\n",
    "            # Example: content might be \"Llama: Vanilla is best.\" â€” include verbatim.\n",
    "            lines.append(content)\n",
    "    # final cue for the model to respond\n",
    "    lines.append(\"Assistant:\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def system_message_for_target(target: str) -> SpeakerMessage:\n",
    "    if target.lower() == \"llama\":\n",
    "        return {\"speaker\": \"System\", \"content\": \"You are Llama (assistant). Participants: Human and Qwen. When others speak, their name will be prefixed (e.g. 'Qwen: ...'). Answer helpfully and concisely.\"}\n",
    "    else:\n",
    "        return {\"speaker\": \"System\", \"content\": \"You are Qwen (assistant). Participants: Human and Llama. When others speak, their name will be prefixed (e.g. 'Llama: ...'). Answer helpfully and concisely.\"}\n",
    "\n",
    "# -------------------------\n",
    "# Graph & nodes (with checkpointing calls)\n",
    "# -------------------------\n",
    "def create_graph(llama_llm, qwen_llm):\n",
    "    def get_user_input(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering get_user_input\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Enter text (or 'quit' to exit). Type 'verbose' or 'quiet' to toggle tracing.\")\n",
    "        print(\"Special commands: history, reset, save_and_exit\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"> \", end=\"\")\n",
    "        raw = input()\n",
    "\n",
    "        low = raw.strip().lower()\n",
    "        if low == \"verbose\":\n",
    "            return {\"user_input\": raw, \"should_exit\": False, \"verbose\": True}\n",
    "        if low == \"quiet\":\n",
    "            return {\"user_input\": raw, \"should_exit\": False, \"verbose\": False}\n",
    "        if low in (\"quit\", \"exit\", \"q\"):\n",
    "            return {\"user_input\": raw, \"should_exit\": True}\n",
    "        if low == \"history\":\n",
    "            msgs = state.get(\"messages\", [])\n",
    "            print(\"\\n[HISTORY] canonical messages (most recent last):\")\n",
    "            for m in msgs:\n",
    "                print(f\"  {m.get('speaker')}: {m.get('content')}\")\n",
    "            return {\"user_input\": \"\", \"should_exit\": False}\n",
    "        if low == \"reset\":\n",
    "            sys_msg = {\"speaker\": \"System\", \"content\": \"You are an assistant participating in a multi-agent chat: Human, Llama, Qwen.\"}\n",
    "            print(\"[NOTICE] History reset.\")\n",
    "            new_state = {\"user_input\": \"\", \"should_exit\": False, \"messages\": [sys_msg], \"last_model\": \"\"}\n",
    "            # Save checkpoint immediately so reset is durable\n",
    "            save_checkpoint(new_state)  # safe: save minimal fields\n",
    "            return new_state\n",
    "        if low == \"save_and_exit\":\n",
    "            # persist and exit gracefully\n",
    "            new_state = {\"user_input\": raw, \"should_exit\": True}\n",
    "            # Save full state now\n",
    "            full_state = {\n",
    "                \"messages\": state.get(\"messages\", []),\n",
    "                \"verbose\": bool(state.get(\"verbose\", False)),\n",
    "                \"last_model\": state.get(\"last_model\", \"\")\n",
    "            }\n",
    "            atomic_write_json(CHECKPOINT_PATH, full_state)\n",
    "            print(f\"[NOTICE] Checkpoint saved to {CHECKPOINT_PATH}; exiting.\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        # Normal input - append human message to canonical history\n",
    "        msgs = list(state.get(\"messages\", []))\n",
    "        msgs.append({\"speaker\": \"Human\", \"content\": raw})\n",
    "\n",
    "        # immediately persist the human turn before heavy model work (durable checkpoint)\n",
    "        new_state = {\"user_input\": raw, \"should_exit\": False, \"messages\": msgs, \"last_model\": \"\"}\n",
    "        try:\n",
    "            # Save checkpoint with current canonical messages & flags\n",
    "            ck = {\"messages\": msgs, \"verbose\": bool(state.get(\"verbose\", False)), \"last_model\": \"\"}\n",
    "            atomic_write_json(CHECKPOINT_PATH, ck)\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(f\"[TRACE] Checkpoint saved after human turn (messages={len(msgs)})\")\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Failed to checkpoint after human input:\", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def route_after_input(state: AgentState) -> str:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] route_after_input - user_input:\", repr(state.get(\"user_input\")))\n",
    "        if state.get(\"should_exit\", False):\n",
    "            return END\n",
    "        raw = str(state.get(\"user_input\", \"\") or \"\")\n",
    "        if raw.strip() == \"\":\n",
    "            print(\"[NOTICE] Empty input received â€” please type something.\")\n",
    "            return \"get_user_input\"\n",
    "        return \"call_model\"\n",
    "\n",
    "    def call_model(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] call_model invoked\")\n",
    "\n",
    "        raw = str(state.get(\"user_input\", \"\") or \"\")\n",
    "        canonical_msgs = list(state.get(\"messages\", []))\n",
    "\n",
    "        # Determine target\n",
    "        target_initial = \"Qwen\" if raw.lstrip().lower().startswith(\"hey qwen\") else \"Llama\"\n",
    "        target = target_initial\n",
    "\n",
    "        model_obj = qwen_llm if target.lower() == \"qwen\" else llama_llm\n",
    "        if target.lower() == \"qwen\" and model_obj is None:\n",
    "            # append tool notice and route to Llama\n",
    "            canonical_msgs.append({\"speaker\": \"Tool\", \"content\": \"Qwen is disabled in this runtime; routed to Llama instead.\"})\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] Qwen disabled; appended Tool message and routing to Llama\")\n",
    "            target = \"Llama\"\n",
    "            model_obj = llama_llm\n",
    "\n",
    "        # Build role messages for target and prepend system message\n",
    "        role_msgs = build_role_messages_for_target(target, canonical_msgs)\n",
    "        sys_msg = system_message_for_target(target)\n",
    "        role_msgs_with_sys = [{\"role\": \"system\", \"content\": sys_msg[\"content\"]}] + role_msgs\n",
    "\n",
    "        # If original human message started with Hey Qwen, remove that trigger from the prompt copy\n",
    "        if raw.lstrip().lower().startswith(\"hey qwen\"):\n",
    "            role_msgs_mod = [dict(rm) for rm in role_msgs]\n",
    "            for i in range(len(role_msgs_mod) - 1, -1, -1):\n",
    "                rm = role_msgs_mod[i]\n",
    "                if rm[\"role\"] == \"user\" and rm[\"content\"].lower().startswith(\"human:\"):\n",
    "                    after = rm[\"content\"][len(\"Human:\"):].lstrip()\n",
    "                    if after.lower().startswith(\"hey qwen\"):\n",
    "                        new_after = after[len(\"hey qwen\"):].lstrip()\n",
    "                        rm[\"content\"] = f\"Human: {new_after}\" if new_after != \"\" else rm[\"content\"]\n",
    "                    break\n",
    "            role_msgs_with_sys = [{\"role\": \"system\", \"content\": sys_msg[\"content\"]}] + role_msgs_mod\n",
    "\n",
    "        prompt_text = prompt_from_role_messages(role_msgs_with_sys)\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Prompt for target\", target, \"(truncated):\")\n",
    "            print(prompt_text[:1200])\n",
    "\n",
    "        # Call the model (may take time)\n",
    "        try:\n",
    "            response_text = model_obj.invoke(prompt_text)\n",
    "        except Exception as e:\n",
    "            response_text = f\"[MODEL-ERROR] {e}\"\n",
    "            print(\"[ERROR] Model invocation failed:\", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Sanitize model output and append\n",
    "        response_text_clean = sanitize_model_output(response_text, target)\n",
    "        canonical_msgs.append({\"speaker\": target, \"content\": response_text_clean})\n",
    "\n",
    "        # Save checkpoint after model reply (durable)\n",
    "        try:\n",
    "            ck = {\"messages\": canonical_msgs, \"verbose\": bool(state.get(\"verbose\", False)), \"last_model\": target}\n",
    "            atomic_write_json(CHECKPOINT_PATH, ck)\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(f\"[TRACE] Checkpoint saved after model reply (messages={len(canonical_msgs)})\")\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Failed to checkpoint after model reply:\", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Return updated state\n",
    "        llama_snip = response_text_clean if target.lower() == \"llama\" else \"\"\n",
    "        qwen_snip = response_text_clean if target.lower() == \"qwen\" else \"\"\n",
    "        return {\n",
    "            \"messages\": canonical_msgs,\n",
    "            \"last_model\": target,\n",
    "            \"llama_response\": llama_snip,\n",
    "            \"qwen_response\": qwen_snip\n",
    "        }\n",
    "\n",
    "    def print_response(state: AgentState) -> dict:\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] print_response invoked; last_model=\", state.get(\"last_model\"))\n",
    "\n",
    "        last_model = state.get(\"last_model\", \"\")\n",
    "        llama_text = (state.get(\"llama_response\") or \"\").strip()\n",
    "        qwen_text = (state.get(\"qwen_response\") or \"\").strip()\n",
    "\n",
    "        printed = False\n",
    "        if last_model.lower() == \"llama\" and llama_text:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ¦™ LLaMA Response\")\n",
    "            print(\"=\" * 70)\n",
    "            print(llama_text)\n",
    "            printed = True\n",
    "        elif last_model.lower() == \"qwen\" and qwen_text:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ§  Qwen Response\")\n",
    "            print(\"=\" * 70)\n",
    "            print(qwen_text)\n",
    "            printed = True\n",
    "        else:\n",
    "            msgs = state.get(\"messages\", []) or []\n",
    "            if msgs:\n",
    "                last_msg = msgs[-1]\n",
    "                sp = last_msg.get(\"speaker\", \"\")\n",
    "                cont = (last_msg.get(\"content\") or \"\").strip()\n",
    "                if sp.lower() in (\"llama\", \"qwen\") and cont:\n",
    "                    header = \"ðŸ¦™ LLaMA Response\" if sp.lower() == \"llama\" else \"ðŸ§  Qwen Response\"\n",
    "                    print(\"\\n\" + \"=\" * 70)\n",
    "                    print(header)\n",
    "                    print(\"=\" * 70)\n",
    "                    print(cont)\n",
    "                    printed = True\n",
    "\n",
    "        if not printed:\n",
    "            print(\"\\n[NOTICE] No model produced output this turn.\")\n",
    "            print(\"[DIAGNOSTIC] last_model:\", repr(last_model))\n",
    "            msgs = state.get(\"messages\", [])\n",
    "            print(\"[DIAGNOSTIC] messages count:\", len(msgs))\n",
    "            if len(msgs) > 0:\n",
    "                print(\"[DIAGNOSTIC] last messages (most recent last):\")\n",
    "                for m in msgs[-6:]:\n",
    "                    print(f\"  {m.get('speaker')}: {m.get('content')!s}\")\n",
    "\n",
    "        # Clear snippet fields\n",
    "        return {\"last_model\": \"\", \"llama_response\": \"\", \"qwen_response\": \"\"}\n",
    "\n",
    "    # Graph construction\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"get_user_input\", get_user_input)\n",
    "    graph.add_node(\"call_model\", call_model)\n",
    "    graph.add_node(\"print_response\", print_response)\n",
    "\n",
    "    graph.add_edge(START, \"get_user_input\")\n",
    "    graph.add_conditional_edges(\n",
    "        \"get_user_input\",\n",
    "        route_after_input,\n",
    "        {\n",
    "            \"get_user_input\": \"get_user_input\",\n",
    "            \"call_model\": \"call_model\",\n",
    "            END: END,\n",
    "        },\n",
    "    )\n",
    "    graph.add_edge(\"call_model\", \"print_response\")\n",
    "    graph.add_edge(\"print_response\", \"get_user_input\")\n",
    "\n",
    "    return graph.compile()\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Dual-LLM LangGraph Agent with Checkpointing (Llama <-> Qwen)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Load checkpoint if present\n",
    "    ck = load_checkpoint()\n",
    "    restored_messages = ck.get(\"messages\")\n",
    "    restored_verbose = ck.get(\"verbose\", False)\n",
    "    restored_last_model = ck.get(\"last_model\", \"\")\n",
    "\n",
    "    # Create models\n",
    "    llama_llm, qwen_llm = create_models()\n",
    "    graph = create_graph(llama_llm, qwen_llm)\n",
    "\n",
    "    # Build initial state from checkpoint or defaults\n",
    "    if restored_messages is None:\n",
    "        system_msg = {\"speaker\": \"System\", \"content\": \"You are an assistant participating in a multi-agent chat: Human, Llama, Qwen.\"}\n",
    "        initial_messages = [system_msg]\n",
    "    else:\n",
    "        initial_messages = restored_messages\n",
    "\n",
    "    initial_state: AgentState = {\n",
    "        \"user_input\": \"\",\n",
    "        \"should_exit\": False,\n",
    "        \"last_model\": restored_last_model or \"\",\n",
    "        \"verbose\": bool(restored_verbose),\n",
    "        \"messages\": initial_messages,\n",
    "        \"llama_response\": \"\",\n",
    "        \"qwen_response\": \"\",\n",
    "    }\n",
    "\n",
    "    # Register exit handlers that persist the current checkpoint (wrap in closure)\n",
    "    def _save_on_exit():\n",
    "        try:\n",
    "            state_snapshot = {\n",
    "                \"messages\": initial_state.get(\"messages\", []),\n",
    "                \"verbose\": bool(initial_state.get(\"verbose\", False)),\n",
    "                \"last_model\": initial_state.get(\"last_model\", \"\")\n",
    "            }\n",
    "            atomic_write_json(CHECKPOINT_PATH, state_snapshot)\n",
    "            print(f\"[INFO] Saved final checkpoint to {CHECKPOINT_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Could not save final checkpoint on exit:\", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "    atexit.register(_save_on_exit)\n",
    "\n",
    "    # Signal handlers: attempt to save (best-effort)\n",
    "    def _handle_signal(sig, frame):\n",
    "        print(f\"\\n[INFO] Caught signal {sig}; saving checkpoint and exiting...\")\n",
    "        try:\n",
    "            state_snapshot = {\n",
    "                \"messages\": initial_state.get(\"messages\", []),\n",
    "                \"verbose\": bool(initial_state.get(\"verbose\", False)),\n",
    "                \"last_model\": initial_state.get(\"last_model\", \"\")\n",
    "            }\n",
    "            atomic_write_json(CHECKPOINT_PATH, state_snapshot)\n",
    "            print(f\"[INFO] Checkpoint saved to {CHECKPOINT_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Failed to save checkpoint on signal:\", e)\n",
    "            traceback.print_exc()\n",
    "        # exit after saving\n",
    "        os._exit(0)\n",
    "\n",
    "    signal.signal(signal.SIGINT, _handle_signal)\n",
    "    try:\n",
    "        signal.signal(signal.SIGTERM, _handle_signal)\n",
    "    except Exception:\n",
    "        # some environments don't allow setting SIGTERM\n",
    "        pass\n",
    "\n",
    "    if not TRANSFORMERS_OK or qwen_llm is None:\n",
    "        print(\"\\n[NOTICE] Running with fallbacks or Qwen disabled. Checkpointing still active.\\n\")\n",
    "\n",
    "    # Run the graph interactively\n",
    "    graph.invoke(initial_state)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
