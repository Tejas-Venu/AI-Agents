{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9Nx0Vf5eE6X"
      },
      "source": [
        "Pre-Task - Setting Up Ollama Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl5VocA_Su6K",
        "outputId": "98d5fdf5-4187-4bbc-a3c8-37f7230d8f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch datasets accelerate tqdm bitsandbytes huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWpA2D5Rexkj",
        "outputId": "5dae592d-03ad-40c6-9109-97302a38fa26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LHZ6rOhezyS",
        "outputId": "02ea9fd9-64d5-44a3-bec3-e01ed8497d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 66 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y zstd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJWESq5FqLD9",
        "outputId": "51cfc61e-3ad5-4098-948f-f381975f8123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*   Trying 34.36.133.15:443...\n",
            "* Connected to ollama.com (34.36.133.15) port 443 (#0)\n",
            "* ALPN, offering h2\n",
            "* ALPN, offering http/1.1\n",
            "*  CAfile: /etc/ssl/certs/ca-certificates.crt\n",
            "*  CApath: /etc/ssl/certs\n",
            "* TLSv1.0 (OUT), TLS header, Certificate Status (22):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n",
            "} [512 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Certificate Status (22):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Server hello (2):\n",
            "{ [122 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Finished (20):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n",
            "{ [19 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Certificate (11):\n",
            "{ [4053 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n",
            "{ [264 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Finished (20):\n",
            "{ [52 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Finished (20):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n",
            "} [1 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS handshake, Finished (20):\n",
            "} [52 bytes data]\n",
            "* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n",
            "* ALPN, server accepted to use h2\n",
            "* Server certificate:\n",
            "*  subject: CN=ollama.com\n",
            "*  start date: Jan  7 10:32:26 2026 GMT\n",
            "*  expire date: Apr  7 11:26:40 2026 GMT\n",
            "*  subjectAltName: host \"ollama.com\" matched cert's \"ollama.com\"\n",
            "*  issuer: C=US; O=Google Trust Services; CN=WR3\n",
            "*  SSL certificate verify ok.\n",
            "* Using HTTP2, server supports multiplexing\n",
            "* Connection state changed (HTTP/2 confirmed)\n",
            "* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* Using Stream ID: 1 (easy handle 0x5b8388a1deb0)\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "> GET /install.sh HTTP/2\r\n",
            "> Host: ollama.com\r\n",
            "> user-agent: curl/7.81.0\r\n",
            "> accept: */*\r\n",
            "> \r\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n",
            "{ [284 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n",
            "{ [284 bytes data]\n",
            "* old SSL session ID is stale, removing\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "< HTTP/2 307 \r\n",
            "< content-type: text/html; charset=utf-8\r\n",
            "< location: https://github.com/ollama/ollama/releases/latest/download/install.sh\r\n",
            "< set-cookie: aid=0093441a-8550-43f8-890a-55208d68518c; Path=/; Max-Age=31536000; HttpOnly; Secure; SameSite=Lax\r\n",
            "< x-build-commit: b0778b1e2590b6112e427257b2ad95201375baf9\r\n",
            "< x-build-time: 2026-02-21T15:24:09-08:00\r\n",
            "< x-frame-options: DENY\r\n",
            "< x-request-id: 845c6461-37e8-4feb-926a-8aaf54a7fc86\r\n",
            "< content-length: 0\r\n",
            "< date: Sun, 22 Feb 2026 18:42:12 GMT\r\n",
            "< server: Google Frontend\r\n",
            "< x-cloud-trace-context: d77e4ab1d7af1d98c5bc2e122670a114/8402593015183930932\r\n",
            "< traceparent: 00-d77e4ab1d7af1d98c5bc2e122670a114-749c01d416377634-00\r\n",
            "< via: 1.1 google\r\n",
            "< alt-svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000\r\n",
            "< \r\n",
            "{ [0 bytes data]\n",
            "* Connection #0 to host ollama.com left intact\n",
            "* Issue another request to this URL: 'https://github.com/ollama/ollama/releases/latest/download/install.sh'\n",
            "*   Trying 140.82.121.3:443...\n",
            "* Connected to github.com (140.82.121.3) port 443 (#1)\n",
            "* ALPN, offering h2\n",
            "* ALPN, offering http/1.1\n",
            "*  CAfile: /etc/ssl/certs/ca-certificates.crt\n",
            "*  CApath: /etc/ssl/certs\n",
            "* TLSv1.0 (OUT), TLS header, Certificate Status (22):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n",
            "} [512 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Certificate Status (22):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Server hello (2):\n",
            "{ [122 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Finished (20):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n",
            "{ [19 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Certificate (11):\n",
            "{ [2740 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n",
            "{ [80 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Finished (20):\n",
            "{ [36 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Finished (20):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n",
            "} [1 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS handshake, Finished (20):\n",
            "} [36 bytes data]\n",
            "* SSL connection using TLSv1.3 / TLS_AES_128_GCM_SHA256\n",
            "* ALPN, server accepted to use h2\n",
            "* Server certificate:\n",
            "*  subject: CN=github.com\n",
            "*  start date: Jan  6 00:00:00 2026 GMT\n",
            "*  expire date: Apr  5 23:59:59 2026 GMT\n",
            "*  subjectAltName: host \"github.com\" matched cert's \"github.com\"\n",
            "*  issuer: C=GB; O=Sectigo Limited; CN=Sectigo Public Server Authentication CA DV E36\n",
            "*  SSL certificate verify ok.\n",
            "* Using HTTP2, server supports multiplexing\n",
            "* Connection state changed (HTTP/2 confirmed)\n",
            "* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* Using Stream ID: 1 (easy handle 0x5b8388a1deb0)\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "> GET /ollama/ollama/releases/latest/download/install.sh HTTP/2\r\n",
            "> Host: github.com\r\n",
            "> user-agent: curl/7.81.0\r\n",
            "> accept: */*\r\n",
            "> \n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n",
            "{ [57 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n",
            "{ [57 bytes data]\n",
            "* old SSL session ID is stale, removing\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "< HTTP/2 302 \n",
            "< date: Sun, 22 Feb 2026 18:40:05 GMT\n",
            "< content-type: text/html; charset=utf-8\n",
            "< content-length: 0\n",
            "< vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, X-Requested-With, Sec-Fetch-Site,Accept-Encoding, Accept, X-Requested-With\n",
            "< location: https://github.com/ollama/ollama/releases/download/v0.16.3/install.sh\n",
            "< cache-control: no-cache\n",
            "< strict-transport-security: max-age=31536000; includeSubdomains; preload\n",
            "< x-frame-options: deny\n",
            "< x-content-type-options: nosniff\n",
            "< x-xss-protection: 0\n",
            "< referrer-policy: no-referrer-when-downgrade\n",
            "< content-security-policy: default-src 'none'; base-uri 'self'; child-src github.githubassets.com github.com/assets-cdn/worker/ github.com/assets/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com *.rel.tunnels.api.visualstudio.com wss://*.rel.tunnels.api.visualstudio.com github.githubassets.com objects-origin.githubusercontent.com copilot-proxy.githubusercontent.com proxy.individual.githubcopilot.com proxy.business.githubcopilot.com proxy.enterprise.githubcopilot.com *.actions.githubusercontent.com wss://*.actions.githubusercontent.com productionresultssa0.blob.core.windows.net/ productionresultssa1.blob.core.windows.net/ productionresultssa2.blob.core.windows.net/ productionresultssa3.blob.core.windows.net/ productionresultssa4.blob.core.windows.net/ productionresultssa5.blob.core.windows.net/ productionresultssa6.blob.core.windows.net/ productionresultssa7.blob.core.windows.net/ productionresultssa8.blob.core.windows.net/ productionresultssa9.blob.core.windows.net/ productionresultssa10.blob.core.windows.net/ productionresultssa11.blob.core.windows.net/ productionresultssa12.blob.core.windows.net/ productionresultssa13.blob.core.windows.net/ productionresultssa14.blob.core.windows.net/ productionresultssa15.blob.core.windows.net/ productionresultssa16.blob.core.windows.net/ productionresultssa17.blob.core.windows.net/ productionresultssa18.blob.core.windows.net/ productionresultssa19.blob.core.windows.net/ github-production-repository-image-32fea6.s3.amazonaws.com github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com wss://alive.github.com wss://alive-staging.github.com api.githubcopilot.com api.individual.githubcopilot.com api.business.githubcopilot.com api.enterprise.githubcopilot.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com copilot-workspace.githubnext.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: blob: github.githubassets.com media.githubusercontent.com camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com private-avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com release-assets.githubusercontent.com secured-user-images.githubusercontent.com/ user-images.githubusercontent.com/ private-user-images.githubusercontent.com opengraph.githubassets.com marketplace-screenshots.githubusercontent.com/ copilotprodattachments.blob.core.windows.net/github-production-copilot-attachments/ github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com spotlights-feed.github.com objects-origin.githubusercontent.com *.githubusercontent.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ secured-user-images.githubusercontent.com/ private-user-images.githubusercontent.com github-production-user-asset-6210df.s3.amazonaws.com gist.github.com github.githubassets.com; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; upgrade-insecure-requests; worker-src github.githubassets.com github.com/assets-cdn/worker/ github.com/assets/ gist.github.com/assets-cdn/worker/\n",
            "< server: github.com\n",
            "< set-cookie: _gh_sess=p4LdIo4c6HJ3PuM6ttNZIuCdqrXA2ZEnc2g28EeRH0kAId7aVHFolWBr0wqXMvIEo23QnAEKwzxRtQOJxmWxjmATIqJ0l7Aj0GnFm%2B%2FQ9ZVktypDdyYckDwbO8YQTv0NhTpveABdsnOHIqkyw0jFJTzEWrNrNbrM00b2jco4Dk7XjkqpJvB7ehBBlNafxgjEmCqWPq63BnCOlhgqOdZI7ehUkfgLakDHwsMdsl%2BWYmM3JFp9SlK2CcCqrtRRwDQtkWf9kxZkxGByakIv3ZwORg%3D%3D--2gw22BPz9tVSIUEz--mlwqBmaG6Tpjd94ncdmE5A%3D%3D; Path=/; HttpOnly; Secure; SameSite=Lax\n",
            "< set-cookie: _octo=GH1.1.2089898415.1771785732; Path=/; Domain=github.com; Expires=Mon, 22 Feb 2027 18:42:12 GMT; Secure; SameSite=Lax\n",
            "< set-cookie: logged_in=no; Path=/; Domain=github.com; Expires=Mon, 22 Feb 2027 18:42:12 GMT; HttpOnly; Secure; SameSite=Lax\n",
            "< x-github-request-id: ABFA:3F4A0B:3CB7AC7:2E2B60E:699B4E04\n",
            "< \n",
            "{ [0 bytes data]\n",
            "* Connection #1 to host github.com left intact\n",
            "* Issue another request to this URL: 'https://github.com/ollama/ollama/releases/download/v0.16.3/install.sh'\n",
            "* Found bundle for host github.com: 0x5b8388bc6660 [can multiplex]\n",
            "* Re-using existing connection! (#1) with host github.com\n",
            "* Connected to github.com (140.82.121.3) port 443 (#1)\n",
            "* Using Stream ID: 3 (easy handle 0x5b8388a1deb0)\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "> GET /ollama/ollama/releases/download/v0.16.3/install.sh HTTP/2\n",
            "> Host: github.com\n",
            "> user-agent: curl/7.81.0\n",
            "> accept: */*\n",
            "> \n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "< HTTP/2 302 \n",
            "< date: Sun, 22 Feb 2026 18:41:23 GMT\n",
            "< content-type: text/html; charset=utf-8\n",
            "< content-length: 0\n",
            "< vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, X-Requested-With, Sec-Fetch-Site,Accept-Encoding, Accept, X-Requested-With\n",
            "< location: https://release-assets.githubusercontent.com/github-production-release-asset/658928958/a08bb125-d2ac-4873-b369-2137a330e32d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-02-22T19%3A23%3A49Z&rscd=attachment%3B+filename%3Dinstall.sh&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-02-22T18%3A23%3A07Z&ske=2026-02-22T19%3A23%3A49Z&sks=b&skv=2018-11-09&sig=4q1SrtvdtbmPwNlTO3Lk1GLrq8%2FGPotlLthgmiLkgUU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc3MTc4NTk4MywibmJmIjoxNzcxNzg1NjgzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.nVDfYlkWswwk0LzYgOnwfaHKONxZMRiFv_RlIHqPOxo&response-content-disposition=attachment%3B%20filename%3Dinstall.sh&response-content-type=application%2Foctet-stream\n",
            "< cache-control: no-cache\n",
            "< strict-transport-security: max-age=31536000; includeSubdomains; preload\n",
            "< x-frame-options: deny\n",
            "< x-content-type-options: nosniff\n",
            "< x-xss-protection: 0\n",
            "< referrer-policy: no-referrer-when-downgrade\n",
            "< content-security-policy: default-src 'none'; base-uri 'self'; child-src github.githubassets.com github.com/assets-cdn/worker/ github.com/assets/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com *.rel.tunnels.api.visualstudio.com wss://*.rel.tunnels.api.visualstudio.com github.githubassets.com objects-origin.githubusercontent.com copilot-proxy.githubusercontent.com proxy.individual.githubcopilot.com proxy.business.githubcopilot.com proxy.enterprise.githubcopilot.com *.actions.githubusercontent.com wss://*.actions.githubusercontent.com productionresultssa0.blob.core.windows.net/ productionresultssa1.blob.core.windows.net/ productionresultssa2.blob.core.windows.net/ productionresultssa3.blob.core.windows.net/ productionresultssa4.blob.core.windows.net/ productionresultssa5.blob.core.windows.net/ productionresultssa6.blob.core.windows.net/ productionresultssa7.blob.core.windows.net/ productionresultssa8.blob.core.windows.net/ productionresultssa9.blob.core.windows.net/ productionresultssa10.blob.core.windows.net/ productionresultssa11.blob.core.windows.net/ productionresultssa12.blob.core.windows.net/ productionresultssa13.blob.core.windows.net/ productionresultssa14.blob.core.windows.net/ productionresultssa15.blob.core.windows.net/ productionresultssa16.blob.core.windows.net/ productionresultssa17.blob.core.windows.net/ productionresultssa18.blob.core.windows.net/ productionresultssa19.blob.core.windows.net/ github-production-repository-image-32fea6.s3.amazonaws.com github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com wss://alive.github.com wss://alive-staging.github.com api.githubcopilot.com api.individual.githubcopilot.com api.business.githubcopilot.com api.enterprise.githubcopilot.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com copilot-workspace.githubnext.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: blob: github.githubassets.com media.githubusercontent.com camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com private-avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com release-assets.githubusercontent.com secured-user-images.githubusercontent.com/ user-images.githubusercontent.com/ private-user-images.githubusercontent.com opengraph.githubassets.com marketplace-screenshots.githubusercontent.com/ copilotprodattachments.blob.core.windows.net/github-production-copilot-attachments/ github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com spotlights-feed.github.com objects-origin.githubusercontent.com *.githubusercontent.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ secured-user-images.githubusercontent.com/ private-user-images.githubusercontent.com github-production-user-asset-6210df.s3.amazonaws.com gist.github.com github.githubassets.com; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; upgrade-insecure-requests; worker-src github.githubassets.com github.com/assets-cdn/worker/ github.com/assets/ gist.github.com/assets-cdn/worker/\n",
            "< server: github.com\n",
            "< x-github-request-id: ABFA:3F4A0B:3CB7AE3:2E2B61D:699B4E04\n",
            "< \n",
            "{ [0 bytes data]\n",
            "* Connection #1 to host github.com left intact\n",
            "* Issue another request to this URL: 'https://release-assets.githubusercontent.com/github-production-release-asset/658928958/a08bb125-d2ac-4873-b369-2137a330e32d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-02-22T19%3A23%3A49Z&rscd=attachment%3B+filename%3Dinstall.sh&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-02-22T18%3A23%3A07Z&ske=2026-02-22T19%3A23%3A49Z&sks=b&skv=2018-11-09&sig=4q1SrtvdtbmPwNlTO3Lk1GLrq8%2FGPotlLthgmiLkgUU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc3MTc4NTk4MywibmJmIjoxNzcxNzg1NjgzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.nVDfYlkWswwk0LzYgOnwfaHKONxZMRiFv_RlIHqPOxo&response-content-disposition=attachment%3B%20filename%3Dinstall.sh&response-content-type=application%2Foctet-stream'\n",
            "*   Trying 185.199.108.133:443...\n",
            "* Connected to release-assets.githubusercontent.com (185.199.108.133) port 443 (#2)\n",
            "* ALPN, offering h2\n",
            "* ALPN, offering http/1.1\n",
            "*  CAfile: /etc/ssl/certs/ca-certificates.crt\n",
            "*  CApath: /etc/ssl/certs\n",
            "* TLSv1.0 (OUT), TLS header, Certificate Status (22):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n",
            "} [512 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Certificate Status (22):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Server hello (2):\n",
            "{ [122 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Finished (20):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n",
            "{ [19 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Certificate (11):\n",
            "{ [4668 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n",
            "{ [264 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Finished (20):\n",
            "{ [36 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Finished (20):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n",
            "} [1 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.3 (OUT), TLS handshake, Finished (20):\n",
            "} [36 bytes data]\n",
            "* SSL connection using TLSv1.3 / TLS_AES_128_GCM_SHA256\n",
            "* ALPN, server accepted to use h2\n",
            "* Server certificate:\n",
            "*  subject: CN=*.github.io\n",
            "*  start date: Mar  7 00:00:00 2025 GMT\n",
            "*  expire date: Mar  7 23:59:59 2026 GMT\n",
            "*  subjectAltName: host \"release-assets.githubusercontent.com\" matched cert's \"*.githubusercontent.com\"\n",
            "*  issuer: C=GB; ST=Greater Manchester; L=Salford; O=Sectigo Limited; CN=Sectigo RSA Domain Validation Secure Server CA\n",
            "*  SSL certificate verify ok.\n",
            "* Using HTTP2, server supports multiplexing\n",
            "* Connection state changed (HTTP/2 confirmed)\n",
            "* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* Using Stream ID: 1 (easy handle 0x5b8388a1deb0)\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "> GET /github-production-release-asset/658928958/a08bb125-d2ac-4873-b369-2137a330e32d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-02-22T19%3A23%3A49Z&rscd=attachment%3B+filename%3Dinstall.sh&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-02-22T18%3A23%3A07Z&ske=2026-02-22T19%3A23%3A49Z&sks=b&skv=2018-11-09&sig=4q1SrtvdtbmPwNlTO3Lk1GLrq8%2FGPotlLthgmiLkgUU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc3MTc4NTk4MywibmJmIjoxNzcxNzg1NjgzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.nVDfYlkWswwk0LzYgOnwfaHKONxZMRiFv_RlIHqPOxo&response-content-disposition=attachment%3B%20filename%3Dinstall.sh&response-content-type=application%2Foctet-stream HTTP/2\n",
            "> Host: release-assets.githubusercontent.com\n",
            "> user-agent: curl/7.81.0\n",
            "> accept: */*\n",
            "> \n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n",
            "{ [193 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (OUT), TLS header, Supplemental data (23):\n",
            "} [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "< HTTP/2 200 \n",
            "< last-modified: Fri, 20 Feb 2026 00:55:46 GMT\n",
            "< etag: \"0x8DE701AC820C989\"\n",
            "< server: Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0\n",
            "< x-ms-request-id: 28875788-601e-0061-1804-a2c216000000\n",
            "< x-ms-version: 2018-11-09\n",
            "< x-ms-creation-time: Fri, 20 Feb 2026 00:55:46 GMT\n",
            "< x-ms-blob-content-md5: wJdgsTaP3cCg1D3GAdVA8A==\n",
            "< x-ms-lease-status: unlocked\n",
            "< x-ms-lease-state: available\n",
            "< x-ms-blob-type: BlockBlob\n",
            "< x-ms-server-encrypted: true\n",
            "< via: 1.1 varnish, 1.1 varnish\n",
            "< accept-ranges: bytes\n",
            "< date: Sun, 22 Feb 2026 18:42:12 GMT\n",
            "< age: 6291\n",
            "< x-served-by: cache-iad-kcgs7200040-IAD, cache-ams2100133-AMS\n",
            "< x-cache: HIT, HIT\n",
            "< x-cache-hits: 27, 1\n",
            "< x-timer: S1771785733.592110,VS0,VE1\n",
            "< content-disposition: attachment; filename=install.sh\n",
            "< content-type: application/octet-stream\n",
            "< content-length: 15902\n",
            "< \n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* TLSv1.2 (IN), TLS header, Supplemental data (23):\n",
            "{ [5 bytes data]\n",
            "* Connection #2 to host release-assets.githubusercontent.com left intact\n",
            "-rw-r--r-- 1 root root 15902 Feb 22 18:42 /tmp/ollama_install.sh\n",
            "#!/bin/sh\n",
            "# This script installs Ollama on Linux and macOS.\n",
            "# It detects the current operating system architecture and installs the appropriate version of Ollama.\n",
            "\n",
            "# Wrap script in main function so that a truncated partial download doesn't end\n",
            "# up executing half a script.\n",
            "main() {\n",
            "\n",
            "set -eu\n",
            "\n",
            "red=\"$( (/usr/bin/tput bold || :; /usr/bin/tput setaf 1 || :) 2>&-)\"\n",
            "plain=\"$( (/usr/bin/tput sgr0 || :) 2>&-)\"\n",
            "\n",
            "status() { echo \">>> $*\" >&2; }\n",
            "error() { echo \"${red}ERROR:${plain} $*\"; exit 1; }\n",
            "warning() { echo \"${red}WARNING:${plain} $*\"; }\n",
            "\n",
            "TEMP_DIR=$(mktemp -d)\n",
            "cleanup() { rm -rf $TEMP_DIR; }\n",
            "trap cleanup EXIT\n",
            "\n",
            "available() { command -v $1 >/dev/null; }\n",
            "require() {\n",
            "    local MISSING=''\n",
            "    for TOOL in $*; do\n",
            "        if ! available $TOOL; then\n",
            "            MISSING=\"$MISSING $TOOL\"\n",
            "        fi\n",
            "    done\n",
            "\n",
            "    echo $MISSING\n",
            "}\n",
            "\n",
            "OS=\"$(uname -s)\"\n",
            "ARCH=$(uname -m)\n",
            "case \"$ARCH\" in\n",
            "    x86_64) ARCH=\"amd64\" ;;\n",
            "    aarch64|arm64) ARCH=\"arm64\" ;;\n",
            "    *) error \"Unsupported architecture: $ARCH\" ;;\n",
            "esac\n",
            "\n",
            "VER_PARAM=\"${OLLAMA_VERSION:+?version=$OLLAMA_VERSION}\"\n",
            "\n",
            "###########################################\n",
            "# macOS\n",
            "###########################################\n",
            "\n",
            "if [ \"$OS\" = \"Darwin\" ]; then\n",
            "    NEEDS=$(require curl unzip)\n",
            "    if [ -n \"$NEEDS\" ]; then\n",
            "        status \"ERROR: The following tools are required but missing:\"\n",
            "        for NEED in $NEEDS; do\n",
            "            echo \"  - $NEED\"\n",
            "        done\n",
            "        exit 1\n",
            "    fi\n",
            "\n",
            "    DOWNLOAD_URL=\"https://ollama.com/download/Ollama-darwin.zip${VER_PARAM}\"\n",
            "\n",
            "    if pgrep -x Ollama >/dev/null 2>&1; then\n",
            "        status \"Stopping running Ollama instance...\"\n",
            "        pkill -x Ollama 2>/dev/null || true\n",
            "        sleep 2\n",
            "    fi\n",
            "\n",
            "    if [ -d \"/Applications/Ollama.app\" ]; then\n",
            "        status \"Removing existing Ollama installation...\"\n",
            "        rm -rf \"/Applications/Ollama.app\"\n",
            "    fi\n",
            "\n",
            "    status \"Downloading Ollama for macOS...\"\n",
            "    curl --fail --show-error --location --progress-bar \\\n",
            "        -o \"$TEMP_DIR/Ollama-darwin.zip\" \"$DOWNLOAD_URL\"\n",
            "\n",
            "    status \"Installing Ollama to /Applications...\"\n",
            "    unzip -q \"$TEMP_DIR/Ollama-darwin.zip\" -d \"$TEMP_DIR\"\n",
            "    mv \"$TEMP_DIR/Ollama.app\" \"/Applications/\"\n",
            "\n",
            "    if [ ! -L \"/usr/local/bin/ollama\" ] || [ \"$(readlink \"/usr/local/bin/ollama\")\" != \"/Applications/Ollama.app/Contents/Resources/ollama\" ]; then\n",
            "        status \"Adding 'ollama' command to PATH (may require password)...\"\n",
            "        mkdir -p \"/usr/local/bin\" 2>/dev/null || sudo mkdir -p \"/usr/local/bin\"\n",
            "        ln -sf \"/Applications/Ollama.app/Contents/Resources/ollama\" \"/usr/local/bin/ollama\" 2>/dev/null || \\\n",
            "            sudo ln -sf \"/Applications/Ollama.app/Contents/Resources/ollama\" \"/usr/local/bin/ollama\"\n",
            "    fi\n",
            "\n",
            "    if [ -z \"${OLLAMA_NO_START:-}\" ]; then\n",
            "        status \"Starting Ollama...\"\n",
            "        open -a Ollama --args hidden\n",
            "    fi\n",
            "\n",
            "    status \"Install complete. You can now run 'ollama'.\"\n",
            "    exit 0\n",
            "fi\n",
            "\n",
            "###########################################\n",
            "# Linux\n",
            "###########################################\n",
            "\n",
            "[ \"$OS\" = \"Linux\" ] || error 'This script is intended to run on Linux and macOS only.'\n",
            "\n",
            "IS_WSL2=false\n",
            "\n",
            "KERN=$(uname -r)\n",
            "case \"$KERN\" in\n",
            "    *icrosoft*WSL2 | *icrosoft*wsl2) IS_WSL2=true;;\n",
            "    *icrosoft) error \"Microsoft WSL1 is not currently supported. Please use WSL2 with 'wsl --set-version <distro> 2'\" ;;\n",
            "    *) ;;\n",
            "esac\n",
            "\n",
            "SUDO=\n",
            "if [ \"$(id -u)\" -ne 0 ]; then\n",
            "    # Running as root, no need for sudo\n",
            "    if ! available sudo; then\n",
            "        error \"This script requires superuser permissions. Please re-run as root.\"\n",
            "    fi\n",
            "\n",
            "    SUDO=\"sudo\"\n",
            "fi\n",
            "\n",
            "NEEDS=$(require curl awk grep sed tee xargs)\n",
            "if [ -n \"$NEEDS\" ]; then\n",
            "    status \"ERROR: The following tools are required but missing:\"\n",
            "    for NEED in $NEEDS; do\n",
            "        echo \"  - $NEED\"\n",
            "    done\n",
            "    exit 1\n",
            "fi\n",
            "\n",
            "# Function to download and extract with fallback from zst to tgz\n",
            "download_and_extract() {\n",
            "    local url_base=\"$1\"\n",
            "    local dest_dir=\"$2\"\n",
            "    local filename=\"$3\"\n",
            "\n",
            "    # Check if .tar.zst is available\n",
            "    if curl --fail --silent --head --location \"${url_base}/${filename}.tar.zst${VER_PARAM}\" >/dev/null 2>&1; then\n",
            "        # zst file exists - check if we have zstd tool\n",
            "        if ! available zstd; then\n",
            "            error \"This version requires zstd for extraction. Please install zstd and try again:\n",
            "  - Debian/Ubuntu: sudo apt-get install zstd\n",
            "  - RHEL/CentOS/Fedora: sudo dnf install zstd\n",
            "  - Arch: sudo pacman -S zstd\"\n",
            "        fi\n",
            "\n",
            "        status \"Downloading ${filename}.tar.zst\"\n",
            "        curl --fail --show-error --location --progress-bar \\\n",
            "            \"${url_base}/${filename}.tar.zst${VER_PARAM}\" | \\\n",
            "            zstd -d | $SUDO tar -xf - -C \"${dest_dir}\"\n",
            "        return 0\n",
            "    fi\n",
            "\n",
            "    # Fall back to .tgz for older versions\n",
            "    status \"Downloading ${filename}.tgz\"\n",
            "    curl --fail --show-error --location --progress-bar \\\n",
            "        \"${url_base}/${filename}.tgz${VER_PARAM}\" | \\\n",
            "        $SUDO tar -xzf - -C \"${dest_dir}\"\n",
            "}\n",
            "\n",
            "for BINDIR in /usr/local/bin /usr/bin /bin; do\n",
            "    echo $PATH | grep -q $BINDIR && break || continue\n",
            "done\n",
            "OLLAMA_INSTALL_DIR=$(dirname ${BINDIR})\n",
            "\n",
            "if [ -d \"$OLLAMA_INSTALL_DIR/lib/ollama\" ] ; then\n",
            "    status \"Cleaning up old version at $OLLAMA_INSTALL_DIR/lib/ollama\"\n",
            "    $SUDO rm -rf \"$OLLAMA_INSTALL_DIR/lib/ollama\"\n",
            "fi\n",
            "status \"Installing ollama to $OLLAMA_INSTALL_DIR\"\n",
            "$SUDO install -o0 -g0 -m755 -d $BINDIR\n",
            "$SUDO install -o0 -g0 -m755 -d \"$OLLAMA_INSTALL_DIR/lib/ollama\"\n",
            "download_and_extract \"https://ollama.com/download\" \"$OLLAMA_INSTALL_DIR\" \"ollama-linux-${ARCH}\"\n",
            "\n",
            "if [ \"$OLLAMA_INSTALL_DIR/bin/ollama\" != \"$BINDIR/ollama\" ] ; then\n",
            "    status \"Making ollama accessible in the PATH in $BINDIR\"\n",
            "    $SUDO ln -sf \"$OLLAMA_INSTALL_DIR/ollama\" \"$BINDIR/ollama\"\n",
            "fi\n",
            "\n",
            "# Check for NVIDIA JetPack systems with additional downloads\n",
            "if [ -f /etc/nv_tegra_release ] ; then\n",
            "    if grep R36 /etc/nv_tegra_release > /dev/null ; then\n",
            "        download_and_extract \"https://ollama.com/download\" \"$OLLAMA_INSTALL_DIR\" \"ollama-linux-${ARCH}-jetpack6\"\n",
            "    elif grep R35 /etc/nv_tegra_release > /dev/null ; then\n",
            "        download_and_extract \"https://ollama.com/download\" \"$OLLAMA_INSTALL_DIR\" \"ollama-linux-${ARCH}-jetpack5\"\n",
            "    else\n",
            "        warning \"Unsupported JetPack version detected.  GPU may not be supported\"\n",
            "    fi\n",
            "fi\n",
            "\n",
            "install_success() {\n",
            "    status 'The Ollama API is now available at 127.0.0.1:11434.'\n",
            "    status 'Install complete. Run \"ollama\" from the command line.'\n",
            "}\n",
            "trap install_success EXIT\n",
            "\n",
            "# Everything from this point onwards is optional.\n",
            "\n",
            "configure_systemd() {\n",
            "    if ! id ollama >/dev/null 2>&1; then\n",
            "        status \"Creating ollama user...\"\n",
            "        $SUDO useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama\n"
          ]
        }
      ],
      "source": [
        "# verbose attempt showing headers and HTTP status\n",
        "!curl -v -fsSL https://ollama.com/install.sh -o /tmp/ollama_install.sh || echo \"curl failed with exit $?\"\n",
        "\n",
        "# show saved response (if any)\n",
        "!ls -l /tmp/ollama_install.sh && sed -n '1,200p' /tmp/ollama_install.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNQmiFCPe4Hx",
        "outputId": "96529fcd-2bab-4422-95d6-153617e093e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1i7CdcXe8Uq",
        "outputId": "10c31dc1-f974-453a-e819-c1654a501ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Ollama server...\n",
            "✓ Ollama server started\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'],\n",
        "                                   stdout=subprocess.PIPE,\n",
        "                                   stderr=subprocess.PIPE)\n",
        "\n",
        "print(\"Starting Ollama server...\")\n",
        "print(\"✓ Ollama server started\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wThWyuvTfEKb",
        "outputId": "6e8d5bc9-5889-48ce-d36f-48dd3e914fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "!ollama pull llama3.2:1b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "f29e39165d6a4df291e162acbc9fb50b",
            "3bc0488c421b4afe92bb9b65c8ab9801",
            "ba8f5ac3478f4dd1bd83fbfef0d3fb78",
            "0f158288593f402f9ce99c51a591a953",
            "6a03c57e863b4621b9711f90ca47e606",
            "a220c6f29b1b4107bbfefe96df36aff5",
            "732a592584b043f7a157ae8672b5ed1b",
            "5aeff9fd2ba3433fbaacd7058b1857e1",
            "7a68be99065946f199caf872656bd08b",
            "946f4fa4fdaf420398fd0a6a62872292",
            "07b2fed01a53443692c798ecad6ece12",
            "99af52b5af6c4755b907fab497de90ed",
            "7fdf2ef5da2e4de3803cb3b584ee7106",
            "c47fcb666f2549fcaf4af051ea626175",
            "7a31506654d2491fac1f8488824cf6cc",
            "c9702b8a175e481bb942e73b4049887d",
            "16b31a1ac7674eb7809f63232b0c6091",
            "2e86a11951174f84929e1432daad2928",
            "a4071ade917e4a0897f4da864cb0f8c3",
            "7c47df64a0f848fc8e77c878d893978c"
          ]
        },
        "id": "2ydOIQFbfJgb",
        "outputId": "ae027355-c1b1-46b1-b861-c178507774d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f29e39165d6a4df291e162acbc9fb50b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf9gYkM2fWcs"
      },
      "source": [
        "Task 1\n",
        "\n",
        "Without Ollama Server - Sequential Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVupBn2bfYSJ",
        "outputId": "460e1758-3349-49bf-e9f0-f97e8c12ae43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing llama_mmlu_astronomy.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile llama_mmlu_astronomy.py\n",
        "\"\"\"\n",
        "Llama 3.2-1B MMLU Evaluation Script - ASTRONOMY ONLY\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "USE_GPU = True\n",
        "MAX_NEW_TOKENS = 1\n",
        "QUANTIZATION_BITS = 4  # Using 4-bit for faster loading in Colab\n",
        "MMLU_SUBJECTS = [\"astronomy\"]  # ONLY ASTRONOMY\n",
        "\n",
        "def get_quantization_config():\n",
        "    if QUANTIZATION_BITS == 4:\n",
        "        return BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "    return None\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    print(f\"Loading {MODEL_NAME} for ASTRONOMY evaluation...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    quant_config = get_quantization_config()\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def format_mmlu_prompt(question, choices):\n",
        "    choice_labels = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    prompt = f\"{question}\\n\\n\"\n",
        "    for label, choice in zip(choice_labels, choices):\n",
        "        prompt += f\"{label}. {choice}\\n\"\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def get_model_prediction(model, tokenizer, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False,\n",
        "            temperature=1.0\n",
        "        )\n",
        "    generated_text = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    answer = generated_text.strip()[:1].upper()\n",
        "    if answer not in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "        for char in generated_text.upper():\n",
        "            if char in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "                answer = char\n",
        "                break\n",
        "        else:\n",
        "            answer = \"A\"\n",
        "    return answer\n",
        "\n",
        "def evaluate_subject(model, tokenizer, subject):\n",
        "    print(f\"Evaluating: {subject}\")\n",
        "    dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for example in tqdm(dataset, desc=f\"Testing {subject}\"):\n",
        "        question = example[\"question\"]\n",
        "        choices = example[\"choices\"]\n",
        "        correct_answer_idx = example[\"answer\"]\n",
        "        correct_answer = [\"A\", \"B\", \"C\", \"D\"][correct_answer_idx]\n",
        "\n",
        "        prompt = format_mmlu_prompt(question, choices)\n",
        "        predicted_answer = get_model_prediction(model, tokenizer, prompt)\n",
        "\n",
        "        if predicted_answer == correct_answer:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    accuracy = (correct / total * 100) if total > 0 else 0\n",
        "    print(f\"Result: {correct}/{total} = {accuracy:.2f}%\")\n",
        "    return {\"subject\": subject, \"correct\": correct, \"total\": total, \"accuracy\": accuracy}\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"ASTRONOMY Evaluation\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    results = []\n",
        "    for subject in MMLU_SUBJECTS:\n",
        "        result = evaluate_subject(model, tokenizer, subject)\n",
        "        results.append(result)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "    output_file = f\"astronomy_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump({\"results\": results, \"duration_seconds\": duration}, f, indent=2)\n",
        "\n",
        "    print(f\"\\nCompleted in {duration:.1f} seconds\")\n",
        "    print(f\"Results saved to: {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0BveI79iNye",
        "outputId": "85e06f42-467a-4891-f128-9b2a59f829e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing llama_mmlu_business.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile llama_mmlu_business.py\n",
        "\"\"\"\n",
        "Llama 3.2-1B MMLU Evaluation Script - BUSINESS ETHICS ONLY\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "USE_GPU = True\n",
        "MAX_NEW_TOKENS = 1\n",
        "QUANTIZATION_BITS = 4\n",
        "MMLU_SUBJECTS = [\"business_ethics\"]  # ONLY BUSINESS ETHICS\n",
        "\n",
        "def get_quantization_config():\n",
        "    if QUANTIZATION_BITS == 4:\n",
        "        return BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "    return None\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    print(f\"Loading {MODEL_NAME} for BUSINESS ETHICS evaluation...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    quant_config = get_quantization_config()\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def format_mmlu_prompt(question, choices):\n",
        "    choice_labels = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    prompt = f\"{question}\\n\\n\"\n",
        "    for label, choice in zip(choice_labels, choices):\n",
        "        prompt += f\"{label}. {choice}\\n\"\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def get_model_prediction(model, tokenizer, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False,\n",
        "            temperature=1.0\n",
        "        )\n",
        "    generated_text = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    answer = generated_text.strip()[:1].upper()\n",
        "    if answer not in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "        for char in generated_text.upper():\n",
        "            if char in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "                answer = char\n",
        "                break\n",
        "        else:\n",
        "            answer = \"A\"\n",
        "    return answer\n",
        "\n",
        "def evaluate_subject(model, tokenizer, subject):\n",
        "    print(f\"Evaluating: {subject}\")\n",
        "    dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for example in tqdm(dataset, desc=f\"Testing {subject}\"):\n",
        "        question = example[\"question\"]\n",
        "        choices = example[\"choices\"]\n",
        "        correct_answer_idx = example[\"answer\"]\n",
        "        correct_answer = [\"A\", \"B\", \"C\", \"D\"][correct_answer_idx]\n",
        "\n",
        "        prompt = format_mmlu_prompt(question, choices)\n",
        "        predicted_answer = get_model_prediction(model, tokenizer, prompt)\n",
        "\n",
        "        if predicted_answer == correct_answer:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    accuracy = (correct / total * 100) if total > 0 else 0\n",
        "    print(f\"Result: {correct}/{total} = {accuracy:.2f}%\")\n",
        "    return {\"subject\": subject, \"correct\": correct, \"total\": total, \"accuracy\": accuracy}\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"BUSINESS ETHICS Evaluation\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    results = []\n",
        "    for subject in MMLU_SUBJECTS:\n",
        "        result = evaluate_subject(model, tokenizer, subject)\n",
        "        results.append(result)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "    output_file = f\"business_ethics_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump({\"results\": results, \"duration_seconds\": duration}, f, indent=2)\n",
        "\n",
        "    print(f\"\\nCompleted in {duration:.1f} seconds\")\n",
        "    print(f\"Results saved to: {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfsKt2cFiauL",
        "outputId": "fe7f31fc-171d-4508-8ff0-5435b3ae6f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ASTRONOMY Evaluation\n",
            "======================================================================\n",
            "Loading meta-llama/Llama-3.2-1B-Instruct for ASTRONOMY evaluation...\n",
            "Evaluating: astronomy\n",
            "Result: 72/152 = 47.37%\n",
            "\n",
            "Completed in 13.4 seconds\n",
            "Results saved to: astronomy_results_20260222_181400.json\n",
            "======================================================================\n",
            "BUSINESS ETHICS Evaluation\n",
            "======================================================================\n",
            "Loading meta-llama/Llama-3.2-1B-Instruct for BUSINESS ETHICS evaluation...\n",
            "Evaluating: business_ethics\n",
            "Result: 39/100 = 39.00%\n",
            "\n",
            "Completed in 10.6 seconds\n",
            "Results saved to: business_ethics_results_20260222_181430.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rLoading weights:   0%|          | 0/146 [00:00<?, ?it/s]\rLoading weights:   1%|          | 1/146 [00:00<00:00, 11366.68it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   1%|          | 1/146 [00:00<00:00, 5540.69it/s, Materializing param=model.embed_tokens.weight] \rLoading weights:   1%|▏         | 2/146 [00:00<01:11,  2.02it/s, Materializing param=model.embed_tokens.weight]  \rLoading weights:   1%|▏         | 2/146 [00:00<01:11,  2.02it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|▏         | 2/146 [00:00<01:11,  2.02it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   2%|▏         | 3/146 [00:00<01:10,  2.02it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   2%|▏         | 3/146 [00:00<01:10,  2.02it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:01<00:53,  2.65it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:01<00:53,  2.65it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:01<00:53,  2.65it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   3%|▎         | 5/146 [00:01<00:53,  2.65it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   3%|▎         | 5/146 [00:01<00:53,  2.65it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   4%|▍         | 6/146 [00:02<00:48,  2.88it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   4%|▍         | 6/146 [00:02<00:48,  2.88it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   4%|▍         | 6/146 [00:02<00:48,  2.88it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   5%|▍         | 7/146 [00:02<00:48,  2.88it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        \rLoading weights:   5%|▍         | 7/146 [00:02<00:48,  2.88it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:02<00:47,  2.88it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:02<00:47,  2.88it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:02<00:30,  4.49it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:02<00:30,  4.49it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:02<00:30,  4.49it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:02<00:30,  4.49it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:02<00:30,  4.49it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   8%|▊         | 11/146 [00:02<00:30,  4.49it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   8%|▊         | 11/146 [00:02<00:30,  4.49it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   8%|▊         | 12/146 [00:02<00:29,  4.49it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   8%|▊         | 12/146 [00:02<00:29,  4.49it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   9%|▉         | 13/146 [00:02<00:29,  4.49it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   9%|▉         | 13/146 [00:02<00:29,  4.49it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:  10%|▉         | 14/146 [00:02<00:17,  7.74it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:  10%|▉         | 14/146 [00:02<00:17,  7.74it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:  10%|▉         | 14/146 [00:02<00:17,  7.74it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:  10%|█         | 15/146 [00:02<00:16,  7.74it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:  10%|█         | 15/146 [00:02<00:16,  7.74it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:  11%|█         | 16/146 [00:02<00:16,  7.74it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        \rLoading weights:  11%|█         | 16/146 [00:02<00:16,  7.74it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:03<00:15,  8.08it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:03<00:15,  8.08it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:03<00:15,  8.08it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:03<00:15,  8.08it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:03<00:15,  8.08it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:03<00:23,  5.43it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:03<00:23,  5.43it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:03<00:23,  5.43it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  14%|█▎        | 20/146 [00:03<00:22,  5.63it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  14%|█▎        | 20/146 [00:03<00:22,  5.63it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:  14%|█▎        | 20/146 [00:03<00:22,  5.63it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:  14%|█▍        | 21/146 [00:03<00:22,  5.63it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:  14%|█▍        | 21/146 [00:03<00:22,  5.63it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:04<00:22,  5.63it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:04<00:22,  5.63it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  16%|█▌        | 23/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  16%|█▌        | 23/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  16%|█▋        | 24/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  16%|█▋        | 24/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  17%|█▋        | 25/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        \rLoading weights:  17%|█▋        | 25/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  18%|█▊        | 26/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  18%|█▊        | 26/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:04<00:21,  5.63it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  19%|█▉        | 28/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  19%|█▉        | 28/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  20%|█▉        | 29/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  20%|█▉        | 29/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  21%|██        | 30/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  21%|██        | 30/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  21%|██        | 31/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  21%|██        | 31/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  22%|██▏       | 32/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  22%|██▏       | 32/146 [00:04<00:20,  5.63it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  23%|██▎       | 33/146 [00:04<00:06, 18.34it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  23%|██▎       | 33/146 [00:04<00:06, 18.34it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 33/146 [00:04<00:06, 18.34it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 34/146 [00:04<00:06, 18.34it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        \rLoading weights:  23%|██▎       | 34/146 [00:04<00:06, 18.34it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:04<00:06, 18.34it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:04<00:06, 18.34it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  25%|██▍       | 36/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▍       | 36/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  26%|██▌       | 38/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  26%|██▌       | 38/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  27%|██▋       | 39/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  27%|██▋       | 39/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  27%|██▋       | 40/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  27%|██▋       | 40/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  28%|██▊       | 41/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  28%|██▊       | 41/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  29%|██▉       | 42/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  29%|██▉       | 42/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  29%|██▉       | 43/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        \rLoading weights:  29%|██▉       | 43/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  30%|███       | 44/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  30%|███       | 44/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  31%|███       | 45/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  31%|███       | 45/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  32%|███▏      | 46/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  32%|███▏      | 46/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  32%|███▏      | 47/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  32%|███▏      | 47/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  33%|███▎      | 48/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  33%|███▎      | 48/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  34%|███▎      | 49/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  34%|███▎      | 49/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  34%|███▍      | 50/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  34%|███▍      | 50/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  35%|███▍      | 51/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  35%|███▍      | 51/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  36%|███▌      | 52/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        \rLoading weights:  36%|███▌      | 52/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  36%|███▋      | 53/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  36%|███▋      | 53/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  37%|███▋      | 54/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  37%|███▋      | 54/146 [00:04<00:05, 18.34it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  38%|███▊      | 55/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  38%|███▊      | 55/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  38%|███▊      | 56/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  38%|███▊      | 56/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  39%|███▉      | 57/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  39%|███▉      | 57/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  40%|███▉      | 58/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  40%|███▉      | 58/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  40%|████      | 59/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  40%|████      | 59/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  41%|████      | 60/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  41%|████      | 60/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  42%|████▏     | 61/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        \rLoading weights:  42%|████▏     | 61/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  42%|████▏     | 62/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  42%|████▏     | 62/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  43%|████▎     | 63/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  43%|████▎     | 63/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  44%|████▍     | 64/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  44%|████▍     | 64/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  45%|████▍     | 65/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  45%|████▍     | 65/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  45%|████▌     | 66/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  45%|████▌     | 66/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  46%|████▌     | 67/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  46%|████▌     | 67/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  47%|████▋     | 68/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  47%|████▋     | 68/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  47%|████▋     | 69/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  47%|████▋     | 69/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  48%|████▊     | 70/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        \rLoading weights:  48%|████▊     | 70/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  49%|████▊     | 71/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  49%|████▊     | 71/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  49%|████▉     | 72/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  49%|████▉     | 72/146 [00:04<00:04, 18.34it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  50%|█████     | 73/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  50%|█████     | 73/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  51%|█████     | 74/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  51%|█████     | 74/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  51%|█████▏    | 75/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  51%|█████▏    | 75/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  52%|█████▏    | 76/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  52%|█████▏    | 76/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  53%|█████▎    | 77/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  53%|█████▎    | 77/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  53%|█████▎    | 78/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  53%|█████▎    | 78/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  54%|█████▍    | 79/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        \rLoading weights:  54%|█████▍    | 79/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  55%|█████▍    | 80/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  55%|█████▍    | 80/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  55%|█████▌    | 81/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  55%|█████▌    | 81/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  56%|█████▌    | 82/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  56%|█████▌    | 82/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  57%|█████▋    | 83/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  57%|█████▋    | 83/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  58%|█████▊    | 84/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  58%|█████▊    | 84/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  58%|█████▊    | 85/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  58%|█████▊    | 85/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  59%|█████▉    | 86/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  59%|█████▉    | 86/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  60%|█████▉    | 87/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  60%|█████▉    | 87/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  60%|██████    | 88/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        \rLoading weights:  60%|██████    | 88/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  61%|██████    | 89/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  61%|██████    | 89/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  62%|██████▏   | 90/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  62%|██████▏   | 90/146 [00:04<00:03, 18.34it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  62%|██████▏   | 91/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  62%|██████▏   | 91/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  63%|██████▎   | 92/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  63%|██████▎   | 92/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  64%|██████▎   | 93/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  64%|██████▎   | 93/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  64%|██████▍   | 94/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  64%|██████▍   | 94/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  65%|██████▌   | 95/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  65%|██████▌   | 95/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  66%|██████▌   | 96/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  66%|██████▌   | 96/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  66%|██████▋   | 97/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        \rLoading weights:  66%|██████▋   | 97/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  67%|██████▋   | 98/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  67%|██████▋   | 98/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  68%|██████▊   | 99/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  68%|██████▊   | 99/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  68%|██████▊   | 100/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  68%|██████▊   | 100/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  69%|██████▉   | 101/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  69%|██████▉   | 101/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  70%|██████▉   | 102/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  70%|██████▉   | 102/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  71%|███████   | 104/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  71%|███████   | 104/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  72%|███████▏  | 105/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  72%|███████▏  | 105/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  73%|███████▎  | 106/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        \rLoading weights:  73%|███████▎  | 106/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  73%|███████▎  | 107/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  73%|███████▎  | 107/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  74%|███████▍  | 108/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  74%|███████▍  | 108/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  75%|███████▍  | 109/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  75%|███████▍  | 109/146 [00:04<00:02, 18.34it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  75%|███████▌  | 110/146 [00:04<00:01, 18.34it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  75%|███████▌  | 110/146 [00:04<00:01, 18.34it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  76%|███████▌  | 111/146 [00:04<00:01, 18.34it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  76%|███████▌  | 111/146 [00:04<00:01, 18.34it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  77%|███████▋  | 112/146 [00:04<00:01, 18.34it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  77%|███████▋  | 112/146 [00:04<00:01, 18.34it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  77%|███████▋  | 113/146 [00:04<00:01, 18.34it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  77%|███████▋  | 113/146 [00:04<00:01, 18.34it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  78%|███████▊  | 114/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  78%|███████▊  | 114/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  78%|███████▊  | 114/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  79%|███████▉  | 115/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        \rLoading weights:  79%|███████▉  | 115/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  79%|███████▉  | 116/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  79%|███████▉  | 116/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  81%|████████  | 118/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  81%|████████  | 118/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  82%|████████▏ | 119/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  82%|████████▏ | 119/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  82%|████████▏ | 120/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  82%|████████▏ | 120/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  83%|████████▎ | 121/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  83%|████████▎ | 121/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  84%|████████▎ | 122/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  84%|████████▎ | 122/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  84%|████████▍ | 123/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  84%|████████▍ | 123/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  85%|████████▍ | 124/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        \rLoading weights:  85%|████████▍ | 124/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  86%|████████▌ | 125/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  86%|████████▌ | 125/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  86%|████████▋ | 126/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  86%|████████▋ | 126/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  87%|████████▋ | 127/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  87%|████████▋ | 127/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  88%|████████▊ | 128/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  88%|████████▊ | 128/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  88%|████████▊ | 129/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  88%|████████▊ | 129/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  89%|████████▉ | 130/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  89%|████████▉ | 130/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  90%|████████▉ | 131/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  90%|████████▉ | 131/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  90%|█████████ | 132/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  90%|█████████ | 132/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  91%|█████████ | 133/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        \rLoading weights:  91%|█████████ | 133/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  92%|█████████▏| 134/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  92%|█████████▏| 134/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  92%|█████████▏| 135/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  92%|█████████▏| 135/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  93%|█████████▎| 136/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  93%|█████████▎| 136/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  94%|█████████▍| 137/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  94%|█████████▍| 137/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  95%|█████████▍| 138/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  95%|█████████▍| 138/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  95%|█████████▌| 139/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  95%|█████████▌| 139/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  96%|█████████▌| 140/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  96%|█████████▌| 140/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  97%|█████████▋| 141/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  97%|█████████▋| 141/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  97%|█████████▋| 142/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        \rLoading weights:  97%|█████████▋| 142/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  98%|█████████▊| 143/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  98%|█████████▊| 143/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  99%|█████████▊| 144/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  99%|█████████▊| 144/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  99%|█████████▉| 145/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  99%|█████████▉| 145/146 [00:04<00:00, 119.14it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights: 100%|██████████| 146/146 [00:04<00:00, 119.14it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|██████████| 146/146 [00:04<00:00, 119.14it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|██████████| 146/146 [00:04<00:00, 34.59it/s, Materializing param=model.norm.weight] \n",
            "\rTesting astronomy:   0%|          | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "\rTesting astronomy:   1%|          | 1/152 [00:00<01:15,  1.99it/s]\rTesting astronomy:   1%|▏         | 2/152 [00:00<00:39,  3.76it/s]\rTesting astronomy:   3%|▎         | 4/152 [00:00<00:21,  6.91it/s]\rTesting astronomy:   4%|▍         | 6/152 [00:00<00:15,  9.34it/s]\rTesting astronomy:   5%|▌         | 8/152 [00:01<00:13, 10.95it/s]\rTesting astronomy:   7%|▋         | 10/152 [00:01<00:12, 11.69it/s]\rTesting astronomy:   8%|▊         | 12/152 [00:01<00:11, 12.29it/s]\rTesting astronomy:   9%|▉         | 14/152 [00:01<00:10, 13.09it/s]\rTesting astronomy:  11%|█         | 16/152 [00:01<00:09, 13.76it/s]\rTesting astronomy:  12%|█▏        | 18/152 [00:01<00:09, 13.92it/s]\rTesting astronomy:  13%|█▎        | 20/152 [00:01<00:09, 14.34it/s]\rTesting astronomy:  14%|█▍        | 22/152 [00:01<00:08, 14.63it/s]\rTesting astronomy:  16%|█▌        | 24/152 [00:02<00:08, 14.80it/s]\rTesting astronomy:  17%|█▋        | 26/152 [00:02<00:08, 14.97it/s]\rTesting astronomy:  18%|█▊        | 28/152 [00:02<00:08, 14.72it/s]\rTesting astronomy:  20%|█▉        | 30/152 [00:02<00:08, 14.86it/s]\rTesting astronomy:  21%|██        | 32/152 [00:02<00:08, 14.57it/s]\rTesting astronomy:  22%|██▏       | 34/152 [00:02<00:08, 14.70it/s]\rTesting astronomy:  24%|██▎       | 36/152 [00:02<00:07, 14.76it/s]\rTesting astronomy:  25%|██▌       | 38/152 [00:03<00:08, 13.01it/s]\rTesting astronomy:  26%|██▋       | 40/152 [00:03<00:08, 12.59it/s]\rTesting astronomy:  28%|██▊       | 42/152 [00:03<00:09, 11.76it/s]\rTesting astronomy:  29%|██▉       | 44/152 [00:03<00:09, 11.79it/s]\rTesting astronomy:  30%|███       | 46/152 [00:03<00:09, 11.71it/s]\rTesting astronomy:  32%|███▏      | 48/152 [00:03<00:08, 11.79it/s]\rTesting astronomy:  33%|███▎      | 50/152 [00:04<00:08, 11.91it/s]\rTesting astronomy:  34%|███▍      | 52/152 [00:04<00:08, 12.15it/s]\rTesting astronomy:  36%|███▌      | 54/152 [00:04<00:08, 11.80it/s]\rTesting astronomy:  37%|███▋      | 56/152 [00:04<00:08, 11.49it/s]\rTesting astronomy:  38%|███▊      | 58/152 [00:04<00:08, 10.89it/s]\rTesting astronomy:  39%|███▉      | 60/152 [00:05<00:08, 10.95it/s]\rTesting astronomy:  41%|████      | 62/152 [00:05<00:08, 10.87it/s]\rTesting astronomy:  42%|████▏     | 64/152 [00:05<00:08, 10.72it/s]\rTesting astronomy:  43%|████▎     | 66/152 [00:05<00:07, 11.17it/s]\rTesting astronomy:  45%|████▍     | 68/152 [00:05<00:06, 12.08it/s]\rTesting astronomy:  46%|████▌     | 70/152 [00:05<00:06, 12.83it/s]\rTesting astronomy:  47%|████▋     | 72/152 [00:06<00:05, 13.37it/s]\rTesting astronomy:  49%|████▊     | 74/152 [00:06<00:05, 13.91it/s]\rTesting astronomy:  50%|█████     | 76/152 [00:06<00:05, 14.31it/s]\rTesting astronomy:  51%|█████▏    | 78/152 [00:06<00:05, 14.48it/s]\rTesting astronomy:  53%|█████▎    | 80/152 [00:06<00:04, 14.61it/s]\rTesting astronomy:  54%|█████▍    | 82/152 [00:06<00:04, 14.44it/s]\rTesting astronomy:  55%|█████▌    | 84/152 [00:06<00:04, 14.45it/s]\rTesting astronomy:  57%|█████▋    | 86/152 [00:06<00:04, 14.65it/s]\rTesting astronomy:  58%|█████▊    | 88/152 [00:07<00:04, 14.74it/s]\rTesting astronomy:  59%|█████▉    | 90/152 [00:07<00:04, 14.70it/s]\rTesting astronomy:  61%|██████    | 92/152 [00:07<00:04, 14.78it/s]\rTesting astronomy:  62%|██████▏   | 94/152 [00:07<00:03, 14.71it/s]\rTesting astronomy:  63%|██████▎   | 96/152 [00:07<00:03, 14.48it/s]\rTesting astronomy:  64%|██████▍   | 98/152 [00:07<00:03, 14.61it/s]\rTesting astronomy:  66%|██████▌   | 100/152 [00:07<00:03, 14.82it/s]\rTesting astronomy:  67%|██████▋   | 102/152 [00:08<00:03, 14.85it/s]\rTesting astronomy:  68%|██████▊   | 104/152 [00:08<00:03, 14.92it/s]\rTesting astronomy:  70%|██████▉   | 106/152 [00:08<00:03, 14.95it/s]\rTesting astronomy:  71%|███████   | 108/152 [00:08<00:02, 14.91it/s]\rTesting astronomy:  72%|███████▏  | 110/152 [00:08<00:02, 14.53it/s]\rTesting astronomy:  74%|███████▎  | 112/152 [00:08<00:02, 14.25it/s]\rTesting astronomy:  75%|███████▌  | 114/152 [00:08<00:02, 14.44it/s]\rTesting astronomy:  76%|███████▋  | 116/152 [00:08<00:02, 14.57it/s]\rTesting astronomy:  78%|███████▊  | 118/152 [00:09<00:02, 14.05it/s]\rTesting astronomy:  79%|███████▉  | 120/152 [00:09<00:02, 14.19it/s]\rTesting astronomy:  80%|████████  | 122/152 [00:09<00:02, 14.34it/s]\rTesting astronomy:  82%|████████▏ | 124/152 [00:09<00:01, 14.70it/s]\rTesting astronomy:  83%|████████▎ | 126/152 [00:09<00:01, 14.50it/s]\rTesting astronomy:  84%|████████▍ | 128/152 [00:09<00:01, 14.71it/s]\rTesting astronomy:  86%|████████▌ | 130/152 [00:09<00:01, 14.68it/s]\rTesting astronomy:  87%|████████▋ | 132/152 [00:10<00:01, 14.53it/s]\rTesting astronomy:  88%|████████▊ | 134/152 [00:10<00:01, 14.60it/s]\rTesting astronomy:  89%|████████▉ | 136/152 [00:10<00:01, 14.79it/s]\rTesting astronomy:  91%|█████████ | 138/152 [00:10<00:00, 14.91it/s]\rTesting astronomy:  92%|█████████▏| 140/152 [00:10<00:00, 15.10it/s]\rTesting astronomy:  93%|█████████▎| 142/152 [00:10<00:00, 14.71it/s]\rTesting astronomy:  95%|█████████▍| 144/152 [00:10<00:00, 14.84it/s]\rTesting astronomy:  96%|█████████▌| 146/152 [00:11<00:00, 15.00it/s]\rTesting astronomy:  97%|█████████▋| 148/152 [00:11<00:00, 15.07it/s]\rTesting astronomy:  99%|█████████▊| 150/152 [00:11<00:00, 15.13it/s]\rTesting astronomy: 100%|██████████| 152/152 [00:11<00:00, 15.15it/s]\rTesting astronomy: 100%|██████████| 152/152 [00:11<00:00, 13.30it/s]\n",
            "\rLoading weights:   0%|          | 0/146 [00:00<?, ?it/s]\rLoading weights:   1%|          | 1/146 [00:00<00:00, 10106.76it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   1%|          | 1/146 [00:00<00:00, 4529.49it/s, Materializing param=model.embed_tokens.weight] \rLoading weights:   1%|▏         | 2/146 [00:00<00:10, 13.46it/s, Materializing param=model.embed_tokens.weight]  \rLoading weights:   1%|▏         | 2/146 [00:00<00:10, 13.46it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|▏         | 2/146 [00:00<00:10, 13.46it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   2%|▏         | 3/146 [00:00<00:10, 13.46it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   2%|▏         | 3/146 [00:00<00:10, 13.46it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   3%|▎         | 5/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   3%|▎         | 5/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   4%|▍         | 6/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   4%|▍         | 6/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   5%|▍         | 7/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        \rLoading weights:   5%|▍         | 7/146 [00:00<00:10, 13.31it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:00<00:06, 20.83it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:00<00:06, 20.83it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:00<00:06, 20.83it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:00<00:06, 20.83it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:00<00:06, 20.83it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:00<00:06, 20.83it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:00<00:06, 20.83it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   8%|▊         | 11/146 [00:00<00:06, 22.43it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   8%|▊         | 11/146 [00:00<00:06, 22.43it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   8%|▊         | 11/146 [00:00<00:06, 22.43it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   8%|▊         | 12/146 [00:00<00:05, 22.43it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   8%|▊         | 12/146 [00:00<00:05, 22.43it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   9%|▉         | 13/146 [00:00<00:05, 22.43it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   9%|▉         | 13/146 [00:00<00:05, 22.43it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:  10%|▉         | 14/146 [00:00<00:05, 22.43it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:  10%|▉         | 14/146 [00:00<00:05, 22.43it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:  10%|█         | 15/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:  10%|█         | 15/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:  10%|█         | 15/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:  11%|█         | 16/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        \rLoading weights:  11%|█         | 16/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  14%|█▎        | 20/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:  14%|█▎        | 20/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:  14%|█▍        | 21/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:  14%|█▍        | 21/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  16%|█▌        | 23/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  16%|█▌        | 23/146 [00:00<00:05, 24.54it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  16%|█▋        | 24/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  16%|█▋        | 24/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  17%|█▋        | 25/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        \rLoading weights:  17%|█▋        | 25/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  18%|█▊        | 26/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  18%|█▊        | 26/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  19%|█▉        | 28/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  19%|█▉        | 28/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  20%|█▉        | 29/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  20%|█▉        | 29/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  21%|██        | 30/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  21%|██        | 30/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  21%|██        | 31/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  21%|██        | 31/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  22%|██▏       | 32/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  22%|██▏       | 32/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  23%|██▎       | 33/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 33/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 34/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        \rLoading weights:  23%|██▎       | 34/146 [00:00<00:04, 24.54it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  25%|██▍       | 36/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▍       | 36/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  26%|██▌       | 38/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  26%|██▌       | 38/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  27%|██▋       | 39/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  27%|██▋       | 39/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  27%|██▋       | 40/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  27%|██▋       | 40/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  28%|██▊       | 41/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  28%|██▊       | 41/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  29%|██▉       | 42/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  29%|██▉       | 42/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  29%|██▉       | 43/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        \rLoading weights:  29%|██▉       | 43/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  30%|███       | 44/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  30%|███       | 44/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  31%|███       | 45/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  31%|███       | 45/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  32%|███▏      | 46/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  32%|███▏      | 46/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  32%|███▏      | 47/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  32%|███▏      | 47/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  33%|███▎      | 48/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  33%|███▎      | 48/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  34%|███▎      | 49/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  34%|███▎      | 49/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  34%|███▍      | 50/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  34%|███▍      | 50/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  35%|███▍      | 51/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  35%|███▍      | 51/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  36%|███▌      | 52/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        \rLoading weights:  36%|███▌      | 52/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  36%|███▋      | 53/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  36%|███▋      | 53/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  37%|███▋      | 54/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  37%|███▋      | 54/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  38%|███▊      | 55/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  38%|███▊      | 55/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  38%|███▊      | 56/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  38%|███▊      | 56/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  39%|███▉      | 57/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  39%|███▉      | 57/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  40%|███▉      | 58/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  40%|███▉      | 58/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  40%|████      | 59/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  40%|████      | 59/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  41%|████      | 60/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  41%|████      | 60/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  42%|████▏     | 61/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        \rLoading weights:  42%|████▏     | 61/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  42%|████▏     | 62/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  42%|████▏     | 62/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  43%|████▎     | 63/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  43%|████▎     | 63/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  44%|████▍     | 64/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  44%|████▍     | 64/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  45%|████▍     | 65/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  45%|████▍     | 65/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  45%|████▌     | 66/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  45%|████▌     | 66/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  46%|████▌     | 67/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  46%|████▌     | 67/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  47%|████▋     | 68/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  47%|████▋     | 68/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  47%|████▋     | 69/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  47%|████▋     | 69/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  48%|████▊     | 70/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        \rLoading weights:  48%|████▊     | 70/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  49%|████▊     | 71/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  49%|████▊     | 71/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  49%|████▉     | 72/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  49%|████▉     | 72/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  50%|█████     | 73/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  50%|█████     | 73/146 [00:00<00:01, 72.84it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  51%|█████     | 74/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  51%|█████     | 74/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  51%|█████▏    | 75/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  51%|█████▏    | 75/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  52%|█████▏    | 76/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  52%|█████▏    | 76/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  53%|█████▎    | 77/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  53%|█████▎    | 77/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  53%|█████▎    | 78/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  53%|█████▎    | 78/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  54%|█████▍    | 79/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        \rLoading weights:  54%|█████▍    | 79/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  55%|█████▍    | 80/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  55%|█████▍    | 80/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  55%|█████▌    | 81/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  55%|█████▌    | 81/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  56%|█████▌    | 82/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  56%|█████▌    | 82/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  57%|█████▋    | 83/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  57%|█████▋    | 83/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  58%|█████▊    | 84/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  58%|█████▊    | 84/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  58%|█████▊    | 85/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  58%|█████▊    | 85/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  59%|█████▉    | 86/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  59%|█████▉    | 86/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  60%|█████▉    | 87/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  60%|█████▉    | 87/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  60%|██████    | 88/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        \rLoading weights:  60%|██████    | 88/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  61%|██████    | 89/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  61%|██████    | 89/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  62%|██████▏   | 90/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  62%|██████▏   | 90/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  62%|██████▏   | 91/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  62%|██████▏   | 91/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  63%|██████▎   | 92/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  63%|██████▎   | 92/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  64%|██████▎   | 93/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  64%|██████▎   | 93/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  64%|██████▍   | 94/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  64%|██████▍   | 94/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  65%|██████▌   | 95/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  65%|██████▌   | 95/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  66%|██████▌   | 96/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  66%|██████▌   | 96/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  66%|██████▋   | 97/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        \rLoading weights:  66%|██████▋   | 97/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  67%|██████▋   | 98/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  67%|██████▋   | 98/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  68%|██████▊   | 99/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  68%|██████▊   | 99/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  68%|██████▊   | 100/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  68%|██████▊   | 100/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  69%|██████▉   | 101/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  69%|██████▉   | 101/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  70%|██████▉   | 102/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  70%|██████▉   | 102/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  71%|███████   | 104/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  71%|███████   | 104/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  72%|███████▏  | 105/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  72%|███████▏  | 105/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  73%|███████▎  | 106/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        \rLoading weights:  73%|███████▎  | 106/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  73%|███████▎  | 107/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  73%|███████▎  | 107/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  74%|███████▍  | 108/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  74%|███████▍  | 108/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  75%|███████▍  | 109/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  75%|███████▍  | 109/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  75%|███████▌  | 110/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  75%|███████▌  | 110/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  76%|███████▌  | 111/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  76%|███████▌  | 111/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  77%|███████▋  | 112/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  77%|███████▋  | 112/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  77%|███████▋  | 113/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  77%|███████▋  | 113/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  78%|███████▊  | 114/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  78%|███████▊  | 114/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  79%|███████▉  | 115/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        \rLoading weights:  79%|███████▉  | 115/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  79%|███████▉  | 116/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  79%|███████▉  | 116/146 [00:00<00:00, 72.84it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  81%|████████  | 118/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  81%|████████  | 118/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  82%|████████▏ | 119/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  82%|████████▏ | 119/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  82%|████████▏ | 120/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  82%|████████▏ | 120/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  83%|████████▎ | 121/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  83%|████████▎ | 121/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  84%|████████▎ | 122/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  84%|████████▎ | 122/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  84%|████████▍ | 123/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  84%|████████▍ | 123/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  85%|████████▍ | 124/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        \rLoading weights:  85%|████████▍ | 124/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  86%|████████▌ | 125/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  86%|████████▌ | 125/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  86%|████████▋ | 126/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  86%|████████▋ | 126/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  87%|████████▋ | 127/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  87%|████████▋ | 127/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  88%|████████▊ | 128/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  88%|████████▊ | 128/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  88%|████████▊ | 129/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  88%|████████▊ | 129/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  89%|████████▉ | 130/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  89%|████████▉ | 130/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  90%|████████▉ | 131/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  90%|████████▉ | 131/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  90%|█████████ | 132/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  90%|█████████ | 132/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  91%|█████████ | 133/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        \rLoading weights:  91%|█████████ | 133/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  92%|█████████▏| 134/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  92%|█████████▏| 134/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  92%|█████████▏| 135/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  92%|█████████▏| 135/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  93%|█████████▎| 136/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  93%|█████████▎| 136/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  94%|█████████▍| 137/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  94%|█████████▍| 137/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  95%|█████████▍| 138/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  95%|█████████▍| 138/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  95%|█████████▌| 139/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  95%|█████████▌| 139/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  96%|█████████▌| 140/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  96%|█████████▌| 140/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  97%|█████████▋| 141/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  97%|█████████▋| 141/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  97%|█████████▋| 142/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        \rLoading weights:  97%|█████████▋| 142/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  98%|█████████▊| 143/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  98%|█████████▊| 143/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  99%|█████████▊| 144/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  99%|█████████▊| 144/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  99%|█████████▉| 145/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  99%|█████████▉| 145/146 [00:00<00:00, 283.31it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights: 100%|██████████| 146/146 [00:00<00:00, 283.31it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|██████████| 146/146 [00:00<00:00, 283.31it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|██████████| 146/146 [00:00<00:00, 157.88it/s, Materializing param=model.norm.weight]\n",
            "\rGenerating test split:   0%|          | 0/100 [00:00<?, ? examples/s]\rGenerating test split: 100%|██████████| 100/100 [00:00<00:00, 22461.86 examples/s]\n",
            "\rGenerating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]\rGenerating validation split: 100%|██████████| 11/11 [00:00<00:00, 4631.33 examples/s]\n",
            "\rGenerating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]\rGenerating dev split: 100%|██████████| 5/5 [00:00<00:00, 2356.62 examples/s]\n",
            "\rTesting business_ethics:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "\rTesting business_ethics:   1%|          | 1/100 [00:00<00:46,  2.11it/s]\rTesting business_ethics:   3%|▎         | 3/100 [00:00<00:17,  5.50it/s]\rTesting business_ethics:   5%|▌         | 5/100 [00:00<00:11,  8.08it/s]\rTesting business_ethics:   7%|▋         | 7/100 [00:00<00:09,  9.99it/s]\rTesting business_ethics:   9%|▉         | 9/100 [00:01<00:07, 11.46it/s]\rTesting business_ethics:  11%|█         | 11/100 [00:01<00:07, 11.92it/s]\rTesting business_ethics:  13%|█▎        | 13/100 [00:01<00:06, 12.65it/s]\rTesting business_ethics:  15%|█▌        | 15/100 [00:01<00:06, 13.41it/s]\rTesting business_ethics:  17%|█▋        | 17/100 [00:01<00:05, 13.85it/s]\rTesting business_ethics:  19%|█▉        | 19/100 [00:01<00:05, 14.08it/s]\rTesting business_ethics:  21%|██        | 21/100 [00:01<00:05, 14.25it/s]\rTesting business_ethics:  23%|██▎       | 23/100 [00:02<00:05, 14.45it/s]\rTesting business_ethics:  25%|██▌       | 25/100 [00:02<00:05, 14.36it/s]\rTesting business_ethics:  27%|██▋       | 27/100 [00:02<00:05, 14.54it/s]\rTesting business_ethics:  29%|██▉       | 29/100 [00:02<00:04, 14.58it/s]\rTesting business_ethics:  31%|███       | 31/100 [00:02<00:04, 14.57it/s]\rTesting business_ethics:  33%|███▎      | 33/100 [00:02<00:04, 14.51it/s]\rTesting business_ethics:  35%|███▌      | 35/100 [00:02<00:04, 14.39it/s]\rTesting business_ethics:  37%|███▋      | 37/100 [00:02<00:04, 14.46it/s]\rTesting business_ethics:  39%|███▉      | 39/100 [00:03<00:04, 14.40it/s]\rTesting business_ethics:  41%|████      | 41/100 [00:03<00:04, 14.37it/s]\rTesting business_ethics:  43%|████▎     | 43/100 [00:03<00:03, 14.46it/s]\rTesting business_ethics:  45%|████▌     | 45/100 [00:03<00:03, 14.48it/s]\rTesting business_ethics:  47%|████▋     | 47/100 [00:03<00:03, 14.47it/s]\rTesting business_ethics:  49%|████▉     | 49/100 [00:03<00:03, 14.51it/s]\rTesting business_ethics:  51%|█████     | 51/100 [00:03<00:03, 14.49it/s]\rTesting business_ethics:  53%|█████▎    | 53/100 [00:04<00:03, 14.38it/s]\rTesting business_ethics:  55%|█████▌    | 55/100 [00:04<00:03, 14.06it/s]\rTesting business_ethics:  57%|█████▋    | 57/100 [00:04<00:03, 14.22it/s]\rTesting business_ethics:  59%|█████▉    | 59/100 [00:04<00:02, 13.92it/s]\rTesting business_ethics:  61%|██████    | 61/100 [00:04<00:02, 14.10it/s]\rTesting business_ethics:  63%|██████▎   | 63/100 [00:04<00:02, 14.29it/s]\rTesting business_ethics:  65%|██████▌   | 65/100 [00:04<00:02, 14.33it/s]\rTesting business_ethics:  67%|██████▋   | 67/100 [00:05<00:02, 14.34it/s]\rTesting business_ethics:  69%|██████▉   | 69/100 [00:05<00:02, 14.28it/s]\rTesting business_ethics:  71%|███████   | 71/100 [00:05<00:02, 14.11it/s]\rTesting business_ethics:  73%|███████▎  | 73/100 [00:05<00:01, 14.27it/s]\rTesting business_ethics:  75%|███████▌  | 75/100 [00:05<00:01, 14.42it/s]\rTesting business_ethics:  77%|███████▋  | 77/100 [00:05<00:01, 14.35it/s]\rTesting business_ethics:  79%|███████▉  | 79/100 [00:05<00:01, 14.50it/s]\rTesting business_ethics:  81%|████████  | 81/100 [00:06<00:01, 14.32it/s]\rTesting business_ethics:  83%|████████▎ | 83/100 [00:06<00:01, 14.02it/s]\rTesting business_ethics:  85%|████████▌ | 85/100 [00:06<00:01, 12.43it/s]\rTesting business_ethics:  87%|████████▋ | 87/100 [00:06<00:01, 12.29it/s]\rTesting business_ethics:  89%|████████▉ | 89/100 [00:06<00:00, 12.38it/s]\rTesting business_ethics:  91%|█████████ | 91/100 [00:06<00:00, 12.44it/s]\rTesting business_ethics:  93%|█████████▎| 93/100 [00:07<00:00, 12.26it/s]\rTesting business_ethics:  95%|█████████▌| 95/100 [00:07<00:00, 12.40it/s]\rTesting business_ethics:  97%|█████████▋| 97/100 [00:07<00:00, 12.34it/s]\rTesting business_ethics:  99%|█████████▉| 99/100 [00:07<00:00, 12.22it/s]\rTesting business_ethics: 100%|██████████| 100/100 [00:07<00:00, 13.11it/s]\n",
            "\n",
            "real\t1m9.236s\n",
            "user\t0m48.995s\n",
            "sys\t0m5.109s\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "time { python llama_mmlu_astronomy.py ; python llama_mmlu_business.py ; }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or5QIsjKkT98"
      },
      "source": [
        "Without Ollama Server - Parallel Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ftOH0AikXla",
        "outputId": "4e8c9a4c-901f-47d9-f5ec-8ea93c7b0ecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "BUSINESS ETHICS Evaluation\n",
            "======================================================================\n",
            "Loading meta-llama/Llama-3.2-1B-Instruct for BUSINESS ETHICS evaluation...\n",
            "======================================================================\n",
            "ASTRONOMY Evaluation\n",
            "======================================================================\n",
            "Loading meta-llama/Llama-3.2-1B-Instruct for ASTRONOMY evaluation...\n",
            "Evaluating: business_ethics\n",
            "Evaluating: astronomy\n",
            "Result: 39/100 = 39.00%\n",
            "\n",
            "Completed in 17.9 seconds\n",
            "Results saved to: business_ethics_results_20260222_181918.json\n",
            "Result: 72/152 = 47.37%\n",
            "\n",
            "Completed in 23.3 seconds\n",
            "Results saved to: astronomy_results_20260222_181923.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rLoading weights:   0%|          | 0/146 [00:00<?, ?it/s]\rLoading weights:   1%|          | 1/146 [00:00<00:00, 9709.04it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   1%|          | 1/146 [00:00<00:00, 4447.83it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   0%|          | 0/146 [00:00<?, ?it/s]\rLoading weights:   1%|          | 1/146 [00:00<00:00, 9157.87it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   1%|          | 1/146 [00:00<00:00, 3847.99it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   1%|▏         | 2/146 [00:00<00:13, 10.76it/s, Materializing param=model.embed_tokens.weight]  \rLoading weights:   1%|▏         | 2/146 [00:00<00:13, 10.76it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|▏         | 2/146 [00:00<00:13, 10.76it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   2%|▏         | 3/146 [00:00<00:13, 10.76it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   2%|▏         | 3/146 [00:00<00:13, 10.76it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   1%|▏         | 2/146 [00:00<00:14, 10.08it/s, Materializing param=model.embed_tokens.weight]  \rLoading weights:   1%|▏         | 2/146 [00:00<00:14, 10.08it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|▏         | 2/146 [00:00<00:14, 10.08it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   2%|▏         | 3/146 [00:00<00:14, 10.08it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   2%|▏         | 3/146 [00:00<00:14, 10.08it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:13, 10.85it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:13, 10.85it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:13, 10.85it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   3%|▎         | 5/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   3%|▎         | 5/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   4%|▍         | 6/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   4%|▍         | 6/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   5%|▍         | 7/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        \rLoading weights:   5%|▍         | 7/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   3%|▎         | 4/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   3%|▎         | 5/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   3%|▎         | 5/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   4%|▍         | 6/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   4%|▍         | 6/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   5%|▍         | 7/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        \rLoading weights:   5%|▍         | 7/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:00<00:12, 10.85it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   5%|▌         | 8/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   6%|▌         | 9/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   7%|▋         | 10/146 [00:00<00:15,  9.03it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   8%|▊         | 11/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   8%|▊         | 11/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   8%|▊         | 12/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   8%|▊         | 12/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   8%|▊         | 11/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   8%|▊         | 11/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   8%|▊         | 11/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   8%|▊         | 12/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   8%|▊         | 12/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   9%|▉         | 13/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   9%|▉         | 13/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   9%|▉         | 13/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   9%|▉         | 13/146 [00:00<00:05, 25.05it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:  10%|▉         | 14/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:  10%|▉         | 14/146 [00:00<00:05, 24.59it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:  10%|▉         | 14/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:  10%|▉         | 14/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:  10%|▉         | 14/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:  10%|█         | 15/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:  10%|█         | 15/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:  10%|█         | 15/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:  11%|█         | 16/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        \rLoading weights:  11%|█         | 16/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  10%|█         | 15/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:  10%|█         | 15/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:  11%|█         | 16/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        \rLoading weights:  11%|█         | 16/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  12%|█▏        | 17/146 [00:00<00:04, 27.70it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  12%|█▏        | 18/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:  14%|█▎        | 20/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:  14%|█▎        | 20/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:  14%|█▍        | 21/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:  14%|█▍        | 21/146 [00:00<00:04, 27.49it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  13%|█▎        | 19/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:  14%|█▎        | 20/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:  14%|█▎        | 20/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:  14%|█▍        | 21/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:  14%|█▍        | 21/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  16%|█▌        | 23/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  16%|█▌        | 23/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  15%|█▌        | 22/146 [00:00<00:04, 30.38it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  16%|█▌        | 23/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  16%|█▌        | 23/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  16%|█▌        | 23/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  16%|█▋        | 24/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  16%|█▋        | 24/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  17%|█▋        | 25/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        \rLoading weights:  17%|█▋        | 25/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  18%|█▊        | 26/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  16%|█▋        | 24/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  16%|█▋        | 24/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  18%|█▊        | 26/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  17%|█▋        | 25/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        \rLoading weights:  17%|█▋        | 25/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  18%|█▊        | 26/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  18%|█▊        | 26/146 [00:00<00:03, 36.74it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  19%|█▉        | 28/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  19%|█▉        | 28/146 [00:00<00:03, 35.83it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  18%|█▊        | 27/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  19%|█▉        | 28/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  19%|█▉        | 28/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  20%|█▉        | 29/146 [00:00<00:02, 39.12it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  20%|█▉        | 29/146 [00:00<00:02, 39.12it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  20%|█▉        | 29/146 [00:00<00:02, 39.12it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  21%|██        | 30/146 [00:00<00:02, 39.12it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  21%|██        | 30/146 [00:00<00:02, 39.12it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  20%|█▉        | 29/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  20%|█▉        | 29/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  21%|██        | 30/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  21%|██        | 30/146 [00:00<00:03, 36.40it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  21%|██        | 31/146 [00:00<00:02, 39.12it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  21%|██        | 31/146 [00:00<00:02, 39.12it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  21%|██        | 31/146 [00:01<00:03, 36.40it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  21%|██        | 31/146 [00:01<00:03, 36.40it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  22%|██▏       | 32/146 [00:01<00:02, 39.12it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  22%|██▏       | 32/146 [00:01<00:02, 39.12it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  22%|██▏       | 32/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  22%|██▏       | 32/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  22%|██▏       | 32/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  23%|██▎       | 33/146 [00:01<00:02, 39.12it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 33/146 [00:01<00:02, 39.12it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 34/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 34/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        \rLoading weights:  23%|██▎       | 34/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  23%|██▎       | 33/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 33/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  23%|██▎       | 34/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        \rLoading weights:  23%|██▎       | 34/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  24%|██▍       | 35/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  25%|██▍       | 36/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▍       | 36/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▍       | 36/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▍       | 36/146 [00:01<00:02, 39.43it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  26%|██▌       | 38/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  26%|██▌       | 38/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  27%|██▋       | 39/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  27%|██▋       | 39/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  25%|██▌       | 37/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  27%|██▋       | 40/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  27%|██▋       | 40/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  26%|██▌       | 38/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  26%|██▌       | 38/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  27%|██▋       | 39/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  27%|██▋       | 39/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  28%|██▊       | 41/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  27%|██▋       | 40/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  27%|██▋       | 40/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  28%|██▊       | 41/146 [00:01<00:02, 41.65it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  28%|██▊       | 41/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  28%|██▊       | 41/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  29%|██▉       | 42/146 [00:01<00:02, 51.91it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  29%|██▉       | 42/146 [00:01<00:02, 51.91it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  29%|██▉       | 42/146 [00:01<00:02, 51.91it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  29%|██▉       | 43/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        \rLoading weights:  29%|██▉       | 42/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  29%|██▉       | 42/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  29%|██▉       | 43/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        \rLoading weights:  29%|██▉       | 43/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  29%|██▉       | 43/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  30%|███       | 44/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  30%|███       | 44/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  30%|███       | 44/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  30%|███       | 44/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  31%|███       | 45/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  31%|███       | 45/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  31%|███       | 45/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  31%|███       | 45/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  32%|███▏      | 46/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  32%|███▏      | 46/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  32%|███▏      | 46/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  32%|███▏      | 47/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  32%|███▏      | 47/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  33%|███▎      | 48/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  33%|███▎      | 48/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  32%|███▏      | 46/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  34%|███▎      | 49/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  34%|███▎      | 49/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  32%|███▏      | 47/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  32%|███▏      | 47/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  33%|███▎      | 48/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  33%|███▎      | 48/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  34%|███▍      | 50/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  34%|███▍      | 50/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  34%|███▎      | 49/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  34%|███▎      | 49/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  35%|███▍      | 51/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  35%|███▍      | 51/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  36%|███▌      | 52/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        \rLoading weights:  36%|███▌      | 52/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  36%|███▋      | 53/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  36%|███▋      | 53/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  37%|███▋      | 54/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  37%|███▋      | 54/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  34%|███▍      | 50/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  38%|███▊      | 55/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  38%|███▊      | 55/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  38%|███▊      | 56/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  38%|███▊      | 56/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  39%|███▉      | 57/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  39%|███▉      | 57/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  34%|███▍      | 50/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  40%|███▉      | 58/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  40%|███▉      | 58/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  35%|███▍      | 51/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  35%|███▍      | 51/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  40%|████      | 59/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  40%|████      | 59/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  36%|███▌      | 52/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        \rLoading weights:  36%|███▌      | 52/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  41%|████      | 60/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  41%|████      | 60/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  42%|████▏     | 61/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        \rLoading weights:  42%|████▏     | 61/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  42%|████▏     | 62/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  42%|████▏     | 62/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  36%|███▋      | 53/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  36%|███▋      | 53/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  43%|████▎     | 63/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  43%|████▎     | 63/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  44%|████▍     | 64/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  44%|████▍     | 64/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  37%|███▋      | 54/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  45%|████▍     | 65/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  45%|████▍     | 65/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  45%|████▌     | 66/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  37%|███▋      | 54/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  45%|████▌     | 66/146 [00:01<00:02, 39.65it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  38%|███▊      | 55/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  38%|███▊      | 55/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  46%|████▌     | 67/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  46%|████▌     | 67/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  38%|███▊      | 56/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  38%|███▊      | 56/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  39%|███▉      | 57/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  39%|███▉      | 57/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  47%|████▋     | 68/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  47%|████▋     | 68/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  40%|███▉      | 58/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  40%|███▉      | 58/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  40%|████      | 59/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  47%|████▋     | 69/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  47%|████▋     | 69/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  40%|████      | 59/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  48%|████▊     | 70/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        \rLoading weights:  48%|████▊     | 70/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  41%|████      | 60/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  41%|████      | 60/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  49%|████▊     | 71/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  49%|████▊     | 71/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  42%|████▏     | 61/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        \rLoading weights:  42%|████▏     | 61/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  49%|████▉     | 72/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  49%|████▉     | 72/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  42%|████▏     | 62/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  42%|████▏     | 62/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  50%|█████     | 73/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  50%|█████     | 73/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  43%|████▎     | 63/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  43%|████▎     | 63/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  51%|█████     | 74/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  51%|█████     | 74/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  51%|█████▏    | 75/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  51%|█████▏    | 75/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  44%|████▍     | 64/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  44%|████▍     | 64/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  52%|█████▏    | 76/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  52%|█████▏    | 76/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  45%|████▍     | 65/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  45%|████▍     | 65/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  45%|████▌     | 66/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  45%|████▌     | 66/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  53%|█████▎    | 77/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  53%|█████▎    | 77/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  46%|████▌     | 67/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  46%|████▌     | 67/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  53%|█████▎    | 78/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  53%|█████▎    | 78/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  54%|█████▍    | 79/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        \rLoading weights:  54%|█████▍    | 79/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  47%|████▋     | 68/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  55%|█████▍    | 80/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  55%|█████▍    | 80/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  47%|████▋     | 68/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  55%|█████▌    | 81/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  55%|█████▌    | 81/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  47%|████▋     | 69/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  47%|████▋     | 69/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  48%|████▊     | 70/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        \rLoading weights:  48%|████▊     | 70/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  56%|█████▌    | 82/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  56%|█████▌    | 82/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  49%|████▊     | 71/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  49%|████▊     | 71/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  57%|█████▋    | 83/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  57%|█████▋    | 83/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  58%|█████▊    | 84/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  58%|█████▊    | 84/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  49%|████▉     | 72/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  49%|████▉     | 72/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  58%|█████▊    | 85/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  58%|█████▊    | 85/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  50%|█████     | 73/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  50%|█████     | 73/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  59%|█████▉    | 86/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  51%|█████     | 74/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  59%|█████▉    | 86/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  51%|█████     | 74/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  51%|█████▏    | 75/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  51%|█████▏    | 75/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  60%|█████▉    | 87/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  60%|█████▉    | 87/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  60%|██████    | 88/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        \rLoading weights:  60%|██████    | 88/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  52%|█████▏    | 76/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  52%|█████▏    | 76/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  61%|██████    | 89/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  61%|██████    | 89/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  53%|█████▎    | 77/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  53%|█████▎    | 77/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  62%|██████▏   | 90/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  62%|██████▏   | 90/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  53%|█████▎    | 78/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  53%|█████▎    | 78/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  54%|█████▍    | 79/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        \rLoading weights:  62%|██████▏   | 91/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  62%|██████▏   | 91/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  54%|█████▍    | 79/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  55%|█████▍    | 80/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  55%|█████▍    | 80/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  63%|██████▎   | 92/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  63%|██████▎   | 92/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  64%|██████▎   | 93/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  64%|██████▎   | 93/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  55%|█████▌    | 81/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  55%|█████▌    | 81/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  64%|██████▍   | 94/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  64%|██████▍   | 94/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  56%|█████▌    | 82/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  56%|█████▌    | 82/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  57%|█████▋    | 83/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  65%|██████▌   | 95/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  65%|██████▌   | 95/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  57%|█████▋    | 83/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  58%|█████▊    | 84/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  58%|█████▊    | 84/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  66%|██████▌   | 96/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  66%|██████▌   | 96/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  66%|██████▋   | 97/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        \rLoading weights:  66%|██████▋   | 97/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  58%|█████▊    | 85/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  58%|█████▊    | 85/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  67%|██████▋   | 98/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  67%|██████▋   | 98/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  59%|█████▉    | 86/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  59%|█████▉    | 86/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  68%|██████▊   | 99/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  68%|██████▊   | 99/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  60%|█████▉    | 87/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  60%|█████▉    | 87/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  60%|██████    | 88/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        \rLoading weights:  68%|██████▊   | 100/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  60%|██████    | 88/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  68%|██████▊   | 100/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  61%|██████    | 89/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  61%|██████    | 89/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  69%|██████▉   | 101/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  69%|██████▉   | 101/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  70%|██████▉   | 102/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  70%|██████▉   | 102/146 [00:01<00:01, 39.65it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  62%|██████▏   | 90/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  62%|██████▏   | 90/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  62%|██████▏   | 91/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  62%|██████▏   | 91/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  71%|███████   | 104/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  63%|██████▎   | 92/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  71%|███████   | 104/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  63%|██████▎   | 92/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  64%|██████▎   | 93/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  64%|██████▎   | 93/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  72%|███████▏  | 105/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  72%|███████▏  | 105/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  73%|███████▎  | 106/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        \rLoading weights:  73%|███████▎  | 106/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  64%|██████▍   | 94/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  64%|██████▍   | 94/146 [00:01<00:01, 51.91it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  73%|███████▎  | 107/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  73%|███████▎  | 107/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  65%|██████▌   | 95/146 [00:01<00:00, 51.91it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  65%|██████▌   | 95/146 [00:01<00:00, 51.91it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  74%|███████▍  | 108/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  74%|███████▍  | 108/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  66%|██████▌   | 96/146 [00:01<00:00, 51.91it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  66%|██████▌   | 96/146 [00:01<00:00, 51.91it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  66%|██████▋   | 97/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  75%|███████▍  | 109/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  75%|███████▍  | 109/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  66%|██████▋   | 97/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        \rLoading weights:  66%|██████▋   | 97/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  75%|███████▌  | 110/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  75%|███████▌  | 110/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  76%|███████▌  | 111/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  67%|██████▋   | 98/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  76%|███████▌  | 111/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  67%|██████▋   | 98/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  77%|███████▋  | 112/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  77%|███████▋  | 112/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  68%|██████▊   | 99/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  68%|██████▊   | 99/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  77%|███████▋  | 113/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  77%|███████▋  | 113/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  68%|██████▊   | 100/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  68%|██████▊   | 100/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  69%|██████▉   | 101/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  78%|███████▊  | 114/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  78%|███████▊  | 114/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  69%|██████▉   | 101/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  79%|███████▉  | 115/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        \rLoading weights:  70%|██████▉   | 102/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  79%|███████▉  | 115/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  70%|██████▉   | 102/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  71%|███████   | 103/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  79%|███████▉  | 116/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  79%|███████▉  | 116/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  71%|███████   | 104/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  71%|███████   | 104/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  72%|███████▏  | 105/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  72%|███████▏  | 105/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  73%|███████▎  | 106/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        \rLoading weights:  81%|████████  | 118/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  81%|████████  | 118/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  73%|███████▎  | 106/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  73%|███████▎  | 107/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  73%|███████▎  | 107/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  82%|████████▏ | 119/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  82%|████████▏ | 119/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  82%|████████▏ | 120/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  82%|████████▏ | 120/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  74%|███████▍  | 108/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  74%|███████▍  | 108/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  83%|████████▎ | 121/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  83%|████████▎ | 121/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  75%|███████▍  | 109/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  75%|███████▍  | 109/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  84%|████████▎ | 122/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  84%|████████▎ | 122/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  75%|███████▌  | 110/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  75%|███████▌  | 110/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  76%|███████▌  | 111/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  76%|███████▌  | 111/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  84%|████████▍ | 123/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  84%|████████▍ | 123/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  85%|████████▍ | 124/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        \rLoading weights:  85%|████████▍ | 124/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  77%|███████▋  | 112/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  77%|███████▋  | 112/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  86%|████████▌ | 125/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  86%|████████▌ | 125/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  77%|███████▋  | 113/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  77%|███████▋  | 113/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  86%|████████▋ | 126/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  86%|████████▋ | 126/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  78%|███████▊  | 114/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  78%|███████▊  | 114/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  87%|████████▋ | 127/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  87%|████████▋ | 127/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  79%|███████▉  | 115/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        \rLoading weights:  79%|███████▉  | 115/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  88%|████████▊ | 128/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  88%|████████▊ | 128/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  88%|████████▊ | 129/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  88%|████████▊ | 129/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  79%|███████▉  | 116/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  79%|███████▉  | 116/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  89%|████████▉ | 130/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  89%|████████▉ | 130/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  80%|████████  | 117/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  90%|████████▉ | 131/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  90%|████████▉ | 131/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  81%|████████  | 118/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  81%|████████  | 118/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  82%|████████▏ | 119/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  90%|█████████ | 132/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  90%|█████████ | 132/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  82%|████████▏ | 119/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  82%|████████▏ | 120/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  91%|█████████ | 133/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        \rLoading weights:  91%|█████████ | 133/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  82%|████████▏ | 120/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  83%|████████▎ | 121/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  92%|█████████▏| 134/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  83%|████████▎ | 121/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  92%|█████████▏| 134/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  84%|████████▎ | 122/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  84%|████████▎ | 122/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  92%|█████████▏| 135/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  92%|█████████▏| 135/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  84%|████████▍ | 123/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  93%|█████████▎| 136/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  84%|████████▍ | 123/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  93%|█████████▎| 136/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  85%|████████▍ | 124/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        \rLoading weights:  85%|████████▍ | 124/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  94%|█████████▍| 137/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  94%|█████████▍| 137/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  95%|█████████▍| 138/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  95%|█████████▍| 138/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  86%|████████▌ | 125/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  86%|████████▌ | 125/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  95%|█████████▌| 139/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  95%|█████████▌| 139/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  86%|████████▋ | 126/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  86%|████████▋ | 126/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  96%|█████████▌| 140/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  96%|█████████▌| 140/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  87%|████████▋ | 127/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  87%|████████▋ | 127/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  97%|█████████▋| 141/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  97%|█████████▋| 141/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  88%|████████▊ | 128/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  88%|████████▊ | 128/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  97%|█████████▋| 142/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        \rLoading weights:  97%|█████████▋| 142/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  88%|████████▊ | 129/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  88%|████████▊ | 129/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  98%|█████████▊| 143/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  98%|█████████▊| 143/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  89%|████████▉ | 130/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  89%|████████▉ | 130/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  99%|█████████▊| 144/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  99%|█████████▊| 144/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  90%|████████▉ | 131/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  90%|████████▉ | 131/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  99%|█████████▉| 145/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  99%|█████████▉| 145/146 [00:01<00:00, 202.62it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights: 100%|██████████| 146/146 [00:01<00:00, 202.62it/s, Materializing param=model.norm.weight]                      \rLoading weights:  90%|█████████ | 132/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights: 100%|██████████| 146/146 [00:01<00:00, 202.62it/s, Materializing param=model.norm.weight]\rLoading weights:  90%|█████████ | 132/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights: 100%|██████████| 146/146 [00:01<00:00, 110.17it/s, Materializing param=model.norm.weight]\rLoading weights:  91%|█████████ | 133/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        \n",
            "\rLoading weights:  91%|█████████ | 133/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  92%|█████████▏| 134/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  92%|█████████▏| 134/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  92%|█████████▏| 135/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  92%|█████████▏| 135/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  93%|█████████▎| 136/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  93%|█████████▎| 136/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  94%|█████████▍| 137/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  94%|█████████▍| 137/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  95%|█████████▍| 138/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  95%|█████████▍| 138/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  95%|█████████▌| 139/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  95%|█████████▌| 139/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  96%|█████████▌| 140/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  96%|█████████▌| 140/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  97%|█████████▋| 141/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  97%|█████████▋| 141/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  97%|█████████▋| 142/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        \rLoading weights:  97%|█████████▋| 142/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  98%|█████████▊| 143/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  98%|█████████▊| 143/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  99%|█████████▊| 144/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  99%|█████████▊| 144/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  99%|█████████▉| 145/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  99%|█████████▉| 145/146 [00:01<00:00, 192.86it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights: 100%|██████████| 146/146 [00:01<00:00, 192.86it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|██████████| 146/146 [00:01<00:00, 192.86it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|██████████| 146/146 [00:01<00:00, 109.75it/s, Materializing param=model.norm.weight]\n",
            "\rTesting business_ethics:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "\rTesting astronomy:   0%|          | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "\rTesting business_ethics:   1%|          | 1/100 [00:00<01:02,  1.58it/s]\rTesting astronomy:   1%|          | 1/152 [00:00<01:36,  1.57it/s]\rTesting business_ethics:   3%|▎         | 3/100 [00:00<00:24,  3.96it/s]\rTesting business_ethics:   4%|▍         | 4/100 [00:01<00:20,  4.64it/s]\rTesting astronomy:   1%|▏         | 2/152 [00:00<00:53,  2.79it/s]\rTesting business_ethics:   5%|▌         | 5/100 [00:01<00:18,  5.20it/s]\rTesting astronomy:   2%|▏         | 3/152 [00:00<00:38,  3.85it/s]\rTesting business_ethics:   6%|▌         | 6/100 [00:01<00:17,  5.51it/s]\rTesting astronomy:   3%|▎         | 4/152 [00:01<00:32,  4.56it/s]\rTesting astronomy:   3%|▎         | 5/152 [00:01<00:28,  5.18it/s]\rTesting business_ethics:   7%|▋         | 7/100 [00:01<00:16,  5.78it/s]\rTesting astronomy:   4%|▍         | 6/152 [00:01<00:26,  5.56it/s]\rTesting business_ethics:   8%|▊         | 8/100 [00:01<00:15,  5.99it/s]\rTesting business_ethics:   9%|▉         | 9/100 [00:01<00:14,  6.12it/s]\rTesting astronomy:   5%|▍         | 7/152 [00:01<00:25,  5.78it/s]\rTesting astronomy:   5%|▌         | 8/152 [00:01<00:23,  6.00it/s]\rTesting business_ethics:  10%|█         | 10/100 [00:01<00:14,  6.19it/s]\rTesting business_ethics:  11%|█         | 11/100 [00:02<00:14,  6.32it/s]\rTesting astronomy:   6%|▌         | 9/152 [00:01<00:23,  6.11it/s]\rTesting business_ethics:  12%|█▏        | 12/100 [00:02<00:13,  6.34it/s]\rTesting astronomy:   7%|▋         | 10/152 [00:02<00:23,  5.97it/s]\rTesting business_ethics:  13%|█▎        | 13/100 [00:02<00:13,  6.22it/s]\rTesting astronomy:   7%|▋         | 11/152 [00:02<00:23,  6.11it/s]\rTesting business_ethics:  14%|█▍        | 14/100 [00:02<00:13,  6.30it/s]\rTesting astronomy:   8%|▊         | 12/152 [00:02<00:22,  6.22it/s]\rTesting business_ethics:  15%|█▌        | 15/100 [00:02<00:13,  6.36it/s]\rTesting astronomy:   9%|▊         | 13/152 [00:02<00:21,  6.36it/s]\rTesting business_ethics:  16%|█▌        | 16/100 [00:02<00:13,  6.41it/s]\rTesting astronomy:   9%|▉         | 14/152 [00:02<00:21,  6.41it/s]\rTesting business_ethics:  17%|█▋        | 17/100 [00:03<00:12,  6.50it/s]\rTesting astronomy:  10%|▉         | 15/152 [00:02<00:20,  6.53it/s]\rTesting astronomy:  11%|█         | 16/152 [00:02<00:20,  6.57it/s]\rTesting business_ethics:  18%|█▊        | 18/100 [00:03<00:12,  6.47it/s]\rTesting astronomy:  11%|█         | 17/152 [00:03<00:20,  6.56it/s]\rTesting business_ethics:  19%|█▉        | 19/100 [00:03<00:12,  6.51it/s]\rTesting business_ethics:  20%|██        | 20/100 [00:03<00:12,  6.51it/s]\rTesting astronomy:  12%|█▏        | 18/152 [00:03<00:20,  6.50it/s]\rTesting business_ethics:  21%|██        | 21/100 [00:03<00:12,  6.57it/s]\rTesting astronomy:  12%|█▎        | 19/152 [00:03<00:21,  6.05it/s]\rTesting business_ethics:  22%|██▏       | 22/100 [00:03<00:11,  6.56it/s]\rTesting astronomy:  13%|█▎        | 20/152 [00:03<00:20,  6.32it/s]\rTesting astronomy:  14%|█▍        | 21/152 [00:03<00:20,  6.38it/s]\rTesting business_ethics:  23%|██▎       | 23/100 [00:03<00:12,  5.95it/s]\rTesting astronomy:  14%|█▍        | 22/152 [00:03<00:19,  6.60it/s]\rTesting business_ethics:  24%|██▍       | 24/100 [00:04<00:14,  5.27it/s]\rTesting astronomy:  15%|█▌        | 23/152 [00:04<00:18,  6.97it/s]\rTesting business_ethics:  26%|██▌       | 26/100 [00:04<00:11,  6.45it/s]\rTesting astronomy:  16%|█▌        | 24/152 [00:04<00:21,  5.94it/s]\rTesting business_ethics:  27%|██▋       | 27/100 [00:04<00:11,  6.60it/s]\rTesting astronomy:  16%|█▋        | 25/152 [00:04<00:23,  5.39it/s]\rTesting business_ethics:  28%|██▊       | 28/100 [00:04<00:10,  7.04it/s]\rTesting astronomy:  17%|█▋        | 26/152 [00:04<00:21,  5.93it/s]\rTesting astronomy:  18%|█▊        | 27/152 [00:04<00:18,  6.63it/s]\rTesting business_ethics:  29%|██▉       | 29/100 [00:04<00:11,  6.01it/s]\rTesting astronomy:  18%|█▊        | 28/152 [00:04<00:22,  5.53it/s]\rTesting business_ethics:  31%|███       | 31/100 [00:05<00:10,  6.79it/s]\rTesting business_ethics:  32%|███▏      | 32/100 [00:05<00:09,  7.23it/s]\rTesting astronomy:  19%|█▉        | 29/152 [00:05<00:23,  5.18it/s]\rTesting business_ethics:  33%|███▎      | 33/100 [00:05<00:09,  6.89it/s]\rTesting astronomy:  20%|█▉        | 30/152 [00:05<00:22,  5.36it/s]\rTesting business_ethics:  34%|███▍      | 34/100 [00:05<00:10,  6.48it/s]\rTesting astronomy:  20%|██        | 31/152 [00:05<00:22,  5.29it/s]\rTesting business_ethics:  35%|███▌      | 35/100 [00:05<00:10,  6.42it/s]\rTesting astronomy:  21%|██        | 32/152 [00:05<00:21,  5.53it/s]\rTesting business_ethics:  36%|███▌      | 36/100 [00:05<00:09,  6.51it/s]\rTesting business_ethics:  37%|███▋      | 37/100 [00:06<00:09,  6.60it/s]\rTesting astronomy:  22%|██▏       | 33/152 [00:05<00:21,  5.53it/s]\rTesting astronomy:  22%|██▏       | 34/152 [00:05<00:18,  6.30it/s]\rTesting business_ethics:  38%|███▊      | 38/100 [00:06<00:10,  5.82it/s]\rTesting astronomy:  23%|██▎       | 35/152 [00:06<00:17,  6.57it/s]\rTesting astronomy:  24%|██▎       | 36/152 [00:06<00:17,  6.63it/s]\rTesting business_ethics:  39%|███▉      | 39/100 [00:06<00:10,  5.57it/s]\rTesting astronomy:  24%|██▍       | 37/152 [00:06<00:17,  6.46it/s]\rTesting business_ethics:  40%|████      | 40/100 [00:06<00:11,  5.32it/s]\rTesting astronomy:  25%|██▌       | 38/152 [00:06<00:17,  6.63it/s]\rTesting business_ethics:  41%|████      | 41/100 [00:06<00:10,  5.42it/s]\rTesting astronomy:  26%|██▌       | 39/152 [00:06<00:17,  6.50it/s]\rTesting business_ethics:  42%|████▏     | 42/100 [00:07<00:09,  5.84it/s]\rTesting astronomy:  26%|██▋       | 40/152 [00:06<00:17,  6.45it/s]\rTesting business_ethics:  43%|████▎     | 43/100 [00:07<00:09,  6.05it/s]\rTesting astronomy:  27%|██▋       | 41/152 [00:07<00:17,  6.34it/s]\rTesting business_ethics:  44%|████▍     | 44/100 [00:07<00:09,  6.08it/s]\rTesting astronomy:  28%|██▊       | 42/152 [00:07<00:17,  6.38it/s]\rTesting business_ethics:  45%|████▌     | 45/100 [00:07<00:08,  6.21it/s]\rTesting astronomy:  28%|██▊       | 43/152 [00:07<00:16,  6.46it/s]\rTesting business_ethics:  46%|████▌     | 46/100 [00:07<00:08,  6.34it/s]\rTesting astronomy:  29%|██▉       | 44/152 [00:07<00:16,  6.56it/s]\rTesting business_ethics:  47%|████▋     | 47/100 [00:07<00:08,  6.12it/s]\rTesting astronomy:  30%|██▉       | 45/152 [00:07<00:16,  6.47it/s]\rTesting business_ethics:  48%|████▊     | 48/100 [00:07<00:08,  6.22it/s]\rTesting astronomy:  30%|███       | 46/152 [00:07<00:16,  6.42it/s]\rTesting business_ethics:  49%|████▉     | 49/100 [00:08<00:08,  6.27it/s]\rTesting astronomy:  31%|███       | 47/152 [00:07<00:16,  6.49it/s]\rTesting business_ethics:  50%|█████     | 50/100 [00:08<00:07,  6.26it/s]\rTesting astronomy:  32%|███▏      | 48/152 [00:08<00:15,  6.56it/s]\rTesting business_ethics:  51%|█████     | 51/100 [00:08<00:07,  6.29it/s]\rTesting astronomy:  32%|███▏      | 49/152 [00:08<00:15,  6.46it/s]\rTesting business_ethics:  52%|█████▏    | 52/100 [00:08<00:07,  6.21it/s]\rTesting astronomy:  33%|███▎      | 50/152 [00:08<00:16,  6.30it/s]\rTesting business_ethics:  53%|█████▎    | 53/100 [00:08<00:07,  6.23it/s]\rTesting astronomy:  34%|███▎      | 51/152 [00:08<00:16,  6.31it/s]\rTesting business_ethics:  54%|█████▍    | 54/100 [00:08<00:07,  6.16it/s]\rTesting astronomy:  34%|███▍      | 52/152 [00:08<00:15,  6.33it/s]\rTesting business_ethics:  55%|█████▌    | 55/100 [00:09<00:07,  6.22it/s]\rTesting astronomy:  35%|███▍      | 53/152 [00:08<00:15,  6.28it/s]\rTesting business_ethics:  56%|█████▌    | 56/100 [00:09<00:06,  6.33it/s]\rTesting astronomy:  36%|███▌      | 54/152 [00:09<00:15,  6.44it/s]\rTesting business_ethics:  57%|█████▋    | 57/100 [00:09<00:06,  6.43it/s]\rTesting astronomy:  36%|███▌      | 55/152 [00:09<00:14,  6.48it/s]\rTesting business_ethics:  58%|█████▊    | 58/100 [00:09<00:06,  6.40it/s]\rTesting astronomy:  37%|███▋      | 56/152 [00:09<00:15,  6.40it/s]\rTesting astronomy:  38%|███▊      | 57/152 [00:09<00:14,  6.36it/s]\rTesting business_ethics:  59%|█████▉    | 59/100 [00:09<00:07,  5.81it/s]\rTesting business_ethics:  60%|██████    | 60/100 [00:09<00:06,  6.16it/s]\rTesting astronomy:  38%|███▊      | 58/152 [00:09<00:15,  6.26it/s]\rTesting business_ethics:  61%|██████    | 61/100 [00:10<00:06,  6.10it/s]\rTesting astronomy:  39%|███▉      | 59/152 [00:09<00:14,  6.23it/s]\rTesting business_ethics:  62%|██████▏   | 62/100 [00:10<00:06,  6.03it/s]\rTesting astronomy:  39%|███▉      | 60/152 [00:10<00:14,  6.18it/s]\rTesting business_ethics:  63%|██████▎   | 63/100 [00:10<00:06,  6.09it/s]\rTesting astronomy:  40%|████      | 61/152 [00:10<00:14,  6.24it/s]\rTesting business_ethics:  64%|██████▍   | 64/100 [00:10<00:05,  6.12it/s]\rTesting astronomy:  41%|████      | 62/152 [00:10<00:14,  6.23it/s]\rTesting business_ethics:  65%|██████▌   | 65/100 [00:10<00:05,  6.12it/s]\rTesting astronomy:  41%|████▏     | 63/152 [00:10<00:14,  6.21it/s]\rTesting business_ethics:  66%|██████▌   | 66/100 [00:10<00:05,  6.08it/s]\rTesting astronomy:  42%|████▏     | 64/152 [00:10<00:14,  6.17it/s]\rTesting business_ethics:  67%|██████▋   | 67/100 [00:11<00:05,  6.20it/s]\rTesting astronomy:  43%|████▎     | 65/152 [00:10<00:13,  6.23it/s]\rTesting business_ethics:  68%|██████▊   | 68/100 [00:11<00:05,  6.14it/s]\rTesting astronomy:  43%|████▎     | 66/152 [00:11<00:13,  6.20it/s]\rTesting astronomy:  44%|████▍     | 67/152 [00:11<00:13,  6.13it/s]\rTesting business_ethics:  69%|██████▉   | 69/100 [00:11<00:05,  6.05it/s]\rTesting business_ethics:  70%|███████   | 70/100 [00:11<00:04,  6.17it/s]\rTesting astronomy:  45%|████▍     | 68/152 [00:11<00:13,  6.22it/s]\rTesting business_ethics:  71%|███████   | 71/100 [00:11<00:04,  6.21it/s]\rTesting astronomy:  45%|████▌     | 69/152 [00:11<00:13,  6.21it/s]\rTesting business_ethics:  72%|███████▏  | 72/100 [00:11<00:04,  6.24it/s]\rTesting astronomy:  46%|████▌     | 70/152 [00:11<00:13,  6.18it/s]\rTesting business_ethics:  73%|███████▎  | 73/100 [00:12<00:04,  6.13it/s]\rTesting astronomy:  47%|████▋     | 71/152 [00:11<00:13,  6.14it/s]\rTesting business_ethics:  74%|███████▍  | 74/100 [00:12<00:04,  6.18it/s]\rTesting astronomy:  47%|████▋     | 72/152 [00:11<00:13,  6.14it/s]\rTesting business_ethics:  75%|███████▌  | 75/100 [00:12<00:04,  6.21it/s]\rTesting astronomy:  48%|████▊     | 73/152 [00:12<00:12,  6.18it/s]\rTesting business_ethics:  76%|███████▌  | 76/100 [00:12<00:03,  6.20it/s]\rTesting astronomy:  49%|████▊     | 74/152 [00:12<00:12,  6.23it/s]\rTesting business_ethics:  77%|███████▋  | 77/100 [00:12<00:03,  6.19it/s]\rTesting astronomy:  49%|████▉     | 75/152 [00:12<00:12,  6.25it/s]\rTesting business_ethics:  78%|███████▊  | 78/100 [00:12<00:03,  6.23it/s]\rTesting astronomy:  50%|█████     | 76/152 [00:12<00:12,  6.26it/s]\rTesting business_ethics:  79%|███████▉  | 79/100 [00:13<00:03,  6.29it/s]\rTesting astronomy:  51%|█████     | 77/152 [00:12<00:11,  6.26it/s]\rTesting astronomy:  51%|█████▏    | 78/152 [00:12<00:11,  6.33it/s]\rTesting business_ethics:  80%|████████  | 80/100 [00:13<00:03,  6.16it/s]\rTesting astronomy:  52%|█████▏    | 79/152 [00:13<00:11,  6.26it/s]\rTesting business_ethics:  81%|████████  | 81/100 [00:13<00:03,  6.20it/s]\rTesting astronomy:  53%|█████▎    | 80/152 [00:13<00:11,  6.34it/s]\rTesting business_ethics:  82%|████████▏ | 82/100 [00:13<00:02,  6.30it/s]\rTesting astronomy:  53%|█████▎    | 81/152 [00:13<00:11,  6.21it/s]\rTesting business_ethics:  83%|████████▎ | 83/100 [00:13<00:02,  6.23it/s]\rTesting astronomy:  54%|█████▍    | 82/152 [00:13<00:11,  6.22it/s]\rTesting business_ethics:  84%|████████▍ | 84/100 [00:13<00:02,  6.24it/s]\rTesting business_ethics:  85%|████████▌ | 85/100 [00:13<00:02,  6.32it/s]\rTesting astronomy:  55%|█████▍    | 83/152 [00:13<00:11,  6.23it/s]\rTesting business_ethics:  86%|████████▌ | 86/100 [00:14<00:02,  6.28it/s]\rTesting astronomy:  55%|█████▌    | 84/152 [00:13<00:11,  6.16it/s]\rTesting astronomy:  56%|█████▌    | 85/152 [00:14<00:10,  6.17it/s]\rTesting business_ethics:  87%|████████▋ | 87/100 [00:14<00:02,  6.02it/s]\rTesting astronomy:  57%|█████▋    | 86/152 [00:14<00:10,  6.10it/s]\rTesting business_ethics:  88%|████████▊ | 88/100 [00:14<00:01,  6.08it/s]\rTesting astronomy:  57%|█████▋    | 87/152 [00:14<00:10,  6.27it/s]\rTesting business_ethics:  89%|████████▉ | 89/100 [00:14<00:01,  6.24it/s]\rTesting business_ethics:  90%|█████████ | 90/100 [00:14<00:01,  6.29it/s]\rTesting astronomy:  58%|█████▊    | 88/152 [00:14<00:10,  6.26it/s]\rTesting business_ethics:  91%|█████████ | 91/100 [00:14<00:01,  6.34it/s]\rTesting astronomy:  59%|█████▊    | 89/152 [00:14<00:10,  6.27it/s]\rTesting business_ethics:  92%|█████████▏| 92/100 [00:15<00:01,  6.36it/s]\rTesting astronomy:  59%|█████▉    | 90/152 [00:14<00:09,  6.33it/s]\rTesting business_ethics:  93%|█████████▎| 93/100 [00:15<00:01,  6.24it/s]\rTesting astronomy:  60%|█████▉    | 91/152 [00:15<00:09,  6.25it/s]\rTesting business_ethics:  94%|█████████▍| 94/100 [00:15<00:00,  6.32it/s]\rTesting astronomy:  61%|██████    | 92/152 [00:15<00:09,  6.30it/s]\rTesting business_ethics:  95%|█████████▌| 95/100 [00:15<00:00,  6.39it/s]\rTesting astronomy:  61%|██████    | 93/152 [00:15<00:09,  6.31it/s]\rTesting business_ethics:  96%|█████████▌| 96/100 [00:15<00:00,  6.26it/s]\rTesting astronomy:  62%|██████▏   | 94/152 [00:15<00:09,  6.29it/s]\rTesting business_ethics:  97%|█████████▋| 97/100 [00:15<00:00,  6.23it/s]\rTesting astronomy:  62%|██████▎   | 95/152 [00:15<00:09,  6.23it/s]\rTesting business_ethics:  98%|█████████▊| 98/100 [00:16<00:00,  6.15it/s]\rTesting astronomy:  63%|██████▎   | 96/152 [00:15<00:09,  6.18it/s]\rTesting astronomy:  64%|██████▍   | 97/152 [00:15<00:08,  6.30it/s]\rTesting business_ethics:  99%|█████████▉| 99/100 [00:16<00:00,  6.22it/s]\rTesting astronomy:  64%|██████▍   | 98/152 [00:16<00:08,  6.33it/s]\rTesting business_ethics: 100%|██████████| 100/100 [00:16<00:00,  6.24it/s]\rTesting business_ethics: 100%|██████████| 100/100 [00:16<00:00,  6.11it/s]\n",
            "\rTesting astronomy:  66%|██████▌   | 100/152 [00:16<00:06,  7.75it/s]\rTesting astronomy:  67%|██████▋   | 102/152 [00:16<00:05,  8.78it/s]\rTesting astronomy:  68%|██████▊   | 104/152 [00:16<00:05,  9.48it/s]\rTesting astronomy:  69%|██████▉   | 105/152 [00:16<00:05,  9.39it/s]\rTesting astronomy:  70%|██████▉   | 106/152 [00:16<00:04,  9.32it/s]\rTesting astronomy:  70%|███████   | 107/152 [00:17<00:05,  8.65it/s]\rTesting astronomy:  71%|███████   | 108/152 [00:17<00:05,  7.58it/s]\rTesting astronomy:  72%|███████▏  | 109/152 [00:17<00:05,  7.60it/s]\rTesting astronomy:  73%|███████▎  | 111/152 [00:17<00:05,  7.73it/s]\rTesting astronomy:  74%|███████▎  | 112/152 [00:17<00:05,  6.76it/s]\rTesting astronomy:  74%|███████▍  | 113/152 [00:17<00:05,  6.54it/s]\rTesting astronomy:  75%|███████▌  | 114/152 [00:18<00:06,  6.11it/s]\rTesting astronomy:  76%|███████▋  | 116/152 [00:18<00:04,  7.48it/s]\rTesting astronomy:  77%|███████▋  | 117/152 [00:18<00:04,  7.13it/s]\rTesting astronomy:  78%|███████▊  | 118/152 [00:18<00:05,  6.75it/s]\rTesting astronomy:  78%|███████▊  | 119/152 [00:18<00:04,  6.60it/s]\rTesting astronomy:  79%|███████▉  | 120/152 [00:19<00:04,  6.44it/s]\rTesting astronomy:  80%|███████▉  | 121/152 [00:19<00:04,  6.32it/s]\rTesting astronomy:  80%|████████  | 122/152 [00:19<00:04,  6.98it/s]\rTesting astronomy:  82%|████████▏ | 124/152 [00:19<00:03,  8.17it/s]\rTesting astronomy:  83%|████████▎ | 126/152 [00:19<00:02,  9.01it/s]\rTesting astronomy:  84%|████████▍ | 128/152 [00:19<00:02, 10.25it/s]\rTesting astronomy:  86%|████████▌ | 130/152 [00:19<00:01, 11.06it/s]\rTesting astronomy:  87%|████████▋ | 132/152 [00:20<00:01, 11.50it/s]\rTesting astronomy:  88%|████████▊ | 134/152 [00:20<00:01, 11.79it/s]\rTesting astronomy:  89%|████████▉ | 136/152 [00:20<00:01, 12.43it/s]\rTesting astronomy:  91%|█████████ | 138/152 [00:20<00:01, 12.97it/s]\rTesting astronomy:  92%|█████████▏| 140/152 [00:20<00:00, 13.41it/s]\rTesting astronomy:  93%|█████████▎| 142/152 [00:20<00:00, 13.54it/s]\rTesting astronomy:  95%|█████████▍| 144/152 [00:21<00:00, 13.70it/s]\rTesting astronomy:  96%|█████████▌| 146/152 [00:21<00:00, 13.94it/s]\rTesting astronomy:  97%|█████████▋| 148/152 [00:21<00:00, 13.86it/s]\rTesting astronomy:  99%|█████████▊| 150/152 [00:21<00:00, 14.02it/s]\rTesting astronomy: 100%|██████████| 152/152 [00:21<00:00, 14.19it/s]\rTesting astronomy: 100%|██████████| 152/152 [00:21<00:00,  7.05it/s]\n",
            "\n",
            "real\t0m48.855s\n",
            "user\t1m10.995s\n",
            "sys\t0m5.926s\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "time { python llama_mmlu_astronomy.py & python llama_mmlu_business.py & wait; }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kT1v1TDmLrM"
      },
      "source": [
        "With Ollama Server - Sequential Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM4iV1gAmRNf",
        "outputId": "af3b62b1-859c-48d8-f0b8-176c64f1c986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing llama_mmlu_astronomy_ollama.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile llama_mmlu_astronomy_ollama.py\n",
        "\"\"\"\n",
        "Llama 3.2-1B MMLU Evaluation Script - ASTRONOMY (Ollama)\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
        "MODEL_NAME = \"llama3.2:1b\"\n",
        "MMLU_SUBJECTS = [\"astronomy\"]\n",
        "\n",
        "def format_mmlu_prompt(question, choices):\n",
        "    choice_labels = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    prompt = f\"{question}\\n\\n\"\n",
        "    for label, choice in zip(choice_labels, choices):\n",
        "        prompt += f\"{label}. {choice}\\n\"\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def get_model_prediction(prompt):\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            OLLAMA_URL,\n",
        "            json={\n",
        "                \"model\": MODEL_NAME,\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False,\n",
        "                \"options\": {\n",
        "                    \"temperature\": 0,\n",
        "                    \"num_predict\": 1\n",
        "                }\n",
        "            },\n",
        "            timeout=30\n",
        "        )\n",
        "        result = response.json()\n",
        "        answer = result['response'].strip()[:1].upper()\n",
        "\n",
        "        if answer not in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "            for char in result['response'].upper():\n",
        "                if char in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "                    answer = char\n",
        "                    break\n",
        "            else:\n",
        "                answer = \"A\"\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"A\"\n",
        "\n",
        "def evaluate_subject(subject):\n",
        "    print(f\"Evaluating: {subject}\")\n",
        "    dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for example in tqdm(dataset, desc=f\"Testing {subject}\"):\n",
        "        question = example[\"question\"]\n",
        "        choices = example[\"choices\"]\n",
        "        correct_answer_idx = example[\"answer\"]\n",
        "        correct_answer = [\"A\", \"B\", \"C\", \"D\"][correct_answer_idx]\n",
        "\n",
        "        prompt = format_mmlu_prompt(question, choices)\n",
        "        predicted_answer = get_model_prediction(prompt)\n",
        "\n",
        "        if predicted_answer == correct_answer:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    accuracy = (correct / total * 100) if total > 0 else 0\n",
        "    print(f\"Result: {correct}/{total} = {accuracy:.2f}%\")\n",
        "    return {\"subject\": subject, \"correct\": correct, \"total\": total, \"accuracy\": accuracy}\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"ASTRONOMY Evaluation (Ollama)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    results = []\n",
        "\n",
        "    for subject in MMLU_SUBJECTS:\n",
        "        result = evaluate_subject(subject)\n",
        "        results.append(result)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "    output_file = f\"astronomy_ollama_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump({\"results\": results, \"duration_seconds\": duration}, f, indent=2)\n",
        "\n",
        "    print(f\"\\nCompleted in {duration:.1f} seconds\")\n",
        "    print(f\"Results saved to: {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YezQfMxGml3V",
        "outputId": "492480ee-16a7-4aeb-856e-b40a04ca877f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing llama_mmlu_business_ollama.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile llama_mmlu_business_ollama.py\n",
        "\"\"\"\n",
        "Llama 3.2-1B MMLU Evaluation Script - BUSINESS ETHICS (Ollama)\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
        "MODEL_NAME = \"llama3.2:1b\"\n",
        "MMLU_SUBJECTS = [\"business_ethics\"]\n",
        "\n",
        "def format_mmlu_prompt(question, choices):\n",
        "    choice_labels = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    prompt = f\"{question}\\n\\n\"\n",
        "    for label, choice in zip(choice_labels, choices):\n",
        "        prompt += f\"{label}. {choice}\\n\"\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def get_model_prediction(prompt):\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            OLLAMA_URL,\n",
        "            json={\n",
        "                \"model\": MODEL_NAME,\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False,\n",
        "                \"options\": {\n",
        "                    \"temperature\": 0,\n",
        "                    \"num_predict\": 1\n",
        "                }\n",
        "            },\n",
        "            timeout=30\n",
        "        )\n",
        "        result = response.json()\n",
        "        answer = result['response'].strip()[:1].upper()\n",
        "\n",
        "        if answer not in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "            for char in result['response'].upper():\n",
        "                if char in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "                    answer = char\n",
        "                    break\n",
        "            else:\n",
        "                answer = \"A\"\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"A\"\n",
        "\n",
        "def evaluate_subject(subject):\n",
        "    print(f\"Evaluating: {subject}\")\n",
        "    dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for example in tqdm(dataset, desc=f\"Testing {subject}\"):\n",
        "        question = example[\"question\"]\n",
        "        choices = example[\"choices\"]\n",
        "        correct_answer_idx = example[\"answer\"]\n",
        "        correct_answer = [\"A\", \"B\", \"C\", \"D\"][correct_answer_idx]\n",
        "\n",
        "        prompt = format_mmlu_prompt(question, choices)\n",
        "        predicted_answer = get_model_prediction(prompt)\n",
        "\n",
        "        if predicted_answer == correct_answer:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    accuracy = (correct / total * 100) if total > 0 else 0\n",
        "    print(f\"Result: {correct}/{total} = {accuracy:.2f}%\")\n",
        "    return {\"subject\": subject, \"correct\": correct, \"total\": total, \"accuracy\": accuracy}\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"BUSINESS ETHICS Evaluation (Ollama)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    results = []\n",
        "\n",
        "    for subject in MMLU_SUBJECTS:\n",
        "        result = evaluate_subject(subject)\n",
        "        results.append(result)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "    output_file = f\"business_ethics_ollama_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump({\"results\": results, \"duration_seconds\": duration}, f, indent=2)\n",
        "\n",
        "    print(f\"\\nCompleted in {duration:.1f} seconds\")\n",
        "    print(f\"Results saved to: {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7T-cCajmp_K",
        "outputId": "37c29c98-66cd-472b-fc36-11a8a21e3175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ASTRONOMY Evaluation (Ollama)\n",
            "======================================================================\n",
            "Evaluating: astronomy\n",
            "Result: 27/152 = 17.76%\n",
            "\n",
            "Completed in 52.7 seconds\n",
            "Results saved to: astronomy_ollama_results_20260222_184502.json\n",
            "======================================================================\n",
            "BUSINESS ETHICS Evaluation (Ollama)\n",
            "======================================================================\n",
            "Evaluating: business_ethics\n",
            "Result: 30/100 = 30.00%\n",
            "\n",
            "Completed in 32.9 seconds\n",
            "Results saved to: business_ethics_ollama_results_20260222_184536.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting astronomy:   0%|          | 0/152 [00:00<?, ?it/s]\rTesting astronomy:   1%|          | 1/152 [00:02<07:02,  2.80s/it]\rTesting astronomy:   1%|▏         | 2/152 [00:03<03:16,  1.31s/it]\rTesting astronomy:   2%|▏         | 3/152 [00:03<02:04,  1.20it/s]\rTesting astronomy:   3%|▎         | 4/152 [00:03<01:33,  1.59it/s]\rTesting astronomy:   3%|▎         | 5/152 [00:03<01:14,  1.98it/s]\rTesting astronomy:   4%|▍         | 6/152 [00:04<01:02,  2.33it/s]\rTesting astronomy:   5%|▍         | 7/152 [00:04<00:56,  2.56it/s]\rTesting astronomy:   5%|▌         | 8/152 [00:04<00:52,  2.76it/s]\rTesting astronomy:   6%|▌         | 9/152 [00:05<00:47,  3.04it/s]\rTesting astronomy:   7%|▋         | 10/152 [00:05<00:46,  3.09it/s]\rTesting astronomy:   7%|▋         | 11/152 [00:05<00:45,  3.11it/s]\rTesting astronomy:   8%|▊         | 12/152 [00:05<00:43,  3.23it/s]\rTesting astronomy:   9%|▊         | 13/152 [00:06<00:41,  3.35it/s]\rTesting astronomy:   9%|▉         | 14/152 [00:06<00:39,  3.47it/s]\rTesting astronomy:  10%|▉         | 15/152 [00:06<00:39,  3.45it/s]\rTesting astronomy:  11%|█         | 16/152 [00:07<00:38,  3.56it/s]\rTesting astronomy:  11%|█         | 17/152 [00:07<00:37,  3.59it/s]\rTesting astronomy:  12%|█▏        | 18/152 [00:07<00:37,  3.60it/s]\rTesting astronomy:  12%|█▎        | 19/152 [00:07<00:36,  3.60it/s]\rTesting astronomy:  13%|█▎        | 20/152 [00:08<00:36,  3.59it/s]\rTesting astronomy:  14%|█▍        | 21/152 [00:08<00:36,  3.62it/s]\rTesting astronomy:  14%|█▍        | 22/152 [00:08<00:36,  3.56it/s]\rTesting astronomy:  15%|█▌        | 23/152 [00:09<00:45,  2.81it/s]\rTesting astronomy:  16%|█▌        | 24/152 [00:09<00:52,  2.46it/s]\rTesting astronomy:  16%|█▋        | 25/152 [00:10<00:56,  2.27it/s]\rTesting astronomy:  17%|█▋        | 26/152 [00:10<00:59,  2.12it/s]\rTesting astronomy:  18%|█▊        | 27/152 [00:11<01:00,  2.07it/s]\rTesting astronomy:  18%|█▊        | 28/152 [00:11<00:54,  2.27it/s]\rTesting astronomy:  19%|█▉        | 29/152 [00:12<00:48,  2.54it/s]\rTesting astronomy:  20%|█▉        | 30/152 [00:12<00:43,  2.79it/s]\rTesting astronomy:  20%|██        | 31/152 [00:12<00:40,  2.96it/s]\rTesting astronomy:  21%|██        | 32/152 [00:12<00:38,  3.13it/s]\rTesting astronomy:  22%|██▏       | 33/152 [00:13<00:36,  3.25it/s]\rTesting astronomy:  22%|██▏       | 34/152 [00:13<00:35,  3.35it/s]\rTesting astronomy:  23%|██▎       | 35/152 [00:13<00:34,  3.40it/s]\rTesting astronomy:  24%|██▎       | 36/152 [00:14<00:39,  2.90it/s]\rTesting astronomy:  24%|██▍       | 37/152 [00:14<00:37,  3.05it/s]\rTesting astronomy:  25%|██▌       | 38/152 [00:14<00:35,  3.24it/s]\rTesting astronomy:  26%|██▌       | 39/152 [00:15<00:34,  3.31it/s]\rTesting astronomy:  26%|██▋       | 40/152 [00:15<00:33,  3.31it/s]\rTesting astronomy:  27%|██▋       | 41/152 [00:15<00:32,  3.39it/s]\rTesting astronomy:  28%|██▊       | 42/152 [00:15<00:31,  3.46it/s]\rTesting astronomy:  28%|██▊       | 43/152 [00:16<00:31,  3.47it/s]\rTesting astronomy:  29%|██▉       | 44/152 [00:16<00:30,  3.50it/s]\rTesting astronomy:  30%|██▉       | 45/152 [00:16<00:29,  3.58it/s]\rTesting astronomy:  30%|███       | 46/152 [00:16<00:29,  3.57it/s]\rTesting astronomy:  31%|███       | 47/152 [00:17<00:29,  3.54it/s]\rTesting astronomy:  32%|███▏      | 48/152 [00:17<00:28,  3.59it/s]\rTesting astronomy:  32%|███▏      | 49/152 [00:17<00:28,  3.62it/s]\rTesting astronomy:  33%|███▎      | 50/152 [00:18<00:28,  3.63it/s]\rTesting astronomy:  34%|███▎      | 51/152 [00:18<00:28,  3.57it/s]\rTesting astronomy:  34%|███▍      | 52/152 [00:18<00:27,  3.61it/s]\rTesting astronomy:  35%|███▍      | 53/152 [00:18<00:27,  3.60it/s]\rTesting astronomy:  36%|███▌      | 54/152 [00:19<00:26,  3.65it/s]\rTesting astronomy:  36%|███▌      | 55/152 [00:19<00:26,  3.68it/s]\rTesting astronomy:  37%|███▋      | 56/152 [00:19<00:26,  3.63it/s]\rTesting astronomy:  38%|███▊      | 57/152 [00:19<00:25,  3.66it/s]\rTesting astronomy:  38%|███▊      | 58/152 [00:20<00:26,  3.55it/s]\rTesting astronomy:  39%|███▉      | 59/152 [00:20<00:25,  3.62it/s]\rTesting astronomy:  39%|███▉      | 60/152 [00:20<00:25,  3.62it/s]\rTesting astronomy:  40%|████      | 61/152 [00:21<00:24,  3.69it/s]\rTesting astronomy:  41%|████      | 62/152 [00:21<00:25,  3.57it/s]\rTesting astronomy:  41%|████▏     | 63/152 [00:21<00:27,  3.19it/s]\rTesting astronomy:  42%|████▏     | 64/152 [00:22<00:33,  2.65it/s]\rTesting astronomy:  43%|████▎     | 65/152 [00:22<00:36,  2.39it/s]\rTesting astronomy:  43%|████▎     | 66/152 [00:23<00:39,  2.17it/s]\rTesting astronomy:  44%|████▍     | 67/152 [00:23<00:40,  2.08it/s]\rTesting astronomy:  45%|████▍     | 68/152 [00:24<00:41,  2.00it/s]\rTesting astronomy:  45%|████▌     | 69/152 [00:24<00:36,  2.30it/s]\rTesting astronomy:  46%|████▌     | 70/152 [00:25<00:31,  2.56it/s]\rTesting astronomy:  47%|████▋     | 71/152 [00:25<00:29,  2.79it/s]\rTesting astronomy:  47%|████▋     | 72/152 [00:25<00:27,  2.95it/s]\rTesting astronomy:  48%|████▊     | 73/152 [00:25<00:25,  3.13it/s]\rTesting astronomy:  49%|████▊     | 74/152 [00:26<00:23,  3.29it/s]\rTesting astronomy:  49%|████▉     | 75/152 [00:26<00:22,  3.44it/s]\rTesting astronomy:  50%|█████     | 76/152 [00:26<00:22,  3.45it/s]\rTesting astronomy:  51%|█████     | 77/152 [00:26<00:21,  3.47it/s]\rTesting astronomy:  51%|█████▏    | 78/152 [00:27<00:20,  3.55it/s]\rTesting astronomy:  52%|█████▏    | 79/152 [00:27<00:20,  3.59it/s]\rTesting astronomy:  53%|█████▎    | 80/152 [00:27<00:20,  3.55it/s]\rTesting astronomy:  53%|█████▎    | 81/152 [00:28<00:19,  3.57it/s]\rTesting astronomy:  54%|█████▍    | 82/152 [00:28<00:20,  3.50it/s]\rTesting astronomy:  55%|█████▍    | 83/152 [00:28<00:19,  3.46it/s]\rTesting astronomy:  55%|█████▌    | 84/152 [00:28<00:19,  3.48it/s]\rTesting astronomy:  56%|█████▌    | 85/152 [00:29<00:18,  3.56it/s]\rTesting astronomy:  57%|█████▋    | 86/152 [00:29<00:18,  3.56it/s]\rTesting astronomy:  57%|█████▋    | 87/152 [00:29<00:18,  3.55it/s]\rTesting astronomy:  58%|█████▊    | 88/152 [00:30<00:18,  3.54it/s]\rTesting astronomy:  59%|█████▊    | 89/152 [00:30<00:17,  3.57it/s]\rTesting astronomy:  59%|█████▉    | 90/152 [00:30<00:17,  3.61it/s]\rTesting astronomy:  60%|█████▉    | 91/152 [00:30<00:17,  3.52it/s]\rTesting astronomy:  61%|██████    | 92/152 [00:31<00:16,  3.56it/s]\rTesting astronomy:  61%|██████    | 93/152 [00:31<00:16,  3.56it/s]\rTesting astronomy:  62%|██████▏   | 94/152 [00:31<00:16,  3.51it/s]\rTesting astronomy:  62%|██████▎   | 95/152 [00:32<00:15,  3.58it/s]\rTesting astronomy:  63%|██████▎   | 96/152 [00:32<00:15,  3.60it/s]\rTesting astronomy:  64%|██████▍   | 97/152 [00:32<00:15,  3.63it/s]\rTesting astronomy:  64%|██████▍   | 98/152 [00:32<00:15,  3.57it/s]\rTesting astronomy:  65%|██████▌   | 99/152 [00:33<00:14,  3.60it/s]\rTesting astronomy:  66%|██████▌   | 100/152 [00:33<00:14,  3.64it/s]\rTesting astronomy:  66%|██████▋   | 101/152 [00:33<00:13,  3.66it/s]\rTesting astronomy:  67%|██████▋   | 102/152 [00:33<00:14,  3.56it/s]\rTesting astronomy:  68%|██████▊   | 103/152 [00:34<00:13,  3.63it/s]\rTesting astronomy:  68%|██████▊   | 104/152 [00:34<00:13,  3.56it/s]\rTesting astronomy:  69%|██████▉   | 105/152 [00:35<00:16,  2.80it/s]\rTesting astronomy:  70%|██████▉   | 106/152 [00:35<00:18,  2.46it/s]\rTesting astronomy:  70%|███████   | 107/152 [00:36<00:20,  2.22it/s]\rTesting astronomy:  71%|███████   | 108/152 [00:36<00:20,  2.10it/s]\rTesting astronomy:  72%|███████▏  | 109/152 [00:37<00:20,  2.05it/s]\rTesting astronomy:  72%|███████▏  | 110/152 [00:37<00:20,  2.08it/s]\rTesting astronomy:  73%|███████▎  | 111/152 [00:37<00:17,  2.39it/s]\rTesting astronomy:  74%|███████▎  | 112/152 [00:38<00:15,  2.65it/s]\rTesting astronomy:  74%|███████▍  | 113/152 [00:38<00:13,  2.88it/s]\rTesting astronomy:  75%|███████▌  | 114/152 [00:38<00:12,  3.10it/s]\rTesting astronomy:  76%|███████▌  | 115/152 [00:39<00:11,  3.22it/s]\rTesting astronomy:  76%|███████▋  | 116/152 [00:39<00:10,  3.32it/s]\rTesting astronomy:  77%|███████▋  | 117/152 [00:39<00:10,  3.31it/s]\rTesting astronomy:  78%|███████▊  | 118/152 [00:39<00:10,  3.39it/s]\rTesting astronomy:  78%|███████▊  | 119/152 [00:40<00:09,  3.35it/s]\rTesting astronomy:  79%|███████▉  | 120/152 [00:40<00:09,  3.44it/s]\rTesting astronomy:  80%|███████▉  | 121/152 [00:40<00:08,  3.45it/s]\rTesting astronomy:  80%|████████  | 122/152 [00:41<00:08,  3.54it/s]\rTesting astronomy:  81%|████████  | 123/152 [00:41<00:08,  3.55it/s]\rTesting astronomy:  82%|████████▏ | 124/152 [00:41<00:07,  3.59it/s]\rTesting astronomy:  82%|████████▏ | 125/152 [00:41<00:07,  3.64it/s]\rTesting astronomy:  83%|████████▎ | 126/152 [00:42<00:07,  3.61it/s]\rTesting astronomy:  84%|████████▎ | 127/152 [00:42<00:07,  3.56it/s]\rTesting astronomy:  84%|████████▍ | 128/152 [00:42<00:06,  3.61it/s]\rTesting astronomy:  85%|████████▍ | 129/152 [00:42<00:06,  3.59it/s]\rTesting astronomy:  86%|████████▌ | 130/152 [00:43<00:06,  3.50it/s]\rTesting astronomy:  86%|████████▌ | 131/152 [00:43<00:06,  3.50it/s]\rTesting astronomy:  87%|████████▋ | 132/152 [00:43<00:05,  3.49it/s]\rTesting astronomy:  88%|████████▊ | 133/152 [00:44<00:05,  3.51it/s]\rTesting astronomy:  88%|████████▊ | 134/152 [00:44<00:05,  3.46it/s]\rTesting astronomy:  89%|████████▉ | 135/152 [00:44<00:05,  3.35it/s]\rTesting astronomy:  89%|████████▉ | 136/152 [00:45<00:04,  3.42it/s]\rTesting astronomy:  90%|█████████ | 137/152 [00:45<00:04,  3.44it/s]\rTesting astronomy:  91%|█████████ | 138/152 [00:45<00:03,  3.52it/s]\rTesting astronomy:  91%|█████████▏| 139/152 [00:45<00:03,  3.62it/s]\rTesting astronomy:  92%|█████████▏| 140/152 [00:46<00:03,  3.68it/s]\rTesting astronomy:  93%|█████████▎| 141/152 [00:46<00:03,  3.65it/s]\rTesting astronomy:  93%|█████████▎| 142/152 [00:46<00:02,  3.63it/s]\rTesting astronomy:  94%|█████████▍| 143/152 [00:46<00:02,  3.66it/s]\rTesting astronomy:  95%|█████████▍| 144/152 [00:47<00:02,  3.65it/s]\rTesting astronomy:  95%|█████████▌| 145/152 [00:47<00:01,  3.55it/s]\rTesting astronomy:  96%|█████████▌| 146/152 [00:48<00:02,  2.84it/s]\rTesting astronomy:  97%|█████████▋| 147/152 [00:48<00:02,  2.47it/s]\rTesting astronomy:  97%|█████████▋| 148/152 [00:49<00:01,  2.23it/s]\rTesting astronomy:  98%|█████████▊| 149/152 [00:49<00:01,  2.11it/s]\rTesting astronomy:  99%|█████████▊| 150/152 [00:50<00:00,  2.01it/s]\rTesting astronomy:  99%|█████████▉| 151/152 [00:50<00:00,  1.97it/s]\rTesting astronomy: 100%|██████████| 152/152 [00:50<00:00,  2.28it/s]\rTesting astronomy: 100%|██████████| 152/152 [00:50<00:00,  2.98it/s]\n",
            "\rTesting business_ethics:   0%|          | 0/100 [00:00<?, ?it/s]\rTesting business_ethics:   1%|          | 1/100 [00:00<00:27,  3.61it/s]\rTesting business_ethics:   2%|▏         | 2/100 [00:00<00:27,  3.62it/s]\rTesting business_ethics:   3%|▎         | 3/100 [00:00<00:26,  3.61it/s]\rTesting business_ethics:   4%|▍         | 4/100 [00:01<00:26,  3.62it/s]\rTesting business_ethics:   5%|▌         | 5/100 [00:01<00:26,  3.53it/s]\rTesting business_ethics:   6%|▌         | 6/100 [00:01<00:26,  3.59it/s]\rTesting business_ethics:   7%|▋         | 7/100 [00:01<00:26,  3.57it/s]\rTesting business_ethics:   8%|▊         | 8/100 [00:02<00:25,  3.60it/s]\rTesting business_ethics:   9%|▉         | 9/100 [00:02<00:25,  3.61it/s]\rTesting business_ethics:  10%|█         | 10/100 [00:02<00:24,  3.64it/s]\rTesting business_ethics:  11%|█         | 11/100 [00:03<00:24,  3.58it/s]\rTesting business_ethics:  12%|█▏        | 12/100 [00:03<00:24,  3.56it/s]\rTesting business_ethics:  13%|█▎        | 13/100 [00:03<00:24,  3.55it/s]\rTesting business_ethics:  14%|█▍        | 14/100 [00:03<00:24,  3.49it/s]\rTesting business_ethics:  15%|█▌        | 15/100 [00:04<00:24,  3.53it/s]\rTesting business_ethics:  16%|█▌        | 16/100 [00:04<00:23,  3.58it/s]\rTesting business_ethics:  17%|█▋        | 17/100 [00:04<00:22,  3.63it/s]\rTesting business_ethics:  18%|█▊        | 18/100 [00:05<00:23,  3.52it/s]\rTesting business_ethics:  19%|█▉        | 19/100 [00:05<00:22,  3.58it/s]\rTesting business_ethics:  20%|██        | 20/100 [00:05<00:21,  3.65it/s]\rTesting business_ethics:  21%|██        | 21/100 [00:05<00:21,  3.63it/s]\rTesting business_ethics:  22%|██▏       | 22/100 [00:06<00:21,  3.61it/s]\rTesting business_ethics:  23%|██▎       | 23/100 [00:06<00:21,  3.58it/s]\rTesting business_ethics:  24%|██▍       | 24/100 [00:06<00:21,  3.58it/s]\rTesting business_ethics:  25%|██▌       | 25/100 [00:07<00:25,  2.93it/s]\rTesting business_ethics:  26%|██▌       | 26/100 [00:07<00:29,  2.54it/s]\rTesting business_ethics:  27%|██▋       | 27/100 [00:08<00:31,  2.32it/s]\rTesting business_ethics:  28%|██▊       | 28/100 [00:08<00:33,  2.14it/s]\rTesting business_ethics:  29%|██▉       | 29/100 [00:09<00:34,  2.05it/s]\rTesting business_ethics:  30%|███       | 30/100 [00:09<00:35,  1.97it/s]\rTesting business_ethics:  31%|███       | 31/100 [00:10<00:31,  2.18it/s]\rTesting business_ethics:  32%|███▏      | 32/100 [00:10<00:27,  2.46it/s]\rTesting business_ethics:  33%|███▎      | 33/100 [00:10<00:24,  2.71it/s]\rTesting business_ethics:  34%|███▍      | 34/100 [00:11<00:22,  2.93it/s]\rTesting business_ethics:  35%|███▌      | 35/100 [00:11<00:21,  3.04it/s]\rTesting business_ethics:  36%|███▌      | 36/100 [00:11<00:20,  3.15it/s]\rTesting business_ethics:  37%|███▋      | 37/100 [00:11<00:19,  3.22it/s]\rTesting business_ethics:  38%|███▊      | 38/100 [00:12<00:18,  3.33it/s]\rTesting business_ethics:  39%|███▉      | 39/100 [00:12<00:18,  3.35it/s]\rTesting business_ethics:  40%|████      | 40/100 [00:12<00:17,  3.44it/s]\rTesting business_ethics:  41%|████      | 41/100 [00:13<00:16,  3.51it/s]\rTesting business_ethics:  42%|████▏     | 42/100 [00:13<00:16,  3.46it/s]\rTesting business_ethics:  43%|████▎     | 43/100 [00:13<00:16,  3.56it/s]\rTesting business_ethics:  44%|████▍     | 44/100 [00:13<00:15,  3.52it/s]\rTesting business_ethics:  45%|████▌     | 45/100 [00:14<00:15,  3.57it/s]\rTesting business_ethics:  46%|████▌     | 46/100 [00:14<00:15,  3.55it/s]\rTesting business_ethics:  47%|████▋     | 47/100 [00:14<00:14,  3.58it/s]\rTesting business_ethics:  48%|████▊     | 48/100 [00:15<00:14,  3.58it/s]\rTesting business_ethics:  49%|████▉     | 49/100 [00:15<00:14,  3.60it/s]\rTesting business_ethics:  50%|█████     | 50/100 [00:15<00:14,  3.57it/s]\rTesting business_ethics:  51%|█████     | 51/100 [00:15<00:13,  3.59it/s]\rTesting business_ethics:  52%|█████▏    | 52/100 [00:16<00:13,  3.54it/s]\rTesting business_ethics:  53%|█████▎    | 53/100 [00:16<00:13,  3.52it/s]\rTesting business_ethics:  54%|█████▍    | 54/100 [00:16<00:13,  3.51it/s]\rTesting business_ethics:  55%|█████▌    | 55/100 [00:16<00:12,  3.55it/s]\rTesting business_ethics:  56%|█████▌    | 56/100 [00:17<00:12,  3.61it/s]\rTesting business_ethics:  57%|█████▋    | 57/100 [00:17<00:12,  3.52it/s]\rTesting business_ethics:  58%|█████▊    | 58/100 [00:17<00:11,  3.55it/s]\rTesting business_ethics:  59%|█████▉    | 59/100 [00:18<00:11,  3.55it/s]\rTesting business_ethics:  60%|██████    | 60/100 [00:18<00:11,  3.59it/s]\rTesting business_ethics:  61%|██████    | 61/100 [00:18<00:11,  3.47it/s]\rTesting business_ethics:  62%|██████▏   | 62/100 [00:18<00:10,  3.55it/s]\rTesting business_ethics:  63%|██████▎   | 63/100 [00:19<00:10,  3.58it/s]\rTesting business_ethics:  64%|██████▍   | 64/100 [00:19<00:10,  3.52it/s]\rTesting business_ethics:  65%|██████▌   | 65/100 [00:19<00:09,  3.55it/s]\rTesting business_ethics:  66%|██████▌   | 66/100 [00:20<00:10,  3.34it/s]\rTesting business_ethics:  67%|██████▋   | 67/100 [00:20<00:11,  2.75it/s]\rTesting business_ethics:  68%|██████▊   | 68/100 [00:21<00:13,  2.31it/s]\rTesting business_ethics:  69%|██████▉   | 69/100 [00:21<00:14,  2.16it/s]\rTesting business_ethics:  70%|███████   | 70/100 [00:22<00:14,  2.09it/s]\rTesting business_ethics:  71%|███████   | 71/100 [00:22<00:14,  2.02it/s]\rTesting business_ethics:  72%|███████▏  | 72/100 [00:23<00:14,  1.93it/s]\rTesting business_ethics:  73%|███████▎  | 73/100 [00:23<00:12,  2.24it/s]\rTesting business_ethics:  74%|███████▍  | 74/100 [00:23<00:10,  2.50it/s]\rTesting business_ethics:  75%|███████▌  | 75/100 [00:24<00:09,  2.76it/s]\rTesting business_ethics:  76%|███████▌  | 76/100 [00:24<00:08,  2.97it/s]\rTesting business_ethics:  77%|███████▋  | 77/100 [00:24<00:07,  3.17it/s]\rTesting business_ethics:  78%|███████▊  | 78/100 [00:25<00:06,  3.32it/s]\rTesting business_ethics:  79%|███████▉  | 79/100 [00:25<00:06,  3.45it/s]\rTesting business_ethics:  80%|████████  | 80/100 [00:25<00:05,  3.45it/s]\rTesting business_ethics:  81%|████████  | 81/100 [00:25<00:05,  3.48it/s]\rTesting business_ethics:  82%|████████▏ | 82/100 [00:26<00:05,  3.54it/s]\rTesting business_ethics:  83%|████████▎ | 83/100 [00:26<00:04,  3.58it/s]\rTesting business_ethics:  84%|████████▍ | 84/100 [00:26<00:04,  3.56it/s]\rTesting business_ethics:  85%|████████▌ | 85/100 [00:27<00:04,  3.51it/s]\rTesting business_ethics:  86%|████████▌ | 86/100 [00:27<00:03,  3.56it/s]\rTesting business_ethics:  87%|████████▋ | 87/100 [00:27<00:03,  3.53it/s]\rTesting business_ethics:  88%|████████▊ | 88/100 [00:27<00:03,  3.61it/s]\rTesting business_ethics:  89%|████████▉ | 89/100 [00:28<00:03,  3.56it/s]\rTesting business_ethics:  90%|█████████ | 90/100 [00:28<00:02,  3.61it/s]\rTesting business_ethics:  91%|█████████ | 91/100 [00:28<00:02,  3.66it/s]\rTesting business_ethics:  92%|█████████▏| 92/100 [00:28<00:02,  3.55it/s]\rTesting business_ethics:  93%|█████████▎| 93/100 [00:29<00:01,  3.56it/s]\rTesting business_ethics:  94%|█████████▍| 94/100 [00:29<00:01,  3.60it/s]\rTesting business_ethics:  95%|█████████▌| 95/100 [00:29<00:01,  3.66it/s]\rTesting business_ethics:  96%|█████████▌| 96/100 [00:30<00:01,  3.59it/s]\rTesting business_ethics:  97%|█████████▋| 97/100 [00:30<00:00,  3.57it/s]\rTesting business_ethics:  98%|█████████▊| 98/100 [00:30<00:00,  3.57it/s]\rTesting business_ethics:  99%|█████████▉| 99/100 [00:30<00:00,  3.56it/s]\rTesting business_ethics: 100%|██████████| 100/100 [00:31<00:00,  3.51it/s]\rTesting business_ethics: 100%|██████████| 100/100 [00:31<00:00,  3.20it/s]\n",
            "\n",
            "real\t1m28.845s\n",
            "user\t0m4.107s\n",
            "sys\t0m0.446s\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "time { python llama_mmlu_astronomy_ollama.py ; python llama_mmlu_business_ollama.py ; }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "853czkn7m2Hz"
      },
      "source": [
        "With Ollama Server - Parallel Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK6v_y_wm6lr",
        "outputId": "d53470ad-2eb1-4289-e773-067a4792cf82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "BUSINESS ETHICS Evaluation (Ollama)\n",
            "======================================================================\n",
            "Evaluating: business_ethics\n",
            "======================================================================\n",
            "ASTRONOMY Evaluation (Ollama)\n",
            "======================================================================\n",
            "Evaluating: astronomy\n",
            "Result: 30/100 = 30.00%\n",
            "\n",
            "Completed in 62.5 seconds\n",
            "Results saved to: business_ethics_ollama_results_20260222_184646.json\n",
            "Result: 27/152 = 17.76%\n",
            "\n",
            "Completed in 80.5 seconds\n",
            "Results saved to: astronomy_ollama_results_20260222_184704.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting business_ethics:   0%|          | 0/100 [00:00<?, ?it/s]\rTesting astronomy:   0%|          | 0/152 [00:00<?, ?it/s]\rTesting business_ethics:   1%|          | 1/100 [00:00<00:55,  1.79it/s]\rTesting astronomy:   1%|          | 1/152 [00:00<01:25,  1.76it/s]\rTesting business_ethics:   2%|▏         | 2/100 [00:01<00:52,  1.85it/s]\rTesting astronomy:   1%|▏         | 2/152 [00:01<01:22,  1.82it/s]\rTesting business_ethics:   3%|▎         | 3/100 [00:01<00:51,  1.88it/s]\rTesting astronomy:   2%|▏         | 3/152 [00:01<01:23,  1.79it/s]\rTesting business_ethics:   4%|▍         | 4/100 [00:02<00:51,  1.88it/s]\rTesting astronomy:   3%|▎         | 4/152 [00:02<01:21,  1.83it/s]\rTesting business_ethics:   5%|▌         | 5/100 [00:02<00:52,  1.83it/s]\rTesting astronomy:   3%|▎         | 5/152 [00:02<01:21,  1.81it/s]\rTesting business_ethics:   6%|▌         | 6/100 [00:03<00:51,  1.83it/s]\rTesting astronomy:   4%|▍         | 6/152 [00:03<01:19,  1.85it/s]\rTesting business_ethics:   7%|▋         | 7/100 [00:03<00:51,  1.81it/s]\rTesting astronomy:   5%|▍         | 7/152 [00:03<01:19,  1.82it/s]\rTesting business_ethics:   8%|▊         | 8/100 [00:04<00:49,  1.85it/s]\rTesting astronomy:   5%|▌         | 8/152 [00:04<01:18,  1.85it/s]\rTesting business_ethics:   9%|▉         | 9/100 [00:04<00:49,  1.84it/s]\rTesting astronomy:   6%|▌         | 9/152 [00:04<01:16,  1.86it/s]\rTesting business_ethics:  10%|█         | 10/100 [00:05<00:48,  1.87it/s]\rTesting astronomy:   7%|▋         | 10/152 [00:05<01:17,  1.83it/s]\rTesting business_ethics:  11%|█         | 11/100 [00:06<00:59,  1.51it/s]\rTesting astronomy:   7%|▋         | 11/152 [00:06<01:33,  1.51it/s]\rTesting business_ethics:  12%|█▏        | 12/100 [00:07<01:07,  1.31it/s]\rTesting astronomy:   8%|▊         | 12/152 [00:07<01:46,  1.31it/s]\rTesting astronomy:   9%|▊         | 13/152 [00:08<01:46,  1.30it/s]\rTesting business_ethics:  13%|█▎        | 13/100 [00:08<01:10,  1.23it/s]\rTesting astronomy:   9%|▉         | 14/152 [00:08<01:46,  1.30it/s]\rTesting business_ethics:  14%|█▍        | 14/100 [00:09<01:08,  1.26it/s]\rTesting astronomy:  10%|▉         | 15/152 [00:09<01:44,  1.31it/s]\rTesting business_ethics:  15%|█▌        | 15/100 [00:09<01:06,  1.28it/s]\rTesting astronomy:  11%|█         | 16/152 [00:10<01:34,  1.43it/s]\rTesting business_ethics:  16%|█▌        | 16/100 [00:10<00:59,  1.40it/s]\rTesting astronomy:  11%|█         | 17/152 [00:10<01:27,  1.54it/s]\rTesting business_ethics:  17%|█▋        | 17/100 [00:10<00:54,  1.53it/s]\rTesting astronomy:  12%|█▏        | 18/152 [00:11<01:23,  1.60it/s]\rTesting business_ethics:  18%|█▊        | 18/100 [00:11<00:51,  1.60it/s]\rTesting business_ethics:  19%|█▉        | 19/100 [00:12<00:50,  1.61it/s]\rTesting astronomy:  12%|█▎        | 19/152 [00:12<01:25,  1.55it/s]\rTesting business_ethics:  20%|██        | 20/100 [00:12<00:48,  1.65it/s]\rTesting astronomy:  13%|█▎        | 20/152 [00:12<01:22,  1.61it/s]\rTesting business_ethics:  21%|██        | 21/100 [00:13<00:46,  1.70it/s]\rTesting astronomy:  14%|█▍        | 21/152 [00:13<01:18,  1.67it/s]\rTesting business_ethics:  22%|██▏       | 22/100 [00:13<00:44,  1.74it/s]\rTesting astronomy:  14%|█▍        | 22/152 [00:13<01:15,  1.72it/s]\rTesting business_ethics:  23%|██▎       | 23/100 [00:14<00:44,  1.73it/s]\rTesting astronomy:  15%|█▌        | 23/152 [00:14<01:15,  1.71it/s]\rTesting business_ethics:  24%|██▍       | 24/100 [00:14<00:42,  1.78it/s]\rTesting astronomy:  16%|█▌        | 24/152 [00:14<01:12,  1.77it/s]\rTesting business_ethics:  25%|██▌       | 25/100 [00:15<00:41,  1.79it/s]\rTesting astronomy:  16%|█▋        | 25/152 [00:15<01:11,  1.78it/s]\rTesting business_ethics:  26%|██▌       | 26/100 [00:15<00:40,  1.84it/s]\rTesting astronomy:  17%|█▋        | 26/152 [00:15<01:09,  1.81it/s]\rTesting business_ethics:  27%|██▋       | 27/100 [00:16<00:40,  1.81it/s]\rTesting astronomy:  18%|█▊        | 27/152 [00:16<01:09,  1.81it/s]\rTesting business_ethics:  28%|██▊       | 28/100 [00:16<00:39,  1.85it/s]\rTesting astronomy:  18%|█▊        | 28/152 [00:16<01:07,  1.84it/s]\rTesting business_ethics:  29%|██▉       | 29/100 [00:17<00:38,  1.84it/s]\rTesting astronomy:  19%|█▉        | 29/152 [00:17<01:07,  1.83it/s]\rTesting business_ethics:  30%|███       | 30/100 [00:18<00:37,  1.86it/s]\rTesting astronomy:  20%|█▉        | 30/152 [00:18<01:06,  1.82it/s]\rTesting business_ethics:  31%|███       | 31/100 [00:18<00:38,  1.81it/s]\rTesting astronomy:  20%|██        | 31/152 [00:18<01:06,  1.81it/s]\rTesting business_ethics:  32%|███▏      | 32/100 [00:19<00:36,  1.84it/s]\rTesting astronomy:  21%|██        | 32/152 [00:19<01:05,  1.84it/s]\rTesting business_ethics:  33%|███▎      | 33/100 [00:19<00:39,  1.70it/s]\rTesting astronomy:  22%|██▏       | 33/152 [00:19<01:10,  1.68it/s]\rTesting business_ethics:  34%|███▍      | 34/100 [00:20<00:47,  1.40it/s]\rTesting astronomy:  22%|██▏       | 34/152 [00:20<01:23,  1.41it/s]\rTesting astronomy:  23%|██▎       | 35/152 [00:21<01:29,  1.30it/s]\rTesting business_ethics:  35%|███▌      | 35/100 [00:21<00:51,  1.25it/s]\rTesting business_ethics:  36%|███▌      | 36/100 [00:22<00:51,  1.23it/s]\rTesting astronomy:  24%|██▎       | 36/152 [00:22<01:35,  1.22it/s]\rTesting business_ethics:  37%|███▋      | 37/100 [00:23<00:49,  1.28it/s]\rTesting astronomy:  24%|██▍       | 37/152 [00:23<01:31,  1.25it/s]\rTesting business_ethics:  38%|███▊      | 38/100 [00:23<00:44,  1.40it/s]\rTesting astronomy:  25%|██▌       | 38/152 [00:23<01:22,  1.38it/s]\rTesting business_ethics:  39%|███▉      | 39/100 [00:24<00:40,  1.50it/s]\rTesting astronomy:  26%|██▌       | 39/152 [00:24<01:15,  1.50it/s]\rTesting business_ethics:  40%|████      | 40/100 [00:25<00:37,  1.59it/s]\rTesting astronomy:  26%|██▋       | 40/152 [00:25<01:10,  1.58it/s]\rTesting business_ethics:  41%|████      | 41/100 [00:25<00:35,  1.68it/s]\rTesting astronomy:  27%|██▋       | 41/152 [00:25<01:07,  1.65it/s]\rTesting business_ethics:  42%|████▏     | 42/100 [00:26<00:33,  1.71it/s]\rTesting astronomy:  28%|██▊       | 42/152 [00:26<01:04,  1.71it/s]\rTesting business_ethics:  43%|████▎     | 43/100 [00:26<00:32,  1.77it/s]\rTesting astronomy:  28%|██▊       | 43/152 [00:26<01:02,  1.74it/s]\rTesting business_ethics:  44%|████▍     | 44/100 [00:27<00:31,  1.79it/s]\rTesting astronomy:  29%|██▉       | 44/152 [00:27<01:01,  1.76it/s]\rTesting business_ethics:  45%|████▌     | 45/100 [00:27<00:30,  1.81it/s]\rTesting astronomy:  30%|██▉       | 45/152 [00:27<00:58,  1.82it/s]\rTesting business_ethics:  46%|████▌     | 46/100 [00:28<00:29,  1.84it/s]\rTesting astronomy:  30%|███       | 46/152 [00:28<00:59,  1.79it/s]\rTesting business_ethics:  47%|████▋     | 47/100 [00:28<00:28,  1.85it/s]\rTesting astronomy:  31%|███       | 47/152 [00:28<00:57,  1.83it/s]\rTesting business_ethics:  48%|████▊     | 48/100 [00:29<00:28,  1.83it/s]\rTesting astronomy:  32%|███▏      | 48/152 [00:29<00:56,  1.83it/s]\rTesting business_ethics:  49%|████▉     | 49/100 [00:29<00:27,  1.86it/s]\rTesting astronomy:  32%|███▏      | 49/152 [00:29<00:55,  1.84it/s]\rTesting business_ethics:  50%|█████     | 50/100 [00:30<00:27,  1.84it/s]\rTesting astronomy:  33%|███▎      | 50/152 [00:30<00:56,  1.80it/s]\rTesting business_ethics:  51%|█████     | 51/100 [00:30<00:26,  1.83it/s]\rTesting astronomy:  34%|███▎      | 51/152 [00:31<00:55,  1.82it/s]\rTesting business_ethics:  52%|█████▏    | 52/100 [00:31<00:26,  1.82it/s]\rTesting astronomy:  34%|███▍      | 52/152 [00:31<00:54,  1.84it/s]\rTesting business_ethics:  53%|█████▎    | 53/100 [00:32<00:25,  1.83it/s]\rTesting astronomy:  35%|███▍      | 53/152 [00:32<00:53,  1.86it/s]\rTesting business_ethics:  54%|█████▍    | 54/100 [00:32<00:25,  1.83it/s]\rTesting astronomy:  36%|███▌      | 54/152 [00:32<00:52,  1.87it/s]\rTesting business_ethics:  55%|█████▌    | 55/100 [00:33<00:24,  1.83it/s]\rTesting astronomy:  36%|███▌      | 55/152 [00:33<00:51,  1.90it/s]\rTesting business_ethics:  56%|█████▌    | 56/100 [00:33<00:27,  1.63it/s]\rTesting astronomy:  37%|███▋      | 56/152 [00:34<01:02,  1.53it/s]\rTesting business_ethics:  57%|█████▋    | 57/100 [00:34<00:32,  1.33it/s]\rTesting astronomy:  38%|███▊      | 57/152 [00:34<01:08,  1.38it/s]\rTesting business_ethics:  58%|█████▊    | 58/100 [00:35<00:33,  1.26it/s]\rTesting astronomy:  38%|███▊      | 58/152 [00:35<01:12,  1.29it/s]\rTesting business_ethics:  59%|█████▉    | 59/100 [00:36<00:34,  1.18it/s]\rTesting astronomy:  39%|███▉      | 59/152 [00:36<01:17,  1.20it/s]\rTesting business_ethics:  60%|██████    | 60/100 [00:37<00:30,  1.30it/s]\rTesting astronomy:  39%|███▉      | 60/152 [00:37<01:09,  1.32it/s]\rTesting business_ethics:  61%|██████    | 61/100 [00:37<00:27,  1.42it/s]\rTesting astronomy:  40%|████      | 61/152 [00:38<01:04,  1.42it/s]\rTesting business_ethics:  62%|██████▏   | 62/100 [00:38<00:24,  1.54it/s]\rTesting astronomy:  41%|████      | 62/152 [00:38<00:58,  1.53it/s]\rTesting business_ethics:  63%|██████▎   | 63/100 [00:39<00:22,  1.62it/s]\rTesting astronomy:  41%|████▏     | 63/152 [00:39<00:55,  1.59it/s]\rTesting business_ethics:  64%|██████▍   | 64/100 [00:39<00:21,  1.69it/s]\rTesting astronomy:  42%|████▏     | 64/152 [00:39<00:52,  1.68it/s]\rTesting business_ethics:  65%|██████▌   | 65/100 [00:40<00:20,  1.72it/s]\rTesting astronomy:  43%|████▎     | 65/152 [00:40<00:50,  1.71it/s]\rTesting business_ethics:  66%|██████▌   | 66/100 [00:40<00:19,  1.75it/s]\rTesting astronomy:  43%|████▎     | 66/152 [00:40<00:48,  1.77it/s]\rTesting business_ethics:  67%|██████▋   | 67/100 [00:41<00:18,  1.80it/s]\rTesting astronomy:  44%|████▍     | 67/152 [00:41<00:48,  1.75it/s]\rTesting business_ethics:  68%|██████▊   | 68/100 [00:41<00:17,  1.82it/s]\rTesting astronomy:  45%|████▍     | 68/152 [00:41<00:47,  1.78it/s]\rTesting business_ethics:  69%|██████▉   | 69/100 [00:42<00:17,  1.82it/s]\rTesting astronomy:  45%|████▌     | 69/152 [00:42<00:46,  1.80it/s]\rTesting business_ethics:  70%|███████   | 70/100 [00:42<00:16,  1.85it/s]\rTesting astronomy:  46%|████▌     | 70/152 [00:42<00:44,  1.83it/s]\rTesting business_ethics:  71%|███████   | 71/100 [00:43<00:15,  1.83it/s]\rTesting astronomy:  47%|████▋     | 71/152 [00:43<00:43,  1.85it/s]\rTesting business_ethics:  72%|███████▏  | 72/100 [00:43<00:15,  1.84it/s]\rTesting astronomy:  47%|████▋     | 72/152 [00:43<00:43,  1.83it/s]\rTesting business_ethics:  73%|███████▎  | 73/100 [00:44<00:14,  1.81it/s]\rTesting astronomy:  48%|████▊     | 73/152 [00:44<00:42,  1.85it/s]\rTesting business_ethics:  74%|███████▍  | 74/100 [00:45<00:14,  1.83it/s]\rTesting astronomy:  49%|████▊     | 74/152 [00:45<00:42,  1.85it/s]\rTesting business_ethics:  75%|███████▌  | 75/100 [00:45<00:13,  1.84it/s]\rTesting astronomy:  49%|████▉     | 75/152 [00:45<00:41,  1.87it/s]\rTesting business_ethics:  76%|███████▌  | 76/100 [00:46<00:13,  1.81it/s]\rTesting astronomy:  50%|█████     | 76/152 [00:46<00:41,  1.85it/s]\rTesting business_ethics:  77%|███████▋  | 77/100 [00:46<00:12,  1.85it/s]\rTesting astronomy:  51%|█████     | 77/152 [00:46<00:40,  1.85it/s]\rTesting business_ethics:  78%|███████▊  | 78/100 [00:47<00:12,  1.78it/s]\rTesting astronomy:  51%|█████▏    | 78/152 [00:47<00:42,  1.73it/s]\rTesting business_ethics:  79%|███████▉  | 79/100 [00:48<00:13,  1.52it/s]\rTesting astronomy:  52%|█████▏    | 79/152 [00:48<00:51,  1.41it/s]\rTesting business_ethics:  80%|████████  | 80/100 [00:49<00:14,  1.37it/s]\rTesting astronomy:  53%|█████▎    | 80/152 [00:49<00:56,  1.27it/s]\rTesting business_ethics:  81%|████████  | 81/100 [00:49<00:14,  1.34it/s]\rTesting astronomy:  53%|█████▎    | 81/152 [00:50<00:58,  1.22it/s]\rTesting business_ethics:  82%|████████▏ | 82/100 [00:50<00:13,  1.34it/s]\rTesting astronomy:  54%|█████▍    | 82/152 [00:51<00:59,  1.18it/s]\rTesting business_ethics:  83%|████████▎ | 83/100 [00:51<00:12,  1.33it/s]\rTesting astronomy:  55%|█████▍    | 83/152 [00:51<00:52,  1.32it/s]\rTesting business_ethics:  84%|████████▍ | 84/100 [00:51<00:11,  1.44it/s]\rTesting astronomy:  55%|█████▌    | 84/152 [00:52<00:47,  1.43it/s]\rTesting business_ethics:  85%|████████▌ | 85/100 [00:52<00:09,  1.55it/s]\rTesting astronomy:  56%|█████▌    | 85/152 [00:52<00:43,  1.54it/s]\rTesting business_ethics:  86%|████████▌ | 86/100 [00:52<00:08,  1.62it/s]\rTesting astronomy:  57%|█████▋    | 86/152 [00:53<00:40,  1.62it/s]\rTesting business_ethics:  87%|████████▋ | 87/100 [00:53<00:07,  1.65it/s]\rTesting astronomy:  57%|█████▋    | 87/152 [00:53<00:38,  1.69it/s]\rTesting business_ethics:  88%|████████▊ | 88/100 [00:54<00:07,  1.68it/s]\rTesting astronomy:  58%|█████▊    | 88/152 [00:54<00:37,  1.71it/s]\rTesting business_ethics:  89%|████████▉ | 89/100 [00:54<00:06,  1.69it/s]\rTesting astronomy:  59%|█████▊    | 89/152 [00:55<00:36,  1.71it/s]\rTesting business_ethics:  90%|█████████ | 90/100 [00:55<00:05,  1.73it/s]\rTesting astronomy:  59%|█████▉    | 90/152 [00:55<00:34,  1.77it/s]\rTesting business_ethics:  91%|█████████ | 91/100 [00:55<00:05,  1.75it/s]\rTesting astronomy:  60%|█████▉    | 91/152 [00:56<00:34,  1.77it/s]\rTesting business_ethics:  92%|█████████▏| 92/100 [00:56<00:04,  1.78it/s]\rTesting astronomy:  61%|██████    | 92/152 [00:56<00:33,  1.79it/s]\rTesting business_ethics:  93%|█████████▎| 93/100 [00:56<00:03,  1.81it/s]\rTesting astronomy:  61%|██████    | 93/152 [00:57<00:32,  1.80it/s]\rTesting business_ethics:  94%|█████████▍| 94/100 [00:57<00:03,  1.84it/s]\rTesting astronomy:  62%|██████▏   | 94/152 [00:57<00:32,  1.78it/s]\rTesting business_ethics:  95%|█████████▌| 95/100 [00:57<00:02,  1.84it/s]\rTesting astronomy:  62%|██████▎   | 95/152 [00:58<00:31,  1.80it/s]\rTesting business_ethics:  96%|█████████▌| 96/100 [00:58<00:02,  1.83it/s]\rTesting astronomy:  63%|██████▎   | 96/152 [00:58<00:30,  1.82it/s]\rTesting business_ethics:  97%|█████████▋| 97/100 [00:59<00:01,  1.80it/s]\rTesting astronomy:  64%|██████▍   | 97/152 [00:59<00:29,  1.83it/s]\rTesting business_ethics:  98%|█████████▊| 98/100 [00:59<00:01,  1.83it/s]\rTesting astronomy:  64%|██████▍   | 98/152 [00:59<00:29,  1.84it/s]\rTesting business_ethics:  99%|█████████▉| 99/100 [01:00<00:00,  1.82it/s]\rTesting astronomy:  65%|██████▌   | 99/152 [01:00<00:28,  1.84it/s]\rTesting business_ethics: 100%|██████████| 100/100 [01:00<00:00,  1.83it/s]\rTesting business_ethics: 100%|██████████| 100/100 [01:00<00:00,  1.65it/s]\n",
            "\rTesting astronomy:  66%|██████▌   | 100/152 [01:00<00:27,  1.87it/s]\rTesting astronomy:  66%|██████▋   | 101/152 [01:01<00:25,  1.97it/s]\rTesting astronomy:  67%|██████▋   | 102/152 [01:01<00:25,  1.93it/s]\rTesting astronomy:  68%|██████▊   | 103/152 [01:02<00:26,  1.88it/s]\rTesting astronomy:  68%|██████▊   | 104/152 [01:03<00:25,  1.89it/s]\rTesting astronomy:  69%|██████▉   | 105/152 [01:03<00:24,  1.89it/s]\rTesting astronomy:  70%|██████▉   | 106/152 [01:04<00:24,  1.88it/s]\rTesting astronomy:  70%|███████   | 107/152 [01:04<00:22,  2.03it/s]\rTesting astronomy:  71%|███████   | 108/152 [01:04<00:18,  2.35it/s]\rTesting astronomy:  72%|███████▏  | 109/152 [01:05<00:16,  2.62it/s]\rTesting astronomy:  72%|███████▏  | 110/152 [01:05<00:14,  2.81it/s]\rTesting astronomy:  73%|███████▎  | 111/152 [01:05<00:13,  3.02it/s]\rTesting astronomy:  74%|███████▎  | 112/152 [01:05<00:12,  3.17it/s]\rTesting astronomy:  74%|███████▍  | 113/152 [01:06<00:12,  3.24it/s]\rTesting astronomy:  75%|███████▌  | 114/152 [01:06<00:11,  3.32it/s]\rTesting astronomy:  76%|███████▌  | 115/152 [01:06<00:10,  3.37it/s]\rTesting astronomy:  76%|███████▋  | 116/152 [01:07<00:10,  3.43it/s]\rTesting astronomy:  77%|███████▋  | 117/152 [01:07<00:10,  3.34it/s]\rTesting astronomy:  78%|███████▊  | 118/152 [01:07<00:10,  3.39it/s]\rTesting astronomy:  78%|███████▊  | 119/152 [01:07<00:09,  3.45it/s]\rTesting astronomy:  79%|███████▉  | 120/152 [01:08<00:09,  3.47it/s]\rTesting astronomy:  80%|███████▉  | 121/152 [01:08<00:08,  3.45it/s]\rTesting astronomy:  80%|████████  | 122/152 [01:08<00:08,  3.54it/s]\rTesting astronomy:  81%|████████  | 123/152 [01:09<00:08,  3.53it/s]\rTesting astronomy:  82%|████████▏ | 124/152 [01:09<00:08,  3.50it/s]\rTesting astronomy:  82%|████████▏ | 125/152 [01:09<00:07,  3.54it/s]\rTesting astronomy:  83%|████████▎ | 126/152 [01:09<00:07,  3.56it/s]\rTesting astronomy:  84%|████████▎ | 127/152 [01:10<00:07,  3.53it/s]\rTesting astronomy:  84%|████████▍ | 128/152 [01:10<00:06,  3.52it/s]\rTesting astronomy:  85%|████████▍ | 129/152 [01:10<00:06,  3.57it/s]\rTesting astronomy:  86%|████████▌ | 130/152 [01:11<00:06,  3.53it/s]\rTesting astronomy:  86%|████████▌ | 131/152 [01:11<00:05,  3.50it/s]\rTesting astronomy:  87%|████████▋ | 132/152 [01:11<00:05,  3.52it/s]\rTesting astronomy:  88%|████████▊ | 133/152 [01:11<00:05,  3.50it/s]\rTesting astronomy:  88%|████████▊ | 134/152 [01:12<00:05,  3.55it/s]\rTesting astronomy:  89%|████████▉ | 135/152 [01:12<00:04,  3.53it/s]\rTesting astronomy:  89%|████████▉ | 136/152 [01:12<00:04,  3.55it/s]\rTesting astronomy:  90%|█████████ | 137/152 [01:13<00:04,  3.59it/s]\rTesting astronomy:  91%|█████████ | 138/152 [01:13<00:03,  3.67it/s]\rTesting astronomy:  91%|█████████▏| 139/152 [01:13<00:03,  3.62it/s]\rTesting astronomy:  92%|█████████▏| 140/152 [01:13<00:03,  3.62it/s]\rTesting astronomy:  93%|█████████▎| 141/152 [01:14<00:03,  3.64it/s]\rTesting astronomy:  93%|█████████▎| 142/152 [01:14<00:02,  3.63it/s]\rTesting astronomy:  94%|█████████▍| 143/152 [01:14<00:03,  2.86it/s]\rTesting astronomy:  95%|█████████▍| 144/152 [01:15<00:03,  2.50it/s]\rTesting astronomy:  95%|█████████▌| 145/152 [01:15<00:03,  2.27it/s]\rTesting astronomy:  96%|█████████▌| 146/152 [01:16<00:02,  2.13it/s]\rTesting astronomy:  97%|█████████▋| 147/152 [01:17<00:02,  2.03it/s]\rTesting astronomy:  97%|█████████▋| 148/152 [01:17<00:01,  2.00it/s]\rTesting astronomy:  98%|█████████▊| 149/152 [01:17<00:01,  2.23it/s]\rTesting astronomy:  99%|█████████▊| 150/152 [01:18<00:00,  2.51it/s]\rTesting astronomy:  99%|█████████▉| 151/152 [01:18<00:00,  2.79it/s]\rTesting astronomy: 100%|██████████| 152/152 [01:18<00:00,  2.96it/s]\rTesting astronomy: 100%|██████████| 152/152 [01:18<00:00,  1.93it/s]\n",
            "\n",
            "real\t1m22.489s\n",
            "user\t0m5.087s\n",
            "sys\t0m0.539s\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "time { python llama_mmlu_astronomy_ollama.py & python llama_mmlu_business_ollama.py & wait; }"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07b2fed01a53443692c798ecad6ece12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f158288593f402f9ce99c51a591a953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_99af52b5af6c4755b907fab497de90ed",
            "style": "IPY_MODEL_7fdf2ef5da2e4de3803cb3b584ee7106",
            "value": true
          }
        },
        "16b31a1ac7674eb7809f63232b0c6091": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e86a11951174f84929e1432daad2928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4071ade917e4a0897f4da864cb0f8c3",
            "placeholder": "​",
            "style": "IPY_MODEL_7c47df64a0f848fc8e77c878d893978c",
            "value": "Connecting..."
          }
        },
        "3bc0488c421b4afe92bb9b65c8ab9801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aeff9fd2ba3433fbaacd7058b1857e1",
            "placeholder": "​",
            "style": "IPY_MODEL_7a68be99065946f199caf872656bd08b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "5aeff9fd2ba3433fbaacd7058b1857e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a03c57e863b4621b9711f90ca47e606": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c47fcb666f2549fcaf4af051ea626175",
            "style": "IPY_MODEL_7a31506654d2491fac1f8488824cf6cc",
            "tooltip": ""
          }
        },
        "732a592584b043f7a157ae8672b5ed1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7a31506654d2491fac1f8488824cf6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7a68be99065946f199caf872656bd08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c47df64a0f848fc8e77c878d893978c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fdf2ef5da2e4de3803cb3b584ee7106": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "946f4fa4fdaf420398fd0a6a62872292": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99af52b5af6c4755b907fab497de90ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a220c6f29b1b4107bbfefe96df36aff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9702b8a175e481bb942e73b4049887d",
            "placeholder": "​",
            "style": "IPY_MODEL_16b31a1ac7674eb7809f63232b0c6091",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "a4071ade917e4a0897f4da864cb0f8c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba8f5ac3478f4dd1bd83fbfef0d3fb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_946f4fa4fdaf420398fd0a6a62872292",
            "placeholder": "​",
            "style": "IPY_MODEL_07b2fed01a53443692c798ecad6ece12",
            "value": ""
          }
        },
        "c47fcb666f2549fcaf4af051ea626175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9702b8a175e481bb942e73b4049887d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29e39165d6a4df291e162acbc9fb50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_732a592584b043f7a157ae8672b5ed1b"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
