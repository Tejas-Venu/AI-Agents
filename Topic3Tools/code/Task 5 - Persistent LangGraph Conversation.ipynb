{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE2TRYUpBpbk"
      },
      "source": [
        "Task 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-jdOCjfrxXG",
        "outputId": "0f7ce342-a594-4b9f-bd57-43df31278d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph.checkpoint.sqlite\n",
            "  Downloading langgraph_checkpoint_sqlite-3.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: aiosqlite>=0.20 in /usr/local/lib/python3.12/dist-packages (from langgraph.checkpoint.sqlite) (0.22.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=3 in /usr/local/lib/python3.12/dist-packages (from langgraph.checkpoint.sqlite) (4.0.0)\n",
            "Collecting sqlite-vec>=0.1.6 (from langgraph.checkpoint.sqlite)\n",
            "  Downloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl.metadata (198 bytes)\n",
            "Requirement already satisfied: langchain-core>=0.2.38 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (1.2.13)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (1.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (2.32.4)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<5.0.0,>=3->langgraph.checkpoint.sqlite) (2.5.0)\n",
            "Downloading langgraph_checkpoint_sqlite-3.0.3-py3-none-any.whl (33 kB)\n",
            "Downloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlite-vec, langgraph.checkpoint.sqlite\n",
            "Successfully installed langgraph.checkpoint.sqlite-3.0.3 sqlite-vec-0.1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph.checkpoint.sqlite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9bU8IMhV3i4",
        "outputId": "cbf2dcbd-4586-4353-907f-d33569d66858"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:373: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:373: SyntaxWarning: invalid escape sequence '\\('\n",
            "/tmp/ipython-input-4057578653.py:373: SyntaxWarning: invalid escape sequence '\\('\n",
            "  - Removes LaTeX inline math delimiters \\( \\) and $ $\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Starting conversation (LangGraph persistent + auto-execute pending tools on resume)...\n",
            "================================================== \n",
            "\n",
            "Graph created successfully!\n",
            "\n",
            "Saving graph visualization...\n",
            "Graph image saved to lg_graph.png\n",
            "Starting new workflow...\n",
            "\n",
            "==================================================\n",
            "Enter your text (or 'quit' to exit):\n",
            "==================================================\n",
            "\n",
            "> What is (2*3)+ 5?\n",
            "Assistant: The result of (2*3) + 5 is 11.\n",
            "\n",
            "==================================================\n",
            "Enter your text (or 'quit' to exit):\n",
            "==================================================\n",
            "\n",
            "> What is sin(pi/2)?\n",
            "Assistant: The result of sin(pi/2) is 1.0.\n",
            "\n",
            "==================================================\n",
            "Enter your text (or 'quit' to exit):\n",
            "==================================================\n",
            "\n",
            "> quit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# final_agent_graph_auto_execute_on_resume_with_clean_display.py\n",
        "import json\n",
        "import ast\n",
        "import math\n",
        "import operator as op\n",
        "import traceback\n",
        "import inspect\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from typing import Any, TypedDict, List, Dict, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "try:\n",
        "    import openai\n",
        "    RateLimitExc = getattr(openai.error, \"RateLimitError\", None)\n",
        "except Exception:\n",
        "    openai = None\n",
        "    RateLimitExc = None\n",
        "\n",
        "_ALLOWED_OPERATORS = {\n",
        "    ast.Add: op.add,\n",
        "    ast.Sub: op.sub,\n",
        "    ast.Mult: op.mul,\n",
        "    ast.Div: op.truediv,\n",
        "    ast.Pow: op.pow,\n",
        "    ast.USub: op.neg,\n",
        "    ast.UAdd: op.pos,\n",
        "    ast.Mod: op.mod,\n",
        "    ast.FloorDiv: op.floordiv,\n",
        "}\n",
        "_ALLOWED_NAMES = {k: getattr(math, k) for k in dir(math) if not k.startswith(\"_\")}\n",
        "_ALLOWED_NAMES.update({\"pi\": math.pi, \"e\": math.e})\n",
        "\n",
        "def safe_eval_expr(expr: str):\n",
        "    def _eval(node):\n",
        "        if isinstance(node, ast.Constant):\n",
        "            if isinstance(node.value, (int, float)):\n",
        "                return node.value\n",
        "            raise ValueError(\"Unsupported constant type\")\n",
        "        if node.__class__.__name__ == \"Num\":  # legacy\n",
        "            val = getattr(node, \"n\", None)\n",
        "            if isinstance(val, (int, float)):\n",
        "                return val\n",
        "            raise ValueError(\"Unsupported legacy numeric literal\")\n",
        "        if isinstance(node, ast.BinOp):\n",
        "            op_type = type(node.op)\n",
        "            if op_type not in _ALLOWED_OPERATORS:\n",
        "                raise ValueError(f\"Operator {op_type} not allowed\")\n",
        "            left = _eval(node.left)\n",
        "            right = _eval(node.right)\n",
        "            return _ALLOWED_OPERATORS[op_type](left, right)\n",
        "        if isinstance(node, ast.UnaryOp):\n",
        "            op_type = type(node.op)\n",
        "            if op_type not in _ALLOWED_OPERATORS:\n",
        "                raise ValueError(f\"Unary operator {op_type} not allowed\")\n",
        "            return _ALLOWED_OPERATORS[op_type](_eval(node.operand))\n",
        "        if isinstance(node, ast.Call):\n",
        "            if not isinstance(node.func, ast.Name):\n",
        "                raise ValueError(\"Only simple function calls allowed\")\n",
        "            fname = node.func.id\n",
        "            if fname not in _ALLOWED_NAMES:\n",
        "                raise ValueError(f\"Function {fname} not allowed\")\n",
        "            f = _ALLOWED_NAMES[fname]\n",
        "            args = [_eval(a) for a in node.args]\n",
        "            return f(*args)\n",
        "        if isinstance(node, ast.Name):\n",
        "            name = node.id\n",
        "            if name in _ALLOWED_NAMES:\n",
        "                return _ALLOWED_NAMES[name]\n",
        "            raise ValueError(f\"Name {name} not allowed\")\n",
        "        raise ValueError(f\"Unsupported AST node: {type(node)}\")\n",
        "    parsed = ast.parse(expr, mode=\"eval\")\n",
        "    return _eval(parsed.body)\n",
        "\n",
        "@tool(description=\"Get the current weather for a single location (simulated).\")\n",
        "def get_weather(location: str) -> str:\n",
        "    weather_data = {\n",
        "        \"San Francisco\": \"Sunny, 72°F\",\n",
        "        \"New York\": \"Cloudy, 55°F\",\n",
        "        \"London\": \"Rainy, 48°F\",\n",
        "        \"Tokyo\": \"Clear, 65°F\",\n",
        "    }\n",
        "    return weather_data.get(location, f\"Weather data not available for {location}\")\n",
        "\n",
        "@tool(description=\"Calculator: accepts a JSON string or raw expression and returns a JSON string with the result.\")\n",
        "def calculator(raw: str) -> str:\n",
        "    try:\n",
        "        try:\n",
        "            data = json.loads(raw)\n",
        "        except Exception:\n",
        "            data = {\"expr\": raw}\n",
        "        if isinstance(data, dict) and \"raw\" in data and isinstance(data[\"raw\"], str):\n",
        "            try:\n",
        "                nested = json.loads(data[\"raw\"])\n",
        "                data = nested\n",
        "            except Exception:\n",
        "                data = {\"expr\": data[\"raw\"]}\n",
        "        if not isinstance(data, dict):\n",
        "            return json.dumps({\"error\":\"unexpected_payload\",\"payload\":str(data)})\n",
        "        if \"expr\" in data:\n",
        "            expr = str(data[\"expr\"])\n",
        "            val = safe_eval_expr(expr)\n",
        "            return json.dumps({\"result\": val, \"expression\": expr})\n",
        "        op_name = str(data.get(\"operation\",\"\")).lower()\n",
        "        if op_name in (\"add\",\"sum\"):\n",
        "            a = float(data[\"a\"]); b = float(data[\"b\"]); return json.dumps({\"result\": a + b})\n",
        "        if op_name in (\"sub\",\"subtract\",\"minus\"):\n",
        "            a = float(data[\"a\"]); b = float(data[\"b\"]); return json.dumps({\"result\": a - b})\n",
        "        if op_name in (\"mul\",\"multiply\"):\n",
        "            a = float(data[\"a\"]); b = float(data[\"b\"]); return json.dumps({\"result\": a * b})\n",
        "        if op_name in (\"div\",\"divide\"):\n",
        "            a = float(data[\"a\"]); b = float(data[\"b\"])\n",
        "            if b == 0: return json.dumps({\"error\":\"division_by_zero\"})\n",
        "            return json.dumps({\"result\": a / b})\n",
        "        if op_name in (\"sin\",\"cos\",\"tan\",\"sqrt\",\"log\",\"ln\",\"exp\"):\n",
        "            x_expr = data.get(\"x\")\n",
        "            if x_expr is None: return json.dumps({\"error\":\"missing_argument\",\"message\": \"x required\"})\n",
        "            x_val = safe_eval_expr(str(x_expr))\n",
        "            if op_name == \"sin\": return json.dumps({\"result\": math.sin(x_val)})\n",
        "            if op_name == \"cos\": return json.dumps({\"result\": math.cos(x_val)})\n",
        "            if op_name == \"tan\": return json.dumps({\"result\": math.tan(x_val)})\n",
        "            if op_name == \"sqrt\": return json.dumps({\"result\": math.sqrt(x_val)})\n",
        "            if op_name in (\"log\",\"ln\"): return json.dumps({\"result\": math.log(x_val)})\n",
        "            if op_name == \"exp\": return json.dumps({\"result\": math.exp(x_val)})\n",
        "        if op_name == \"area_circle\":\n",
        "            r = float(data[\"radius\"]); return json.dumps({\"result\": math.pi * r * r})\n",
        "        if op_name == \"circumference\":\n",
        "            r = float(data[\"radius\"]); return json.dumps({\"result\": 2 * math.pi * r})\n",
        "        if op_name == \"area_rectangle\":\n",
        "            w = float(data[\"width\"]); h = float(data[\"height\"]); return json.dumps({\"result\": w * h})\n",
        "        if op_name == \"perimeter_rectangle\":\n",
        "            w = float(data[\"width\"]); h = float(data[\"height\"]); return json.dumps({\"result\": 2 * (w + h)})\n",
        "        if op_name == \"area_triangle\":\n",
        "            b = float(data[\"base\"]); h = float(data[\"height\"]); return json.dumps({\"result\": 0.5 * b * h})\n",
        "        return json.dumps({\"error\":\"unknown_operation\",\"input\":data})\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\":\"execution_error\",\"message\": str(e)})\n",
        "\n",
        "@tool(description=\"Count occurrences of a letter in given text (case-insensitive).\")\n",
        "def count_letter(text: str, letter: str) -> str:\n",
        "    if not isinstance(text, str) or not isinstance(letter, str) or len(letter) == 0:\n",
        "        return \"0\"\n",
        "    return str(text.lower().count(letter.lower()))\n",
        "\n",
        "@tool(description=\"Return simple text statistics in JSON (length, words_count, vowels_count, unique_words).\")\n",
        "def text_stats(text: str) -> str:\n",
        "    s = text or \"\"\n",
        "    words = [w for w in (s.strip().split()) if w]\n",
        "    vowels = sum(1 for ch in s.lower() if ch in \"aeiou\")\n",
        "    unique = len(set(w.strip(\".,!?;:\\\"'()[]{}\").lower() for w in words if w))\n",
        "    out = {\"length\": len(s), \"words_count\": len(words), \"vowels_count\": vowels, \"unique_words\": unique}\n",
        "    return json.dumps(out)\n",
        "\n",
        "def normalize_args_obj(function_args: Any, tool_name: str = None):\n",
        "    if isinstance(function_args, str):\n",
        "        try:\n",
        "            return json.loads(function_args)\n",
        "        except Exception:\n",
        "            return function_args\n",
        "    elif isinstance(function_args, dict):\n",
        "        parsed = {}\n",
        "        for k, v in function_args.items():\n",
        "            if isinstance(v, str):\n",
        "                try:\n",
        "                    parsed[k] = json.loads(v)\n",
        "                except json.JSONDecodeError:\n",
        "                    parsed[k] = v\n",
        "            else:\n",
        "                parsed[k] = v\n",
        "        if \"text\" in parsed and isinstance(parsed[\"text\"], dict) and \"letter\" in parsed[\"text\"] and \"letter\" not in parsed:\n",
        "            inner_args = parsed.pop(\"text\")\n",
        "            return {**inner_args, **parsed}\n",
        "        return parsed\n",
        "    return function_args\n",
        "\n",
        "def invoke_tool(tool_obj, function_args, tool_name=None):\n",
        "    args_obj = normalize_args_obj(function_args, tool_name=tool_name)\n",
        "    errors = []\n",
        "    def rec(e):\n",
        "        errors.append(traceback.format_exc())\n",
        "    try:\n",
        "        if hasattr(tool_obj, \"run\"):\n",
        "            try:\n",
        "                return tool_obj.run(args_obj)\n",
        "            except TypeError:\n",
        "                try:\n",
        "                    return tool_obj.run(json.dumps(args_obj))\n",
        "                except Exception as e:\n",
        "                    rec(e)\n",
        "            except Exception as e:\n",
        "                rec(e)\n",
        "    except Exception as e:\n",
        "        rec(e)\n",
        "    try:\n",
        "        if hasattr(tool_obj, \"invoke\"):\n",
        "            try:\n",
        "                return tool_obj.invoke(args_obj)\n",
        "            except TypeError:\n",
        "                try:\n",
        "                    return tool_obj.invoke(json.dumps(args_obj))\n",
        "                except Exception as e:\n",
        "                    rec(e)\n",
        "            except Exception as e:\n",
        "                rec(e)\n",
        "    except Exception as e:\n",
        "        rec(e)\n",
        "    try:\n",
        "        if callable(tool_obj):\n",
        "            try:\n",
        "                if isinstance(args_obj, dict):\n",
        "                    return tool_obj(**args_obj)\n",
        "                return tool_obj(args_obj)\n",
        "            except TypeError:\n",
        "                try:\n",
        "                    return tool_obj(json.dumps(args_obj))\n",
        "                except Exception as e:\n",
        "                    rec(e)\n",
        "            except Exception as e:\n",
        "                rec(e)\n",
        "    except Exception as e:\n",
        "        rec(e)\n",
        "    try:\n",
        "        sig = None\n",
        "        try:\n",
        "            sig = inspect.signature(tool_obj)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if sig is not None and isinstance(args_obj, dict):\n",
        "            params = sig.parameters\n",
        "            call_kwargs = {k: v for k, v in args_obj.items() if k in params}\n",
        "            try:\n",
        "                return tool_obj(**call_kwargs)\n",
        "            except Exception as e:\n",
        "                rec(e)\n",
        "    except Exception as e:\n",
        "        rec(e)\n",
        "    return \"[invoke_tool_failed] Could not call tool. Attempts:\\n\" + \"\\n\\n\".join(errors or [\"no details\"])\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "TOOLS = [get_weather, calculator, count_letter, text_stats]\n",
        "tool_map = {}\n",
        "for t in TOOLS:\n",
        "    name = getattr(t, \"name\", None) or getattr(t, \"__name__\", None) or getattr(t, \"__qualname__\", None)\n",
        "    tool_map[name] = t\n",
        "llm_with_tools = llm.bind_tools(TOOLS)\n",
        "\n",
        "SYSTEM_PROMPT_TEXT = (\n",
        "    \"You are a helpful assistant that uses tools. If there is a tool available to perform a calculation, use that tool. \"\n",
        "    \"Do not attempt to do the calculation on your own. When calling calculator, pass a JSON string (e.g. '{\\\"expr\\\":\\\"2+2\\\"}').\"\n",
        ")\n",
        "\n",
        "def safe_invoke_llm(model_callable, message_objs, max_retries: int = 6, base_delay: float = 0.5):\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            return model_callable(message_objs)\n",
        "        except Exception as e:\n",
        "            attempt += 1\n",
        "            text = str(e)\n",
        "            is_rate_limit = False\n",
        "            if RateLimitExc is not None and isinstance(e, RateLimitExc):\n",
        "                is_rate_limit = True\n",
        "            if not is_rate_limit:\n",
        "                if \"RateLimit\" in text or \"rate limit\" in text.lower() or \"429\" in text:\n",
        "                    is_rate_limit = True\n",
        "            if not is_rate_limit:\n",
        "                raise\n",
        "            if attempt > max_retries:\n",
        "                raise RuntimeError(f\"LLM request failed after {max_retries} retries due to rate limits. Last error: {e}\") from e\n",
        "            retry_after = None\n",
        "            try:\n",
        "                if hasattr(e, \"response\") and getattr(e.response, \"headers\", None):\n",
        "                    headers = e.response.headers\n",
        "                    retry_after = headers.get(\"retry-after\") or headers.get(\"Retry-After\")\n",
        "                    if retry_after is not None:\n",
        "                        retry_after = float(retry_after)\n",
        "            except Exception:\n",
        "                retry_after = None\n",
        "            if retry_after is not None:\n",
        "                sleep_for = max(retry_after, base_delay * (2 ** (attempt - 1)))\n",
        "            else:\n",
        "                sleep_for = base_delay * (2 ** (attempt - 1))\n",
        "            jitter = random.uniform(0.8, 1.2)\n",
        "            sleep_for = float(sleep_for) * jitter\n",
        "            print(f\"[RATE LIMIT] attempt {attempt}/{max_retries}. sleeping {sleep_for:.2f}s before retrying...\")\n",
        "            time.sleep(sleep_for)\n",
        "\n",
        "\n",
        "def serialize_message(msg) -> Dict[str, Any]:\n",
        "    if msg is None: return {}\n",
        "    if isinstance(msg, dict): return dict(msg)\n",
        "    if isinstance(msg, HumanMessage): return {\"role\":\"human\", \"content\": getattr(msg,\"content\",\"\")}\n",
        "    if isinstance(msg, SystemMessage): return {\"role\":\"system\", \"content\": getattr(msg,\"content\",\"\")}\n",
        "    if isinstance(msg, ToolMessage): return {\"role\":\"tool\", \"content\": getattr(msg,\"content\",\"\"), \"tool_call_id\": getattr(msg,\"tool_call_id\", None)}\n",
        "    content = getattr(msg,\"content\", None) or getattr(msg,\"text\", None) or str(msg)\n",
        "    tool_calls = getattr(msg,\"tool_calls\", None)\n",
        "    serialized_tool_calls = None\n",
        "    if tool_calls:\n",
        "        calls_list = []\n",
        "        for tc in tool_calls:\n",
        "            if isinstance(tc, dict):\n",
        "                calls_list.append({\"name\": tc.get(\"name\"), \"args\": tc.get(\"args\"), \"id\": tc.get(\"id\")})\n",
        "            else:\n",
        "                calls_list.append({\"name\": getattr(tc,\"name\",None), \"args\": getattr(tc,\"args\",None), \"id\": getattr(tc,\"id\",None)})\n",
        "        serialized_tool_calls = calls_list\n",
        "    out = {\"role\":\"assistant\", \"content\": content}\n",
        "    if serialized_tool_calls is not None: out[\"tool_calls\"] = serialized_tool_calls\n",
        "    return out\n",
        "\n",
        "def deserialize_messages_for_model(serialized: List[Dict[str, Any]]) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Build model payload.\n",
        "    - system/human -> direct mapping\n",
        "    - assistant -> SystemMessage (preserve assistant content)\n",
        "    - tool -> SystemMessage containing the tool output (so the LLM sees tool results)\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for m in serialized:\n",
        "        role = m.get(\"role\")\n",
        "        content = m.get(\"content\",\"\")\n",
        "        if role == \"system\":\n",
        "            out.append(SystemMessage(content=content))\n",
        "        elif role == \"human\":\n",
        "            out.append(HumanMessage(content=content))\n",
        "        elif role == \"assistant\":\n",
        "            out.append(SystemMessage(content=content))\n",
        "        elif role == \"tool\":\n",
        "            out.append(SystemMessage(content=f\"[tool_result] {content}\"))\n",
        "        else:\n",
        "            continue\n",
        "    return out\n",
        "\n",
        "def clean_assistant_content(s: str) -> str:\n",
        "    \"\"\"Convert simple LaTeX fragments to readable plain text for terminal printing.\n",
        "\n",
        "    - Removes LaTeX inline math delimiters \\( \\) and $ $\n",
        "    - Converts \\frac{a}{b} -> (a/b)\n",
        "    - Converts common macros: \\pi -> π, \\sin -> sin, \\cos -> cos, \\tan -> tan\n",
        "    - Removes stray backslashes otherwise.\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return s\n",
        "\n",
        "    out = s\n",
        "\n",
        "    out = re.sub(r'\\\\\\(|\\\\\\)|\\\\\\\\\\(|\\\\\\\\\\)', '', out)\n",
        "    out = re.sub(r'\\$(.*?)\\$', r'\\1', out)\n",
        "    def _frac_to_div(m):\n",
        "        a = m.group(1)\n",
        "        b = m.group(2)\n",
        "        return f\"({a}/{b})\"\n",
        "    out = re.sub(r'\\\\frac\\{([^{}]+)\\}\\{([^{}]+)\\}', _frac_to_div, out)\n",
        "    macros = {\n",
        "        r'\\\\pi': 'π',\n",
        "        r'\\\\sin': 'sin',\n",
        "        r'\\\\cos': 'cos',\n",
        "        r'\\\\tan': 'tan',\n",
        "        r'\\\\sqrt': 'sqrt',\n",
        "        r'\\\\left': '',\n",
        "        r'\\\\right': '',\n",
        "        r'\\\\,': ' ',\n",
        "        r'\\\\;': ' ',\n",
        "    }\n",
        "    for pattern, repl in macros.items():\n",
        "        out = re.sub(pattern, repl, out)\n",
        "\n",
        "    out = out.replace('\\\\', '')\n",
        "\n",
        "    out = re.sub(r'\\s+', ' ', out).strip()\n",
        "    out = re.sub(r'\\(\\s+', '(', out)\n",
        "    out = re.sub(r'\\s+\\)', ')', out)\n",
        "\n",
        "    return out\n",
        "\n",
        "class AgentState(TypedDict, total=False):\n",
        "    user_input: str\n",
        "    should_exit: bool\n",
        "    print_trace: bool\n",
        "    chat_history: List[Dict[str, Any]]        \n",
        "    llm_response: Optional[Dict[str, Any]]    \n",
        "\n",
        "def create_graph():\n",
        "    def get_user_input(state: AgentState) -> dict:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Enter your text (or 'quit' to exit):\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"\\n> \", end=\"\")\n",
        "        user_input = input()\n",
        "        if user_input.lower() in ['quit','exit','q']:\n",
        "            print(\"Goodbye!\"); return {\"user_input\": user_input, \"should_exit\": True}\n",
        "        if user_input.lower() == \"verbose\":\n",
        "            print(\"[TRACE]: verbose\"); return {\"user_input\": user_input, \"should_exit\": False, \"print_trace\": True}\n",
        "        if user_input.lower() == \"quiet\":\n",
        "            return {\"user_input\": user_input, \"should_exit\": False, \"print_trace\": False}\n",
        "        chat_history = state.get(\"chat_history\")\n",
        "        if not chat_history:\n",
        "            chat_history = [{\"role\":\"system\",\"content\": SYSTEM_PROMPT_TEXT}]\n",
        "        chat_history = list(chat_history) + [{\"role\":\"human\",\"content\": user_input}]\n",
        "        return {\"user_input\": user_input, \"should_exit\": False, \"chat_history\": chat_history}\n",
        "\n",
        "    def call_llm(state: AgentState):\n",
        "        serialized_history = state.get(\"chat_history\", [{\"role\":\"system\",\"content\":SYSTEM_PROMPT_TEXT}])\n",
        "\n",
        "        # Truncate history to prevent prompt/token explosion\n",
        "        KEEP_LAST = 18\n",
        "        if isinstance(serialized_history, list) and len(serialized_history) > KEEP_LAST + 1:\n",
        "            head = [m for m in serialized_history[:1] if m.get(\"role\") == \"system\"]\n",
        "            tail = serialized_history[-KEEP_LAST:]\n",
        "            truncated_history = head + tail\n",
        "        else:\n",
        "            truncated_history = serialized_history\n",
        "\n",
        "        message_objs = deserialize_messages_for_model(truncated_history)\n",
        "        if not message_objs or not isinstance(message_objs[-1], HumanMessage):\n",
        "            message_objs.append(HumanMessage(content=state.get(\"user_input\",\"\")))\n",
        "\n",
        "        # Use safe_invoke_llm to handle rate limits\n",
        "        response = safe_invoke_llm(llm_with_tools.invoke, message_objs)\n",
        "        response_serialized = serialize_message(response)\n",
        "        # Persist full (untruncated) history + new assistant message to keep canonical history\n",
        "        new_chat_history = list(serialized_history) + [response_serialized]\n",
        "        return {\"llm_response\": response_serialized, \"chat_history\": new_chat_history}\n",
        "\n",
        "    def route_user_input(state: AgentState):\n",
        "        if state.get(\"should_exit\", False):\n",
        "            if state.get(\"print_trace\", False): print(\"[TRACE] Routing: exiting\")\n",
        "            return END\n",
        "        user_input = state.get(\"user_input\",\"\")\n",
        "        if user_input == \"\" or user_input.lower() in [\"verbose\",\"quiet\"]:\n",
        "            return \"get_user_input\"\n",
        "        if state.get(\"print_trace\", False): print(\"[TRACE] Routing: calling LLM...\")\n",
        "        return \"call_llm\"\n",
        "\n",
        "    def route_after_llm(state: AgentState):\n",
        "        if state.get(\"should_exit\", False):\n",
        "            if state.get(\"print_trace\", False): print(\"[TRACE] Routing: exiting\")\n",
        "            return END\n",
        "        last_message = state.get(\"llm_response\", {})\n",
        "        if last_message and isinstance(last_message, dict) and last_message.get(\"tool_calls\"):\n",
        "            return \"call_tools\"\n",
        "        return \"print_response\"\n",
        "\n",
        "    def call_tools(state: AgentState):\n",
        "        last_message = state.get(\"llm_response\", {}) or {}\n",
        "        tool_calls = last_message.get(\"tool_calls\", []) if isinstance(last_message, dict) else []\n",
        "        serialized_history = state.get(\"chat_history\", [])\n",
        "        tool_results_msgs = []\n",
        "        for tc in tool_calls:\n",
        "            fname = tc.get(\"name\"); fargs = tc.get(\"args\"); fid = tc.get(\"id\")\n",
        "            tool_obj = tool_map.get(fname)\n",
        "            if tool_obj is None:\n",
        "                result = f\"Error: Unknown function {fname}\"\n",
        "            else:\n",
        "                result = invoke_tool(tool_obj, fargs, tool_name=fname)\n",
        "            tm = {\"role\":\"tool\", \"content\": str(result), \"tool_call_id\": fid}\n",
        "            tool_results_msgs.append(tm)\n",
        "            if state.get(\"print_trace\", False):\n",
        "                print(f\"[TRACE] tool {fname} args {fargs} -> {result}\")\n",
        "        new_chat_history = list(serialized_history) + tool_results_msgs\n",
        "        return {\"chat_history\": new_chat_history, \"llm_response\": None}\n",
        "\n",
        "    def print_response(state: AgentState) -> dict:\n",
        "        chat_history = state.get(\"chat_history\", [])\n",
        "        last_assistant = None\n",
        "        for m in reversed(chat_history):\n",
        "            if m.get(\"role\") == \"assistant\":\n",
        "                last_assistant = m; break\n",
        "        if not last_assistant:\n",
        "            last_assistant = state.get(\"llm_response\")\n",
        "        if last_assistant:\n",
        "            content = last_assistant.get(\"content\", \"\")\n",
        "            pretty = clean_assistant_content(content)\n",
        "            print(\"Assistant:\", pretty)\n",
        "        else:\n",
        "            print(\"Assistant: (no content)\")\n",
        "        return {}\n",
        "\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "    graph_builder.add_node(\"get_user_input\", get_user_input)\n",
        "    graph_builder.add_node(\"call_llm\", call_llm)\n",
        "    graph_builder.add_node(\"call_tools\", call_tools)\n",
        "    graph_builder.add_node(\"print_response\", print_response)\n",
        "    graph_builder.add_edge(START, \"get_user_input\")\n",
        "    graph_builder.add_edge(\"print_response\", \"get_user_input\")\n",
        "    graph_builder.add_edge(\"call_tools\", \"call_llm\")\n",
        "    graph_builder.add_conditional_edges(\"call_llm\", route_after_llm, {\"call_tools\":\"call_tools\", \"print_response\":\"print_response\"})\n",
        "    graph_builder.add_conditional_edges(\"get_user_input\", route_user_input, {\"get_user_input\":\"get_user_input\",\"call_llm\":\"call_llm\", END: END})\n",
        "    return graph_builder\n",
        "\n",
        "def find_last_assistant_with_tool_calls(chat: List[Dict[str,Any]]):\n",
        "    for i in range(len(chat)-1, -1, -1):\n",
        "        m = chat[i]\n",
        "        if m.get(\"role\") == \"assistant\" and m.get(\"tool_calls\"):\n",
        "            return i, m\n",
        "    return None, None\n",
        "\n",
        "def save_graph_image(graph, filename=\"lg_graph.png\"):\n",
        "    try:\n",
        "        png_data = graph.get_graph(xray=True).draw_mermaid_png()\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(png_data)\n",
        "        print(f\"Graph image saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save graph image: {e}\")\n",
        "        print(\"Mermaid diagram (copy to renderer):\\ngraph TD\\n    START --> get_user_input\\n    get_user_input --> call_llm\\n    call_llm -->|tool_calls| call_tools\\n    call_llm -->|no tools| print_response\\n    call_tools --> call_llm\\n    print_response --> get_user_input\\n\")\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*50)\n",
        "    print(\"Starting conversation (LangGraph persistent + auto-execute pending tools on resume)...\")\n",
        "    print(\"=\"*50,\"\\n\")\n",
        "    graph_builder = create_graph()\n",
        "    print(\"Graph created successfully!\\n\")\n",
        "    with SqliteSaver.from_conn_string(\"checkpoints.db\") as checkpointer:\n",
        "        graph = graph_builder.compile(checkpointer=checkpointer)\n",
        "        thread_id = \"workflow_1\"\n",
        "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "        current_state = graph.get_state(config)\n",
        "        print(\"Saving graph visualization...\")\n",
        "        save_graph_image(graph)\n",
        "        initial_state: AgentState = {\n",
        "            \"user_input\":\"\", \"should_exit\": False, \"print_trace\": False,\n",
        "            \"chat_history\": [{\"role\":\"system\",\"content\": SYSTEM_PROMPT_TEXT}],\n",
        "            \"llm_response\": None\n",
        "        }\n",
        "\n",
        "        if current_state.next:\n",
        "            next_raw = current_state.next\n",
        "            try:\n",
        "                next_node = next_raw[0] if isinstance(next_raw, (list,tuple)) else next_raw\n",
        "            except Exception:\n",
        "                next_node = str(next_raw)\n",
        "            risky_nodes = (\"call_llm\",\"call_tools\")\n",
        "            if next_node in risky_nodes:\n",
        "                persisted_values = current_state.values or {}\n",
        "                preserved_chat = list(persisted_values.get(\"chat_history\", initial_state.get(\"chat_history\")) or [])\n",
        "                idx, assistant_msg = find_last_assistant_with_tool_calls(preserved_chat)\n",
        "                executed_any = False\n",
        "                if idx is not None and assistant_msg:\n",
        "                    pending_tool_calls = assistant_msg.get(\"tool_calls\", [])\n",
        "                    pending_ids = [tc.get(\"id\") for tc in pending_tool_calls if tc.get(\"id\") is not None]\n",
        "                    already_present_ids = set()\n",
        "                    for m in preserved_chat[idx+1:]:\n",
        "                        if m.get(\"role\") == \"tool\" and m.get(\"tool_call_id\") in pending_ids:\n",
        "                            already_present_ids.add(m.get(\"tool_call_id\"))\n",
        "                    to_execute = [tc for tc in pending_tool_calls if tc.get(\"id\") not in already_present_ids]\n",
        "                    if to_execute:\n",
        "                        print(f\"⟳ Executing {len(to_execute)} pending tool call(s).\")\n",
        "                        tool_results = []\n",
        "                        for tc in to_execute:\n",
        "                            fname = tc.get(\"name\"); fargs = tc.get(\"args\"); fid = tc.get(\"id\")\n",
        "                            tool_obj = tool_map.get(fname)\n",
        "                            if tool_obj is None:\n",
        "                                result = f\"Error: Unknown function {fname}\"\n",
        "                            else:\n",
        "                                result = invoke_tool(tool_obj, fargs, tool_name=fname)\n",
        "                            print(f\"   -> tool {fname} returned: {result}\")\n",
        "                            tool_results.append({\"role\":\"tool\",\"content\": str(result), \"tool_call_id\": fid})\n",
        "                        new_chat = preserved_chat[:idx+1] + tool_results + preserved_chat[idx+1:]\n",
        "                        preserved_chat = new_chat\n",
        "                        executed_any = True\n",
        "                    else:\n",
        "                        print(\"   No missing tool results to execute (they are already present).\")\n",
        "                else:\n",
        "                    print(\"   No assistant message with pending tool_calls was found in history.\")\n",
        "                    i = len(preserved_chat)-1\n",
        "                    removed = []\n",
        "                    while i >=0 and preserved_chat[i].get(\"role\") == \"tool\":\n",
        "                        removed.insert(0, preserved_chat.pop(i))\n",
        "                        i -= 1\n",
        "                    if removed:\n",
        "                        print(\"   Removed trailing orphan 'tool' messages during fallback repair:\")\n",
        "                        for m in removed: print(\"    -\", m.get(\"role\"), \":\", m.get(\"content\"))\n",
        "\n",
        "                N = 10\n",
        "                print(\"\\n--- History after auto-execute/repair (most recent last; up to last {} turns) ---\".format(N))\n",
        "                for m in (preserved_chat or [])[-N:]:\n",
        "                    print(f\"{m.get('role')}: {m.get('content')}\")\n",
        "                print(\"--- end history ---\\n\")\n",
        "\n",
        "                safe_state = {\n",
        "                    \"user_input\": \"\",\n",
        "                    \"should_exit\": False,\n",
        "                    \"print_trace\": persisted_values.get(\"print_trace\", False),\n",
        "                    \"chat_history\": preserved_chat,\n",
        "                    \"llm_response\": None\n",
        "                }\n",
        "                print(\"Resuming at get_user_input with repaired/executed history.\")\n",
        "                graph.invoke(safe_state, config=config)\n",
        "            else:\n",
        "                print(\"Safe to resume where we left off.\")\n",
        "                graph.invoke(None, config=config)\n",
        "        else:\n",
        "            print(\"Starting new workflow...\")\n",
        "            graph.invoke(initial_state, config=config)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07b2fed01a53443692c798ecad6ece12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f158288593f402f9ce99c51a591a953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_99af52b5af6c4755b907fab497de90ed",
            "style": "IPY_MODEL_7fdf2ef5da2e4de3803cb3b584ee7106",
            "value": true
          }
        },
        "16b31a1ac7674eb7809f63232b0c6091": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e86a11951174f84929e1432daad2928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4071ade917e4a0897f4da864cb0f8c3",
            "placeholder": "​",
            "style": "IPY_MODEL_7c47df64a0f848fc8e77c878d893978c",
            "value": "Connecting..."
          }
        },
        "3bc0488c421b4afe92bb9b65c8ab9801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aeff9fd2ba3433fbaacd7058b1857e1",
            "placeholder": "​",
            "style": "IPY_MODEL_7a68be99065946f199caf872656bd08b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "5aeff9fd2ba3433fbaacd7058b1857e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a03c57e863b4621b9711f90ca47e606": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c47fcb666f2549fcaf4af051ea626175",
            "style": "IPY_MODEL_7a31506654d2491fac1f8488824cf6cc",
            "tooltip": ""
          }
        },
        "732a592584b043f7a157ae8672b5ed1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7a31506654d2491fac1f8488824cf6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7a68be99065946f199caf872656bd08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c47df64a0f848fc8e77c878d893978c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fdf2ef5da2e4de3803cb3b584ee7106": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "946f4fa4fdaf420398fd0a6a62872292": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99af52b5af6c4755b907fab497de90ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a220c6f29b1b4107bbfefe96df36aff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9702b8a175e481bb942e73b4049887d",
            "placeholder": "​",
            "style": "IPY_MODEL_16b31a1ac7674eb7809f63232b0c6091",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "a4071ade917e4a0897f4da864cb0f8c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba8f5ac3478f4dd1bd83fbfef0d3fb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_946f4fa4fdaf420398fd0a6a62872292",
            "placeholder": "​",
            "style": "IPY_MODEL_07b2fed01a53443692c798ecad6ece12",
            "value": ""
          }
        },
        "c47fcb666f2549fcaf4af051ea626175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9702b8a175e481bb942e73b4049887d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29e39165d6a4df291e162acbc9fb50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_732a592584b043f7a157ae8672b5ed1b"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
